{"cells":[{"cell_type":"code","source":["SourceSettings = '{\"Directory\": \"landing/aw/\", \"File\": \"*.csv\"}' # \"condition\" : \"target.RowChecksum = source.RowChecksum\",\"mode\":\"merge\"\n","TargetSettings = '{\"SchemaName\":\"aw\", \"mode\":\"overwrite\"}'\n","SourceConnectionSettings = None\n","SinkConnectionSettings = None\n","ActivitySettings = '{\"with_checksum\" : false, \"dedupe\": false, \"ArchiveDirectory\":\"raw/aw\"}' \n","LineageKey = '00000000-0000-0000-0000-000000000000'\n","\n","#Spark CSV Options\n","#https://spark.apache.org/docs/3.5.3/sql-data-sources-csv.html"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"02becf20-708b-4bdb-84f5-9694c006c815","normalized_state":"finished","queued_time":"2024-12-10T15:00:23.8547274Z","session_start_time":null,"execution_start_time":"2024-12-10T15:00:24.5024882Z","execution_finish_time":"2024-12-10T15:00:24.7281761Z","parent_msg_id":"54e81858-b257-4f36-852f-4044fa3f2f09"},"text/plain":"StatementMeta(, 02becf20-708b-4bdb-84f5-9694c006c815, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"5dfede98-c7ab-485e-a32f-0ee72dc69f72"},{"cell_type":"code","source":["import os\n","import json\n","import re\n","import fnmatch\n","from pyspark.sql.functions import lit, input_file_name, expr, sha1,concat_ws, col, coalesce\n","from pyspark.sql import functions as F\n","from datetime import datetime\n","import shutil \n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import input_file_name, regexp_extract\n","\n","source_settings = json.loads(SourceSettings or '{}')\n","target_settings = json.loads(TargetSettings or '{}')\n","activity_settings = json.loads(ActivitySettings or '{}')\n","lakehouse_name = spark.conf.get(\"trident.lakehouse.name\")\n","source_directory = source_settings.pop(\"Directory\")\n","source_file = source_settings.pop(\"File\", None)\n","target_table_name = target_settings.pop(\"TableName\", None)\n","target_schema_name = target_settings.pop(\"SchemaName\", \"stg\").strip(\"_. \")\n","write_mode = target_settings.pop(\"mode\", \"overwrite\")\n","archive_directory = activity_settings.get(\"ArchiveDirectory\")\n","do_archive = bool(archive_directory)\n","dedupe = bool(activity_settings.get(\"dedupe\"))\n","with_checksum = bool(activity_settings.get(\"with_checksum\"))\n","column_names = source_settings.pop(\"names\", None)\n","\n","source_settings.setdefault(\"header\", True)\n","\n","FILES_PREFIX = \"Files\"\n","if not source_directory.startswith(FILES_PREFIX):\n","    source_directory = os.path.join(FILES_PREFIX, source_directory).replace(\"\\\\\", \"/\")\n","\n","if do_archive and not archive_directory.startswith(FILES_PREFIX):\n","    archive_directory = os.path.join(FILES_PREFIX, archive_directory).replace(\"\\\\\", \"/\")\n","\n","if write_mode == \"merge\":\n","    merge_condition = target_settings.pop(\"condition\")\n","\n","if not mssparkutils.fs.exists(source_directory):\n","    print(\"Directory does not exist.\")\n","    mssparkutils.notebook.exit(0)\n","\n","file_list = mssparkutils.fs.ls(source_directory)\n","if not file_list:\n","    print(\"No files found in the source directory.\")\n","    mssparkutils.notebook.exit(0)\n","\n","is_wildcard = \"*\" in source_file or \"?\" in source_file\n","files_to_process = [\n","    os.path.join(source_directory, f.name) for f in file_list if fnmatch.fnmatch(f.name, source_file)\n","] if source_file else [f.path for f in file_list]\n","\n","if not files_to_process:\n","    print(\"No files matched the specified pattern.\")\n","    mssparkutils.notebook.exit(0)\n","\n","table_files_mapping = [files_to_process] if target_table_name else [[file] for file in files_to_process]\n","\n","if len(table_files_mapping) > 1 and column_names:\n","    raise ValueError(\"Cannot supply column names to CSV's going to multiple tables. (Check the source_settings does not have the 'names' parameter, without a target table.)\")\n","\n","for table_files in table_files_mapping:\n","    table_name = target_table_name or os.path.basename(table_files[0]).split(\".\")[0]\n","\n","    if target_schema_name:\n","        table_name = f\"{target_schema_name}.{table_name.strip('_ ')}\"\n","\n","    table_name = table_name.strip('_ ')\n","\n","    print(f\"Extracting {', '.join(table_files)} into {lakehouse_name}.{table_name}.\")\n","    t = datetime.now()\n","\n","    if write_mode == \"overwrite\":\n","        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","\n","    df = spark.read.options(**source_settings).format(\"csv\").load(table_files)\n","\n","    pattern = r'[ ,\\;{}()\\n\\t=]'\n","\n","    clean_headers = [re.sub(pattern, '_', col) for col in column_names or df.columns]\n","\n","    df = df.toDF(*clean_headers)\n","\n","    if with_checksum:\n","        df = df.withColumn(\"RowChecksum\", sha1(concat_ws(\"\", *df.columns)))\n","\n","    df = df.withColumn(\"FileName\", expr(\"substring_index(substring_index(input_file_name(), '/', -1), '?', 1)\"))\n","\n","    if dedupe:\n","        df = df.drop_duplicates([\"RowChecksum\"]) if with_checksum else df.drop_duplicates(clean_headers)\n","\n","    df = df.withColumn(\"LineageKey\", lit(LineageKey))\n","\n","    if not spark.catalog.tableExists(table_name):\n","        write_mode = \"overwrite\"\n","\n","    if write_mode == \"merge\":\n","        sink_df = DeltaTable.forPath(spark, f\"Tables/{table_name.replace('.', '/')}\")\n","        sink_df.alias(\"target\")\\\n","           .merge(df.alias(\"source\"), merge_condition)\\\n","           .whenNotMatchedInsertAll().execute()\n","    else:\n","        df.write.mode(write_mode).format(\"delta\").saveAsTable(table_name)\n","\n","    if source_file:\n","        print(f\"\\t- Wrote {len(table_files)} files into {lakehouse_name}.{table_name} in {(datetime.now()-t).total_seconds()} seconds\")\n","    else:\n","        print(f\"\\t- Wrote {'' if source_file else 'all csvs in directory'} \\\"{table_files}\\\" into {lakehouse_name}.{table_name} in {(datetime.now()-t).total_seconds()} seconds \")\n","\n","    if not do_archive: \n","        print()\n","        continue\n","    \n","    for f in table_files:\n","        source_path = os.path.join(source_directory, os.path.basename(f))\n","\n","        target_path =os.path.join(archive_directory, os.path.basename(f))\n","        print(f\"\\t- Archiving {source_path} to {target_path}.\")\n","        mssparkutils.fs.mv(source_path, target_path, True, True)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"02becf20-708b-4bdb-84f5-9694c006c815","normalized_state":"finished","queued_time":"2024-12-10T15:00:23.9126468Z","session_start_time":null,"execution_start_time":"2024-12-10T15:00:25.1567603Z","execution_finish_time":"2024-12-10T15:00:48.8784858Z","parent_msg_id":"93551577-45f4-4326-8671-254391ac729a"},"text/plain":"StatementMeta(, 02becf20-708b-4bdb-84f5-9694c006c815, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting Files/landing/aw/Account.csv into LH.aw.Account.\n\t- Wrote 1 files into LH.aw.Account in 10.182382 seconds\n\t- Archiving Files/landing/aw/Account.csv to Files/raw/aw/Account.csv.\nExtracting Files/landing/aw/Currency.csv into LH.aw.Currency.\n\t- Wrote 1 files into LH.aw.Currency in 2.02601 seconds\n\t- Archiving Files/landing/aw/Currency.csv to Files/raw/aw/Currency.csv.\nExtracting Files/landing/aw/Date.csv into LH.aw.Date.\n\t- Wrote 1 files into LH.aw.Date in 2.029086 seconds\n\t- Archiving Files/landing/aw/Date.csv to Files/raw/aw/Date.csv.\nExtracting Files/landing/aw/DepartmentGroup.csv into LH.aw.DepartmentGroup.\n\t- Wrote 1 files into LH.aw.DepartmentGroup in 1.765937 seconds\n\t- Archiving Files/landing/aw/DepartmentGroup.csv to Files/raw/aw/DepartmentGroup.csv.\nExtracting Files/landing/aw/Organization.csv into LH.aw.Organization.\n\t- Wrote 1 files into LH.aw.Organization in 1.679574 seconds\n\t- Archiving Files/landing/aw/Organization.csv to Files/raw/aw/Organization.csv.\nExtracting Files/landing/aw/Scenario.csv into LH.aw.Scenario.\n\t- Wrote 1 files into LH.aw.Scenario in 1.681792 seconds\n\t- Archiving Files/landing/aw/Scenario.csv to Files/raw/aw/Scenario.csv.\nExtracting Files/landing/aw/Transactions.csv into LH.aw.Transactions.\n\t- Wrote 1 files into LH.aw.Transactions in 1.920969 seconds\n\t- Archiving Files/landing/aw/Transactions.csv to Files/raw/aw/Transactions.csv.\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bad84b11-25b5-40d2-841b-cf2979cefb58"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d58f4f2d-59d7-406d-ae4c-898354a6a75f","default_lakehouse_name":"LH","default_lakehouse_workspace_id":"5941a6c0-8c98-4d79-b065-a3789e9e0960"}}},"nbformat":4,"nbformat_minor":5}