{"cells":[{"cell_type":"code","source":["# No Parameters as settings obtained dynamically from default spark lakehouse\n","edw=\"DW\"\n","lh=\"LH\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"88b026e1-869a-4768-977f-c8dbfa9c5086","normalized_state":"finished","queued_time":"2024-12-10T15:07:50.3205859Z","session_start_time":"2024-12-10T15:07:50.7263854Z","execution_start_time":"2024-12-10T15:07:59.8511816Z","execution_finish_time":"2024-12-10T15:08:02.1248079Z","parent_msg_id":"635263d5-93cb-4968-aa1c-a402fd27aee7"},"text/plain":"StatementMeta(, 88b026e1-869a-4768-977f-c8dbfa9c5086, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84d4f332-86eb-4306-b4c0-51776a4d857f"},{"cell_type":"code","source":["import pandas as pd\n","from builtin.sql_connection_helper import create_engine\n","import sempy.fabric as fabric\n","import re\n","\n","tenant_id=spark.conf.get(\"trident.tenant.id\")\n","workspace_id=spark.conf.get(\"trident.workspace.id\")\n","lakehouse_id=spark.conf.get(\"trident.lakehouse.id\")\n","lakehouse_name=spark.conf.get(\"trident.lakehouse.name\")\n","sql_end_point=connection_string= fabric.FabricRestClient().get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()['properties']['sqlEndpointProperties']['connectionString']\n","connection_string = \"Driver={{ODBC Driver 18 for SQL Server}};Server={}\".format(sql_end_point)\n","pattern = '[ ,;{}()\\n\\t/=]'\n","\n","# List Datasets from meta data\n","engine = create_engine(connection_string)\n","with engine.connect() as alchemy_connection:  \n","    sql =f\"select  lower(SCHEMA_NAME(schema_id) + '.' + name) as edw_object_name ,  name as edw_table_name, SCHEMA_NAME(schema_id) as schema_name, create_date, modify_date  from {edw}.sys.tables\"\n","    df= pd.read_sql_query (sql, alchemy_connection)\n","    spark_df=spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(\"dict_edw_tables\")\n","\n","    sql =f\"select  lower(SCHEMA_NAME(schema_id) + '.' + name) as edw_object_name ,  name as edw_table_name, SCHEMA_NAME(schema_id) as schema_name, create_date, modify_date  from {lh}.sys.tables\"\n","    df= pd.read_sql_query (sql, alchemy_connection)\n","    spark_df=spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(\"dict_lh_tables\")\n","\n","#Store Fabric Artefacts\n","df=fabric.list_items()\n","df=df.rename(columns=dict(zip(df.columns, [re.sub(pattern, '_', col.strip(pattern).lower()) for col in df.columns])))\n","spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(\"dict_artefacts\")\n","\n","#List Fabric Workspaces\n","df_workspaces =fabric.list_workspaces()\n","df_workspaces=df_workspaces[df_workspaces['Capacity Id'].notna()] \n","\n","#List DataSets and Model Tables \n","df_datasets =fabric.list_datasets()\n","tables=[]\n","for row in df_datasets.itertuples(index=True, name='datasets'):\n","    dataset = row[1]\n","    df =fabric.list_tables(workspace=workspace_id, dataset=dataset)\n","    df=df.rename(columns=dict(zip(df.columns, [re.sub(pattern, '_', col.strip(pattern).lower()) for col in df.columns])))\n","    df.rename(columns={'name': 'table_name'}, inplace=True)\n","    df.insert(0, 'dataset', dataset)\n","    tables.append(df)\n","df=pd.concat(tables, ignore_index=True)\n","df=df.rename(columns=dict(zip(df.columns, [re.sub(pattern, '_', col.strip(pattern).lower()) for col in df.columns])))\n","spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(f\"dict_dataset_tables\") \n","\n","#Store Columns for Data Dictionary\n","columns=[]\n","for row in df_datasets.itertuples(index=True, name='datasets'):\n","    dataset = row[1]\n","    df =fabric.list_tables(workspace=workspace_id, dataset=dataset,include_columns=True)\n","    df=df.rename(columns=dict(zip(df.columns, [re.sub(pattern, '_', col.strip(pattern).lower()) for col in df.columns])))\n","    df.rename(columns={'name': 'table_name'}, inplace=True)\n","    df.insert(0, 'dataset', dataset)\n","    columns.append(df)\n","df=pd.concat(columns, ignore_index=True)\n","df = spark.createDataFrame(df)\n","df.write.mode(\"overwrite\").saveAsTable(f\"dict_dataset_columns\") \n","\n","measures=[]\n","for row in df_datasets.itertuples(index=True, name='datasets'):\n","    dataset = row[1]\n","    df =fabric.list_measures (workspace=workspace_id, dataset=dataset)\n","    df=df.rename(columns=dict(zip(df.columns, [re.sub(pattern, '_', col.strip(pattern).lower()) for col in df.columns])))\n","    df.rename(columns={'name': 'table_name'}, inplace=True)\n","    df.insert(0, 'dataset', dataset)\n","    measures.append(df)\n","df=pd.concat(measures, ignore_index=True)\n","df = spark.createDataFrame(df)\n","df.write.mode(\"overwrite\").saveAsTable(f\"dict_dataset_measures\") \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"88b026e1-869a-4768-977f-c8dbfa9c5086","normalized_state":"finished","queued_time":"2024-12-10T15:07:50.3214582Z","session_start_time":null,"execution_start_time":"2024-12-10T15:08:02.5716098Z","execution_finish_time":"2024-12-10T15:08:53.0315568Z","parent_msg_id":"d0be3f5a-714c-40a4-884c-4fefdf3aae82"},"text/plain":"StatementMeta(, 88b026e1-869a-4768-977f-c8dbfa9c5086, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8fcdc9d2-2f89-4638-9474-990d82299548"}],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"sessionKeepAliveTimeout":0,"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"a365ComputeOptions":null,"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d58f4f2d-59d7-406d-ae4c-898354a6a75f","default_lakehouse_name":"LH","default_lakehouse_workspace_id":"5941a6c0-8c98-4d79-b065-a3789e9e0960"}}},"nbformat":4,"nbformat_minor":5}