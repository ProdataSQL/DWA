{"cells":[{"cell_type":"code","source":["# No Parameters as settings obtained dynamically from default spark lakehouse\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"85fc75e7-cf27-4ed4-93e9-e1cc206c8b79","normalized_state":"finished","queued_time":"2024-12-10T14:00:03.5623627Z","session_start_time":"2024-12-10T14:00:03.8670751Z","execution_start_time":"2024-12-10T14:00:15.9076952Z","execution_finish_time":"2024-12-10T14:00:18.2091192Z","parent_msg_id":"091f9161-aee1-4ed9-8b2b-3334a97b27ec"},"text/plain":"StatementMeta(, 85fc75e7-cf27-4ed4-93e9-e1cc206c8b79, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"895a091c-fe60-4ddd-87d6-611539bd9add"},{"cell_type":"code","source":["# Fabric Refresh Logs\n","import pandas as pd\n","from delta.tables import *\n","import sempy.fabric as fabric\n","import re\n","from pyspark.sql.types import StructType, StructField,  LongType, StringType,DateType, TimestampType,MapType\n","\n","tenant_id=spark.conf.get(\"trident.tenant.id\")\n","workspace_id=spark.conf.get(\"trident.workspace.id\")\n","lakehouse_id=spark.conf.get(\"trident.lakehouse.id\")\n","lakehouse_name=spark.conf.get(\"trident.lakehouse.name\")\n","column_pattern = '[ ,;{}()\\n\\t/=]' #Pattern to remove invalid columns for lakehouse\n","\n","# List Datasets/SemanticModels\n","df_datasets =fabric.list_datasets()\n","\n","table_name=\"fabric_refresh_logs\"\n","for row in df_datasets.itertuples(index=True, name='datasets'):\n","    dataset = row[1]\n","    df=fabric.list_refresh_requests(dataset=dataset, workspace=workspace_id, top_n=100)\n","    df=df.rename(columns=dict(zip(df.columns, [re.sub(column_pattern, '_', col.strip(column_pattern).lower()) for col in df.columns])))\n","    df.insert(0, 'dataset', dataset)\n","    df.insert(1, 'workspace', workspace_id)\n","    df['refresh_attempts'] = df['refresh_attempts'].astype(str) \n","    df.drop(columns=['extended_status'])\n","\n","    schema = StructType([\n","    StructField(\"dataset\", StringType(), True),\n","    StructField(\"workspace\", StringType(), True),\n","    StructField(\"id\",  LongType(), True),\n","    StructField(\"request_id\", StringType(), True),\n","    StructField(\"start_time\", TimestampType(), True),\n","    StructField(\"end_time\", TimestampType(), True),\n","    StructField(\"refresh_type\", StringType(), True),\n","    StructField(\"service_exception_json\", StringType(), True),\n","    StructField(\"status\", StringType(), True),\n","    StructField(\"refresh_attempts\", StringType(), True)\n","    ])\n","\n","    spark_df =spark.createDataFrame(df,schema =schema )\n","    if spark.catalog.tableExists(table_name):\n","        target_table = DeltaTable.forName(spark, f\"{table_name}\")\n","        target_table.alias(\"target\").merge(spark_df.alias(\"source\"), \"source.id=target.id\").whenNotMatchedInsertAll().execute()\n","    else:\n","        spark_df.write.mode('overwrite').saveAsTable(table_name)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"85fc75e7-cf27-4ed4-93e9-e1cc206c8b79","normalized_state":"finished","queued_time":"2024-12-10T14:00:03.5686562Z","session_start_time":null,"execution_start_time":"2024-12-10T14:00:18.604517Z","execution_finish_time":"2024-12-10T14:00:48.7238627Z","parent_msg_id":"258408bd-0258-48cc-8d7c-e48151535c36"},"text/plain":"StatementMeta(, 85fc75e7-cf27-4ed4-93e9-e1cc206c8b79, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8bcff6dc-fb47-4569-99dd-12eeabdd08d8"}],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"sessionKeepAliveTimeout":0,"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"a365ComputeOptions":null,"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d58f4f2d-59d7-406d-ae4c-898354a6a75f","default_lakehouse_name":"LH","default_lakehouse_workspace_id":"5941a6c0-8c98-4d79-b065-a3789e9e0960"}}},"nbformat":4,"nbformat_minor":5}