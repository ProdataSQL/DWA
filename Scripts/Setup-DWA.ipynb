{"cells":[{"cell_type":"markdown","source":["# **Data Warehouse Automation (DWA) Setup in Microsoft Fabric**\n","\n","- Modify the parameter values as needed.\n","- Execute each cell sequentially to ensure proper setup.\n","- Manually verify the created objects within the Fabric Workspace."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9a9979c7-719f-4bf4-9e63-f8a6aaf6ac67"},{"cell_type":"code","source":["lakehouse_name = \"LH\"            #Lakehouse name e.g. Supplychain_silver, Finance_bronze, Sales_lh \n","warehouse_name = \"DW\"            #Warehouse name e.g. Operations_DW, Supplychain_DW\n","metadata_db_name = \"Meta\"        #SQL Database name to store metadata information for the framework\n","lakehouse_schema_enabled = True  #If False then lakehouse and warehouse objects will need to be created manually\n","warehouse_case_sensitive = False #To make data warehouse case sensitive, default is False\n","deploy_aw = True                 #To deploy AdventureWorks files and objects set to True"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:14:21.5848961Z","session_start_time":null,"execution_start_time":"2025-04-04T10:14:21.5859887Z","execution_finish_time":"2025-04-04T10:14:21.9505045Z","parent_msg_id":"d5591788-6a85-4d88-874a-81851e9015dd"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 24, Finished, Available, Finished)"},"metadata":{}}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"19abb929-7fea-4ae9-bb22-f9026054affe"},{"cell_type":"code","source":["import struct\n","import sqlalchemy\n","from sqlalchemy.sql import text\n","from notebookutils import mssparkutils\n","import sempy.fabric as fabric\n","import base64\n","from azure.core.credentials import AccessToken\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import DefaultAzureCredential\n","import os\n","import pyodbc\n","import shutil\n","from git import Repo\n","import requests\n","import json\n","import fnmatch\n","import time\n","from enum import Enum\n","from sqlalchemy.engine import Engine"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":25,"statement_ids":[25],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:14:22.6408573Z","session_start_time":null,"execution_start_time":"2025-04-04T10:14:22.6422743Z","execution_finish_time":"2025-04-04T10:14:22.8794599Z","parent_msg_id":"03d131df-4e55-4c14-92c3-adef70fc53f7"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 25, Finished, Available, Finished)"},"metadata":{}}],"execution_count":23,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee02c50f-f044-4ad4-81c3-d1afaa40738e"},{"cell_type":"markdown","source":["### 1. Create Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecfe6963-fe87-4597-93cb-9bbd40254978"},{"cell_type":"code","source":["workspace_id = fabric.get_workspace_id()\n","workspace_name = fabric.resolve_workspace_name()\n","\n","BASE_URL = \"https://api.fabric.microsoft.com/v1\"\n","access_token = mssparkutils.credentials.getToken('pbi')\n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","lakehouse_url = f\"{BASE_URL}/workspaces/{workspace_id}/lakehouses\"\n","payload = {\n","    \"displayName\": f\"{lakehouse_name}\", \n","    \"description\": \"A schema-enabled lakehouse.\",\n","    \"creationPayload\": {\"enableSchemas\": f\"{lakehouse_schema_enabled}\"} \n","}\n","response = requests.post(lakehouse_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Lakehouse {lakehouse_name} already exists. No change was made.\")\n","elif response.status_code != 201:\n","    raise RuntimeError(f\"Failed to create Lakehouse {lakehouse_name} : {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:09:26.1145277Z","session_start_time":null,"execution_start_time":"2025-04-04T10:09:26.1156587Z","execution_finish_time":"2025-04-04T10:09:27.0941792Z","parent_msg_id":"6c8ac28d-1233-456f-b2ba-77261b5da6ea"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Lakehouse LH already exists. No change was made.\n"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa181b62-1c13-45fd-909d-c415eda39a1e"},{"cell_type":"markdown","source":["### 2. Create Metadata SQL Database\n","#### **Warning:** Verify the SQL DB creation before executing the next cell."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e66b8de5-1aee-4278-ad8d-f3baf2a2554b"},{"cell_type":"code","source":["payload = {\n","    \"displayName\": f\"{metadata_db_name}\",\n","    \"type\": \"SQLDatabase\",\n","    \"description\": \"SQL Database to store metadata for Framework\"\n","}\n","\n","sqldb_url = f'{BASE_URL}/workspaces/{workspace_id}/items'\n","response = requests.post(sqldb_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"SQL DB {metadata_db_name} already exists. No change was made.\")\n","else:\n","    start_time = time.time() \n","    max_wait_time = 600  \n","    check_interval = 120\n","    \n","    while time.time() - start_time < max_wait_time: #Sleep timer to wait for SQL DB creation\n","        time.sleep(check_interval)\n","        db_exists = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", workspace_id)   \n","\n","        if db_exists:\n","            break\n","    else: \n","        raise RuntimeError(f\"Failed to create database {metadata_db_name} within the timeout period.\")\n","    \n","    if response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to create database {metadata_db_name}: {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:09:28.9008335Z","session_start_time":null,"execution_start_time":"2025-04-04T10:09:28.9019707Z","execution_finish_time":"2025-04-04T10:09:29.7162302Z","parent_msg_id":"371f457f-2e80-451d-931b-9a8483a09ffd"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SQL DB Meta already exists. No change was made.\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"786c758b-d46c-462c-a5ff-3a9bf6edf8b0"},{"cell_type":"markdown","source":["### 3. Create Data Warehouse\n","#### **Warning:** Verify the Data Warehouse creation before executing the next cell."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"177d070a-a9c4-4beb-8eac-7f2a40374907"},{"cell_type":"code","source":["if warehouse_case_sensitive == True:\n","    warehouse_collation = \"Latin1_General_100_BIN2_UTF8\"\n","else:\n","    warehouse_collation = \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"\n","\n","payload = {\n","    \"displayName\": f\"{warehouse_name}\",\n","    \"type\": \"warehouse\",\n","    \"properties\": {\n","        \"collation\": f\"{warehouse_collation}\"  \n","    }\n","}\n","\n","dw_url = f'{BASE_URL}/workspaces/{workspace_id}/items'\n","response = requests.post(dw_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Warehouse {warehouse_name} already exists. No change was made.\")\n","else:\n","    start_time = time.time() \n","    max_wait_time = 600  \n","    check_interval = 120\n","    \n","    while time.time() - start_time < max_wait_time: #Sleep timer to wait for SQL DB creation\n","        time.sleep(check_interval)\n","        dw_exists = fabric.resolve_item_id(warehouse_name, \"Warehouse\", workspace_id)   \n","\n","        if dw_exists:\n","            break\n","    else: \n","        raise RuntimeError(f\"Failed to create warehouse {warehouse_name} within the timeout period.\")\n","    \n","    if response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to create warehouse {warehouse_name}: {response.status_code}, {response.text}\")        "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:09:30.9749808Z","session_start_time":null,"execution_start_time":"2025-04-04T10:09:30.9761891Z","execution_finish_time":"2025-04-04T10:09:31.8300297Z","parent_msg_id":"b769364b-510e-46cc-8a47-3e1fe02d86c3"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warehouse DW already exists. No change was made.\n"]}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcc75513-071f-4146-b283-9fddeaa22a6b"},{"cell_type":"markdown","source":["### 4. Upload Notebook Templates"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"954779cc-808c-4fb1-bdbd-94360f997005"},{"cell_type":"code","source":["repo_url = \"https://github.com/ProdataSQL/DWA\"\n","repo_dir = \"DWA_repo\"\n","\n","def clone_git_repo(repo_url, repo_dir):\n","    if os.path.exists(repo_dir):\n","        shutil.rmtree(repo_dir)\n","    Repo.clone_from(repo_url, repo_dir)\n","    os.chdir(repo_dir)\n","    \n","clone_git_repo(repo_url, repo_dir)\n","\n","def get_notebooks(directory, pattern):\n","    notebook_files = []\n","    for root, dirs, files in os.walk(directory):\n","        for file in fnmatch.filter(files, pattern):\n","            notebook_files.append(os.path.join(root, file))\n","    return notebook_files\n","\n","def py_to_notebook(py_content, notebook_name):\n","    notebook_content = {\n","        \"nbformat\": 4,\n","        \"nbformat_minor\": 5,\n","        \"cells\": [\n","            {\n","                \"cell_type\": \"code\",\n","                \"source\": [py_content],\n","                \"execution_count\": None,\n","                \"outputs\": []\n","            }\n","        ],\n","        \"metadata\": {\n","            \"language_info\": {\n","                \"name\": \"python\"\n","            }\n","        }\n","    }\n","    return notebook_content\n","\n","def upload_notebook(notebook_name, notebook_content, workspace_id, headers):\n","    notebook_json = json.dumps(notebook_content)\n","    notebook_base64 = base64.b64encode(notebook_json.encode('utf-8')).decode('utf-8')\n","\n","    notebook_url = f\"{BASE_URL}/workspaces/{workspace_id}/notebooks\"\n","    \n","    payload = {\n","        \"displayName\": notebook_name,\n","        \"description\": f\"Imported notebook {notebook_name}\",\n","        \"definition\": {\n","            \"format\": \"ipynb\",\n","            \"parts\": [\n","                {\n","                    \"path\": \"artifact.content.ipynb\",\n","                    \"payload\": notebook_base64,\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    fabric_response = requests.post(\n","        notebook_url,\n","        headers=headers,\n","        data=json.dumps(payload)\n","    )\n","    \n","    if fabric_response.status_code == 400:\n","        print(f\"The {notebook_name} already exists. No changes were made\")\n","    elif fabric_response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to upload {notebook_name}: {fabric_response.status_code} - {fabric_response.text}\")\n","\n","\n","directory = \"Workspaces/DWA/\"\n","pattern = \"*.py\"\n","\n","notebook_files = get_notebooks(directory, pattern)\n","\n","for notebook_file in notebook_files:\n","    directory, file_name = os.path.split(notebook_file)\n","    base_name = os.path.basename(directory)\n","    if base_name.endswith('.Notebook'):\n","        notebook_name = base_name.split('.')[0]\n","    \n","    with open(notebook_file, 'r') as file:\n","        py_content = file.read()\n","    \n","    notebook_content = py_to_notebook(py_content, notebook_name)\n","    \n","    upload_notebook(notebook_name, notebook_content, workspace_id=workspace_id, headers=headers)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:14:37.0609781Z","session_start_time":null,"execution_start_time":"2025-04-04T10:14:37.0621353Z","execution_finish_time":"2025-04-04T10:14:40.4261169Z","parent_msg_id":"357e52ef-ae41-4b2a-806f-581625d87024"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 27, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The Extract-O365-API already exists. No changes were made\nThe Demo - 10 Mins already exists. No changes were made\nThe Extract-SP-Excel already exists. No changes were made\nThe Extract-Fabric-Logs already exists. No changes were made\nThe SharePoint-Shared-Functions already exists. No changes were made\nThe Ingest-SFTP already exists. No changes were made\nThe Extract-XML already exists. No changes were made\nThe Extract-CSV-Pandas already exists. No changes were made\nThe Refresh-Fabric already exists. No changes were made\nThe Extract-CSV already exists. No changes were made\nThe Extract-Dictionary already exists. No changes were made\n"]}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc8a1f0f-c7c9-4a10-96d9-cbe4ebf02181"},{"cell_type":"markdown","source":["### 5. Create Metadata and Data Warehouse SQL Objects"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb600388-b06e-4f66-bebf-e843454f14b6"},{"cell_type":"code","source":["META_DB_NAME = metadata_db_name\n","DW_NAME = warehouse_name\n","original_dir = os.getcwd()\n","engine_pool = {}\n","\n","class DBType(Enum):\n","    SQLDatabase = \"SQLDatabases\"\n","    Warehouse = \"Warehouses\"\n","\n","    def get_connection_string(self) -> str: \n","        display_name = DW_NAME if self == DBType.Warehouse else META_DB_NAME\n","        client = fabric.FabricRestClient()\n","        endpoint = f\"/v1/workspaces/{workspace_id}/{self.value}\"\n","        databases = client.get(endpoint).json()\n","\n","        selected_database = next((db for db in databases.get(\"value\", []) if db.get(\"displayName\") == display_name), None)\n","        if not selected_database:\n","            raise ValueError(f\"No {self.value} with displayName '{display_name}' found.\")\n","        \n","        server = selected_database['properties'].get('serverFqdn') or selected_database['properties'].get('connectionString')\n","        database = selected_database['properties'].get('databaseName', warehouse_name)\n","            \n","        return f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};database={database};LongAsMax=YES\"\n","\n","    def get_engine(self) -> Engine:\n","        if self not in engine_pool:\n","            token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n","            token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n","            engine_pool[self] = sqlalchemy.create_engine(\n","                \"mssql+pyodbc://\",\n","                creator=lambda: pyodbc.connect(self.get_connection_string(), attrs_before={1256: token_struct})\n","            )\n","        return engine_pool[self]\n","   \n","def read_table_order(file_path: str) -> list:\n","    if not os.path.exists(file_path):\n","        return []\n","    \n","    with open(file_path, \"r\") as f:\n","        return [line.strip() for line in f if line.strip()]\n","\n","def process_sql_script(db_type: DBType, script_path: str):\n","    if not os.path.exists(script_path):\n","        return\n","\n","    with open(script_path, \"r\") as file:\n","        script = file.read()\n","\n","    statements = script.split(\"\\nGO\\n\")\n","    \n","    object_name = script_path.split(\"/\")[-1].split(\".\")[0]\n","    db_name = script_path.split(\"/\")[0].replace(\".\", \" \")\n","    schema_name = script_path.split(\"/\")[1]\n","\n","    if \"Tables/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.tables WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"Views/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.views WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"StoredProcedures/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.procedures WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"Security/\" in script_path:  \n","        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{object_name}'\"\n","    else:\n","        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{schema_name}'\"\n","    \n","    with db_type.get_engine().connect() as conn:\n","        object_exists = conn.execute(text(check_query)).scalar() > 0\n","\n","        if not object_exists:\n","            try:\n","                for statement in filter(None, map(str.strip, statements)):\n","                    conn.execute(text(statement))\n","                conn.commit()\n","            except Exception as e:\n","                conn.rollback()\n","                print(f\"Failed to create {schema_name}.{object_name}: {e}\")\n","                raise\n","        else:\n","            print(f\"The object {schema_name}.{object_name} already exists in {db_name}. No changes made.\")\n","\n","def iterate_sql_objects(db_type: DBType, object_type: str, objects_created: set):\n","    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n","    visited_dirs = set()\n","    \n","    for root, _, files in os.walk(base_path):\n","        if root in visited_dirs:\n","            continue\n","        visited_dirs.add(root)\n","        \n","        for file in filter(lambda f: f.endswith(\".sql\"), files):\n","            script_path = os.path.join(root, file).lstrip(\"./\")\n","           \n","            if script_path in objects_created:\n","                continue\n","\n","            parts = script_path.split(os.sep)\n","            if object_type == \"Schema\":\n","                if any(folder in parts for folder in [\"Tables\", \"Views\", \"StoredProcedures\"]):\n","                    continue  \n","            else:\n","                if object_type not in parts:\n","                    continue  \n","\n","            process_sql_script(db_type, script_path)\n","            objects_created.add(script_path)\n","\n","def process_sql_objects(db_type: DBType, obj_type: str = None):\n","    table_orders_cache = {}\n","    has_changed_directory = False \n","    current_dir = os.path.abspath(os.getcwd())\n","\n","    table_order_files = {\n","        DBType.SQLDatabase: os.path.join(current_dir, \"Setup\", \"Files\", \"SQLDatabase\", \"MetaTableOrder.txt\"),\n","        DBType.Warehouse: os.path.join(current_dir, \"Setup\", \"Files\", \"Warehouse\", \"DWTableOrder.txt\")\n","    }\n","    \n","    for db_type_key, file_path in table_order_files.items():        \n","        if os.path.exists(file_path):\n","            table_orders_cache[db_type_key] = read_table_order(file_path)\n","\n","    table_order = table_orders_cache.get(db_type, [])\n","\n","    if not has_changed_directory:\n","        workspaces_dwa_path = os.path.join(current_dir, \"Workspaces\", \"DWA\")\n","        \n","        if os.path.exists(workspaces_dwa_path):\n","            os.chdir(workspaces_dwa_path)\n","            has_changed_directory = True \n","    \n","    objects_created = set()\n","    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n","    \n","    for table in table_order:\n","        schema = table.split(\".\", maxsplit=1)[0].strip(\"[]\")\n","        table_name = table.split(\".\", maxsplit=1)[1].strip(\"[]\")\n","        script_path = f\"{base_path}/{schema}/{obj_type}/{table_name}.sql\"\n","            \n","        if os.path.exists(script_path) and script_path not in objects_created:\n","            process_sql_script(db_type, script_path)\n","            objects_created.add(script_path)\n","    \n","    iterate_sql_objects(db_type, obj_type, objects_created)\n","    os.chdir(original_dir)\n","\n","#Create meta databse objects\n","process_sql_objects(DBType.SQLDatabase, \"Security\")\n","process_sql_objects(DBType.SQLDatabase, \"Tables\")\n","process_sql_objects(DBType.SQLDatabase, \"Views\")\n","process_sql_objects(DBType.SQLDatabase, \"StoredProcedures\") \n","\n","#Create warehouse objects\n","process_sql_objects(DBType.Warehouse, \"Schema\")\n","process_sql_objects(DBType.Warehouse, \"StoredProcedures\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:03:54.2936592Z","session_start_time":null,"execution_start_time":"2025-04-04T10:03:54.2950987Z","execution_finish_time":"2025-04-04T10:04:02.0360741Z","parent_msg_id":"e075db6c-2304-4c71-88b7-aac377bbf472"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The object Security.audit already exists in Meta SQLDatabase. No changes made.\nThe object Security.devops already exists in Meta SQLDatabase. No changes made.\nThe object Security.config already exists in Meta SQLDatabase. No changes made.\nThe object dbo.dict_artefacts already exists in Meta SQLDatabase. No changes made.\nThe object config.Configurations already exists in Meta SQLDatabase. No changes made.\nThe object config.Datasets already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTables already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTableJoins already exists in Meta SQLDatabase. No changes made.\nThe object config.Templates already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroups already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupLinks already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupTables already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineGroups already exists in Meta SQLDatabase. No changes made.\nThe object config.Pipelines already exists in Meta SQLDatabase. No changes made.\nThe object config.DatasetLineage already exists in Meta SQLDatabase. No changes made.\nThe object audit.PipelineLog already exists in Meta SQLDatabase. No changes made.\nThe object audit.LoadLog already exists in Meta SQLDatabase. No changes made.\nThe object audit.ReleaseLog already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineMeta already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineMetaCache already exists in Meta SQLDatabase. No changes made.\nThe object config.Artefacts already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_TruncateAll already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_PipelineBuildMetaData already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_PipelineQueue already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_PipelineSequence already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_OpsDatasets already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_GetTables already exists in Meta SQLDatabase. No changes made.\nThe object devops.usp_ConfigPackageTables already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_LoadSuccess already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_LoadStart already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineEndFailure already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineStart already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineEndSuccess already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_LoadFailure already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineLogCopy already exists in Meta SQLDatabase. No changes made.\nThe object aw_int.aw_int already exists in DW Warehouse. No changes made.\nThe object stg.stg already exists in DW Warehouse. No changes made.\nThe object reports.reports already exists in DW Warehouse. No changes made.\nThe object int.int already exists in DW Warehouse. No changes made.\nThe object aw.aw already exists in DW Warehouse. No changes made.\nThe object dwa.dwa already exists in DW Warehouse. No changes made.\nThe object int.usp_SettingEndDate already exists in DW Warehouse. No changes made.\nThe object int.usp_LoadNumbers already exists in DW Warehouse. No changes made.\nThe object aw.usp_AccountRangeMapCreate already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_Insert already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_PrestageView already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_CheckSchemaDrift already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_Delete already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableCreateColumns already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_Update already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_CTAS already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableDrop already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableCreate already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_DeploySchemaDrift already exists in DW Warehouse. No changes made.\nThe object dwa.usp_PipelinePostExecute already exists in DW Warehouse. No changes made.\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de3b1a99-895a-4e74-8a0a-32132f617a34"},{"cell_type":"markdown","source":["### 6. Upload Data Pipelines"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da333f14-9428-48c3-aead-65c5d0bf1117"},{"cell_type":"code","source":["# Stores new references for updating old data pipelines connections \n","lh_artifact_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", workspace_id) #Lakehouse ID\n","\n","meta_artifact_id = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", workspace_id) #SQL DB ID and Connection String\n","\n","sqldb_url = f'{BASE_URL}/workspaces/{workspace_id}/SQLDatabases/{meta_artifact_id}'\n","response = requests.get(sqldb_url, headers=headers)\n","\n","meta_endpoint = response.json().get(\"properties\", {}).get(\"serverFqdn\")\n","meta_databasename = response.json().get(\"properties\", {}).get(\"databaseName\")\n","\n","dw_artifact_id = fabric.resolve_item_id(warehouse_name, \"Warehouse\", workspace_id) #Warehouse ID and Connection String\n","\n","dw_url = f'{BASE_URL}/workspaces/{workspace_id}/warehouses/{dw_artifact_id}'\n","response = requests.get(dw_url, headers=headers)\n","\n","dw_endpoint = response.json().get(\"properties\", {}).get(\"connectionString\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:04:04.8021059Z","session_start_time":null,"execution_start_time":"2025-04-04T10:04:04.8032981Z","execution_finish_time":"2025-04-04T10:04:06.2314374Z","parent_msg_id":"acb2e7df-e380-4ea5-b237-12654dd1be39"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eee6400d-7f6f-4b5f-a5a0-3b5433065a31"},{"cell_type":"code","source":["access_token = mssparkutils.credentials.getToken('pbi') #Rebuild access token \n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","class DataPipeline:\n","    pass\n","\n","class DataPipeline:\n","    name: str\n","    git_id: str\n","    real_id: str\n","    raw_definition: str\n","    definition: object\n","    pipeline_created: bool = False\n","\n","    def __init__(self, name, git_id, raw_definition, definition):\n","        self.name = name\n","        self.git_id = git_id\n","        self.definition = definition\n","        self.raw_definition = raw_definition\n","    def __hash__(self) -> int:\n","        return hash(self.git_id)\n","    def __eq__(self, other):\n","        return self.git_id == other.git_id\n","\n","    def __str__(self):\n","        return f\"DataPipeline: {self.name}\"\n","\n","    def __repr__(self):\n","        return f\"DataPipeline: {self.name}\"\n","\n","    def is_created(self) -> bool:\n","        return self in pipelines_created or self.pipeline_created\n","\n","    def create(self, pipelines):\n","        global pipelines_created\n","        if self.is_created():\n","            print(f\"{self.name} already created\")\n","            return\n","\n","        print(f\"Creating pipeline {self.name}\")\n","\n","        child_pipelines: list[DataPipeline] = self.get_child_pipelines(pipelines)\n","\n","        for child_pipeline in child_pipelines:\n","            if child_pipeline.is_created():\n","                continue\n","            child_pipeline.create(pipelines)\n","\n","        self.update_activity_references()\n","        self.upload_pipeline()\n","\n","        print(f\"{self.name} created and uploaded.\")\n","\n","    def get_child_pipelines(self, pipelines : list[DataPipeline]) -> list[DataPipeline]:\n","        self.child_pipelines: set[DataPipeline] = set()\n","\n","        def find_pipelines(activities : list [object]):\n","            for activity in activities:\n","                activity_type = activity.get(\"type\")\n","                type_properties = activity.get(\"typeProperties\", {})\n","\n","                if activity_type == \"ExecutePipeline\":\n","                    reference_name = type_properties.get(\"pipeline\", {}).get(\n","                        \"referenceName\"\n","                    )\n","                    matched_pipeline = next(\n","                        (x for x in pipelines if x.git_id == reference_name), None\n","                    )\n","                    if matched_pipeline:\n","                        self.child_pipelines.add(matched_pipeline)\n","\n","                elif activity_type == \"InvokePipeline\":\n","                    pipeline_id = type_properties.get(\"pipelineId\")\n","                    matched_pipeline = next(\n","                        (x for x in pipelines if x.git_id == pipeline_id), None\n","                    )\n","                    if matched_pipeline:\n","                        self.child_pipelines.add(matched_pipeline)\n","                    \n","                else:\n","                    for key in [\"activities\", \"ifTrueActivities\", \"ifFalseActivities\"]:\n","                        if key in type_properties:\n","                            find_pipelines(type_properties[key])\n","            return self.child_pipelines\n","\n","        return find_pipelines(\n","            self.definition.get(\"properties\", {}).get(\"activities\", [])\n","        )\n","\n","    def update_activity_references(self):\n","        global pipelines_created\n","\n","        def fetch_trident_notebooks():\n","            notebooks_url = f\"{BASE_URL}/workspaces/{workspace_id}/notebooks\"\n","            response = requests.get(notebooks_url, headers=headers)\n","            response.raise_for_status()\n","            return {notebook[\"displayName\"]: notebook[\"id\"] for notebook in response.json().get(\"value\", [])}\n","\n","        trident_notebooks = fetch_trident_notebooks()\n","\n","        new_definition = self.raw_definition\n","        for pipeline in pipelines_created: \n","            new_definition = new_definition.replace(pipeline.git_id, pipeline.real_id)\n","\n","        definition_json = json.loads(new_definition)\n","\n","        def replace_values(obj):\n","            if isinstance(obj, dict):\n","                for key, value in obj.items():\n","                    if key == \"type\" and isinstance(value, str):\n","                        if \"Lakehouse\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict) :\n","                                if \"artifactId\" in obj[\"typeProperties\"]  and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = lh_artifact_id  \n","\n","                        elif \"DataWarehouse\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n","                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = dw_artifact_id \n","                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n","                                obj[\"endpoint\"] = dw_endpoint \n","\n","                        elif \"FabricSqlDatabase\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n","                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = meta_artifact_id  \n","                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n","                                obj[\"endpoint\"] = meta_endpoint  \n","\n","                        elif \"TridentNotebook\" in value:\n","                            notebook_name = obj.get(\"name\")\n","                            if notebook_name in trident_notebooks:\n","                                obj[\"typeProperties\"][\"notebookId\"] = trident_notebooks[notebook_name]\n","\n","                    elif key == \"workspaceId\" and isinstance(value, str) and not value.startswith(\"@\"):\n","                        obj[key] = workspace_id  \n","\n","                    replace_values(value)\n","\n","            elif isinstance(obj, list):\n","                for item in obj:\n","                    replace_values(item)\n","\n","        replace_values(definition_json)\n","\n","        self.raw_definition = json.dumps(definition_json, indent=4)\n","\n","    def upload_pipeline(self):\n","        global pipelines_created\n","        pipelines_url = f\"{BASE_URL}/workspaces/{workspace_id}/dataPipelines\"\n","        pipeline_b64 = base64.b64encode(self.raw_definition.encode()).decode()\n","\n","        payload = {\n","            \"displayName\": self.name,\n","            \"definition\": {\n","                \"parts\": [\n","                    {\n","                        \"path\": \"pipeline.content.json\",\n","                        \"payload\": pipeline_b64,\n","                        \"payloadType\": \"InlineBase64\",\n","                    }\n","                ]\n","            },\n","        }\n","\n","        create_pipeline_request = requests.post(pipelines_url, headers=headers, data=json.dumps(payload))\n","        if not create_pipeline_request.ok:\n","            print(create_pipeline_request.json())\n","            raise RuntimeError(f\"Failed to create pipeline {self.name}. ({create_pipeline_request.text})\")\n","        \n","        self.real_id = create_pipeline_request.json().get(\"id\")\n","        self.pipeline_created = True\n","        pipelines_created.append(self)\n","    \n","pipelines: list[DataPipeline] = []\n","\n","for path, dirs, files in os.walk(\".\"):\n","    if not path.endswith(\".DataPipeline\"):\n","        continue\n","\n","    pipeline_name = path.strip(\".\").split(\".\")[0].replace(\"\\\\\", \"/\").split(\"/\")[-1]\n","    platform = json.load(open(os.path.join(path, \".platform\")))\n","    raw_definition = open(os.path.join(path, \"pipeline-content.json\")).read()\n","    definition = json.loads(raw_definition)\n","    pipeline_id = platform[\"config\"][\"logicalId\"]\n","\n","    data_pipeline = DataPipeline(pipeline_name, pipeline_id, raw_definition, definition)\n","    pipelines.append(data_pipeline)\n","\n","pipelines_url = f\"{BASE_URL}/workspaces/{workspace_id}/dataPipelines\"\n","response = requests.get(pipelines_url, headers=headers)\n","response.raise_for_status()\n","\n","pipelines_created: list[DataPipeline] = []\n","\n","pipelines_existing: list[(str, str)] = [\n","    (pipeline[\"displayName\"], pipeline[\"id\"])\n","    for pipeline in response.json().get(\"value\", [])\n","]\n","\n","for pipeline_name, pipeline_id in pipelines_existing:\n","    pipeline = next((x for x in pipelines if x.name == pipeline_name), None)\n","    if not pipeline:\n","        continue\n","\n","    pipeline.real_id = pipeline_id\n","    pipeline.pipeline_created = True\n","    pipelines_created.append(pipeline)\n","\n","for pipeline in pipelines:\n","    if pipeline in pipelines_created or pipeline.pipeline_created:\n","        continue\n","    pipeline.create(pipelines)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"finished","queued_time":"2025-04-04T10:04:09.008183Z","session_start_time":null,"execution_start_time":"2025-04-04T10:04:09.0099749Z","execution_finish_time":"2025-04-04T10:04:09.7719268Z","parent_msg_id":"94c5ce8c-9461-45dd-be92-ce86138bd232"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19d74a8c-843a-4ba4-bcc8-f55a28ebf92f"},{"cell_type":"markdown","source":["### 7. Create objects for Adventure Works when deploy_aw is set to True"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acc1cdcb-2625-449d-9a11-c903889ec341"},{"cell_type":"code","source":["if deploy_aw == False:\n","    mssparkutils.notebook.exit(1)\n","\n","class CustomTokenCredential:\n","    def get_token(self, *scopes, **kwargs):\n","        return AccessToken(notebookutils.credentials.getToken('storage'), expires_on=9999999999)\n","\n","credential = CustomTokenCredential()\n","service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n","fs = service_client.get_file_system_client(workspace_name)\n","\n","lh_paths = {\n","    \"tmp/landing/\": f\"{lakehouse_name}.Lakehouse/Files/landing\",\n","    \"tmp/backup/\": f\"{lakehouse_name}.Lakehouse/Files/backup\",\n","    \"tmp/Tables/\": f\"{lakehouse_name}.Lakehouse/Tables\"\n","}\n","\n","def unpack_files(Setup_dir, archives):\n","    clone_git_repo(repo_url, repo_dir)\n","    for archive, target in archives.items():\n","        shutil.unpack_archive(os.path.join(Setup_dir, archive), target, \"zip\")\n","\n","def upload_files(local_path, azure_path):\n","    for root, _, files in os.walk(local_path):\n","        for file in files:\n","            file_path_on_local = os.path.join(root, file)\n","            relative_path = os.path.relpath(root, local_path)\n","            file_path_on_azure = os.path.join(azure_path, relative_path, file).replace(\"\\\\\", \"/\")\n","            file_client = fs.get_file_client(file_path_on_azure)\n","            with open(file_path_on_local, \"rb\") as data:\n","                file_client.upload_data(data, overwrite=True)\n","\n","git_lh_directory = \"Setup/Files/Lakehouse\"\n","archives = {\n","    'AW_landing.zip': \"tmp/landing\",\n","    'AW_Backup.zip': \"tmp/backup\",\n","    'AW_tables.zip': \"tmp/Tables\"\n","}\n","\n","unpack_files(git_lh_directory, archives)\n","for local, azure in lh_paths.items():\n","    upload_files(local, azure)\n","\n","#Deploy AW Warehouse Objects \n","time.sleep(120)  # 2mins wait for MD Sync of the Lakehouse objects\n","process_sql_objects(DBType.Warehouse, \"Tables\")\n","process_sql_objects(DBType.Warehouse, \"Views\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"submitted","livy_statement_state":"running","session_id":"cb699a30-ef48-4240-980d-045fe83ce5f0","normalized_state":"running","queued_time":"2025-04-04T10:14:47.3967928Z","session_start_time":null,"execution_start_time":"2025-04-04T10:14:47.3979869Z","execution_finish_time":null,"parent_msg_id":"76da1cdf-d8c7-4390-8a73-a25957ac030b"},"text/plain":"StatementMeta(, cb699a30-ef48-4240-980d-045fe83ce5f0, 28, Submitted, Running, Running)"},"metadata":{}}],"execution_count":26,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e775b-7c60-4d73-a9d7-d6bd77673d28"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"warehouse":{}}},"nbformat":4,"nbformat_minor":5}