{"cells":[{"cell_type":"markdown","source":["# **Data Warehouse Automation (DWA) Setup in Microsoft Fabric**\n","\n","- Modify the parameter values as needed.\n","- Execute each cell sequentially to ensure proper setup.\n","- Manually verify the created objects within the Fabric Workspace."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9a9979c7-719f-4bf4-9e63-f8a6aaf6ac67"},{"cell_type":"code","source":["lakehouse_name = \"LH\"            #Lakehouse name e.g. Supplychain_silver, Finance_bronze, Sales_lh \n","warehouse_name = \"DW\"            #Warehouse name e.g. Operations_DW, Supplychain_DW\n","metadata_db_name = \"Meta\"        #SQL Database name to store metadata information for the framework\n","lakehouse_schema_enabled = True  #If False then lakehouse and warehouse objects will need to be created manually\n","warehouse_case_sensitive = False #To make data warehouse case sensitive, default is False\n","deploy_aw = True                 #To deploy AdventureWorks files and objects set to True"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:27:05.7599231Z","session_start_time":null,"execution_start_time":"2025-04-03T10:27:05.7610745Z","execution_finish_time":"2025-04-03T10:27:06.0198866Z","parent_msg_id":"d955a4ca-3f5c-4377-8afb-39ba602d21ae"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"19abb929-7fea-4ae9-bb22-f9026054affe"},{"cell_type":"code","source":["import struct\n","import sqlalchemy\n","from sqlalchemy.sql import text\n","from notebookutils import mssparkutils\n","import sempy.fabric as fabric\n","import base64\n","from azure.core.credentials import AccessToken\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import DefaultAzureCredential\n","import os\n","import pyodbc\n","import shutil\n","from git import Repo\n","import requests\n","import json\n","import fnmatch\n","import time\n","from enum import Enum\n","from sqlalchemy.engine import Engine"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:27:06.9401059Z","session_start_time":null,"execution_start_time":"2025-04-03T10:27:06.9412674Z","execution_finish_time":"2025-04-03T10:27:13.0159294Z","parent_msg_id":"31db942c-a817-4387-8dd8-7a07574bc797"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee02c50f-f044-4ad4-81c3-d1afaa40738e"},{"cell_type":"markdown","source":["### 1. Create Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecfe6963-fe87-4597-93cb-9bbd40254978"},{"cell_type":"code","source":["workspace_id = fabric.get_workspace_id()\n","workspace_name = fabric.resolve_workspace_name()\n","\n","BASE_URL = \"https://api.fabric.microsoft.com/v1\"\n","access_token = mssparkutils.credentials.getToken('pbi')\n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","lakehouse_url = f\"{BASE_URL}/workspaces/{workspace_id}/lakehouses\"\n","payload = {\n","    \"displayName\": f\"{lakehouse_name}\", \n","    \"description\": \"A schema-enabled lakehouse.\",\n","    \"creationPayload\": {\"enableSchemas\": f\"{lakehouse_schema_enabled}\"} \n","}\n","response = requests.post(lakehouse_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Lakehouse {lakehouse_name} already exists. No change was made.\")\n","elif response.status_code != 201:\n","    raise RuntimeError(f\"Failed to create Lakehouse {lakehouse_name} : {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:27:14.6089936Z","session_start_time":null,"execution_start_time":"2025-04-03T10:27:14.6101809Z","execution_finish_time":"2025-04-03T10:27:19.242967Z","parent_msg_id":"34a29f05-7332-4f1a-81d0-ade8584c2948"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Lakehouse LH already exists. No change was made.\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa181b62-1c13-45fd-909d-c415eda39a1e"},{"cell_type":"markdown","source":["### 2. Create Metadata SQL Database\n","#### **Warning:** Verify the SQL DB creation before executing the next cell."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e66b8de5-1aee-4278-ad8d-f3baf2a2554b"},{"cell_type":"code","source":["payload = {\n","    \"displayName\": f\"{metadata_db_name}\",\n","    \"type\": \"SQLDatabase\",\n","    \"description\": \"SQL Database to store metadata for Framework\"\n","}\n","\n","sqldb_url = f'{BASE_URL}/workspaces/{workspace_id}/items'\n","response = requests.post(sqldb_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"SQL DB {metadata_db_name} already exists. No change was made.\")\n","else:\n","    time.sleep(300)                    #Sleep timer to wait for SQL DB creation\n","    if response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to create database {metadata_db_name}: {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:27:20.8768286Z","session_start_time":null,"execution_start_time":"2025-04-03T10:27:20.8779713Z","execution_finish_time":"2025-04-03T10:27:21.6334926Z","parent_msg_id":"266d1cd5-7896-4274-88f8-c1e87ca0623e"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SQL DB Meta already exists. No change was made.\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"786c758b-d46c-462c-a5ff-3a9bf6edf8b0"},{"cell_type":"markdown","source":["### 3. Create Data Warehouse\n","#### **Warning:** Verify the Data Warehouse creation before executing the next cell."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"177d070a-a9c4-4beb-8eac-7f2a40374907"},{"cell_type":"code","source":["if warehouse_case_sensitive == True:\n","    warehouse_collation = \"Latin1_General_100_BIN2_UTF8\"\n","else:\n","    warehouse_collation = \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"\n","\n","payload = {\n","    \"displayName\": f\"{warehouse_name}\",\n","    \"type\": \"warehouse\",\n","    \"properties\": {\n","        \"collation\": f\"{warehouse_collation}\"  \n","    }\n","}\n","\n","dw_url = f'{BASE_URL}/workspaces/{workspace_id}/items'\n","response = requests.post(dw_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Warehouse {warehouse_name} already exists. No change was made.\")\n","else:\n","    time.sleep(300)                 #Sleep timer to wait for SQL DB creation\n","    if response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to create warehouse {warehouse_name}: {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:27:23.1685089Z","session_start_time":null,"execution_start_time":"2025-04-03T10:27:23.169746Z","execution_finish_time":"2025-04-03T10:27:23.9218717Z","parent_msg_id":"34ff1179-c6f3-49fb-b912-573a53c72870"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warehouse DW already exists. No change was made.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcc75513-071f-4146-b283-9fddeaa22a6b"},{"cell_type":"markdown","source":["### 4. Upload Notebook Templates"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"954779cc-808c-4fb1-bdbd-94360f997005"},{"cell_type":"code","source":["repo_url = \"https://github.com/ProdataSQL/DWA\"\n","repo_dir = \"DWA_repo\"\n","\n","def clone_git_repo(repo_url, repo_dir):\n","    if os.path.exists(repo_dir):\n","        print(\"Repo already cloned.\")\n","    Repo.clone_from(repo_url, repo_dir)\n","    os.chdir(repo_dir)\n","    \n","clone_git_repo(repo_url, repo_dir)\n","\n","def get_notebooks(directory, pattern):\n","    notebook_files = []\n","    for root, dirs, files in os.walk(directory):\n","        for file in fnmatch.filter(files, pattern):\n","            notebook_files.append(os.path.join(root, file))\n","    return notebook_files\n","\n","def py_to_notebook(py_content, notebook_name):\n","    notebook_content = {\n","        \"nbformat\": 4,\n","        \"nbformat_minor\": 5,\n","        \"cells\": [\n","            {\n","                \"cell_type\": \"code\",\n","                \"source\": [py_content],\n","                \"execution_count\": None,\n","                \"outputs\": []\n","            }\n","        ],\n","        \"metadata\": {\n","            \"language_info\": {\n","                \"name\": \"python\"\n","            }\n","        }\n","    }\n","    return notebook_content\n","\n","def upload_notebook(notebook_name, notebook_content, workspace_id, headers):\n","    notebook_json = json.dumps(notebook_content)\n","    notebook_base64 = base64.b64encode(notebook_json.encode('utf-8')).decode('utf-8')\n","\n","    notebook_url = f\"{BASE_URL}/workspaces/{workspace_id}/notebooks\"\n","    \n","    payload = {\n","        \"displayName\": notebook_name,\n","        \"description\": f\"Imported notebook {notebook_name}\",\n","        \"definition\": {\n","            \"format\": \"ipynb\",\n","            \"parts\": [\n","                {\n","                    \"path\": \"artifact.content.ipynb\",\n","                    \"payload\": notebook_base64,\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    fabric_response = requests.post(\n","        notebook_url,\n","        headers=headers,\n","        data=json.dumps(payload)\n","    )\n","    \n","    if fabric_response.status_code == 400:\n","        print(f\"The {notebook_name} already exists. No changes were made\")\n","    elif fabric_response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to upload {notebook_name}: {fabric_response.status_code} - {fabric_response.text}\")\n","\n","\n","directory = \"Workspaces/DWA/\"\n","pattern = \"*.py\"\n","\n","notebook_files = get_notebooks(directory, pattern)\n","\n","for notebook_file in notebook_files:\n","    directory, file_name = os.path.split(notebook_file)\n","    base_name = os.path.basename(directory)\n","    if base_name.endswith('.Notebook'):\n","        notebook_name = base_name.split('.')[0]\n","    \n","    with open(notebook_file, 'r') as file:\n","        py_content = file.read()\n","    \n","    notebook_content = py_to_notebook(py_content, notebook_name)\n","    \n","    upload_notebook(notebook_name, notebook_content, workspace_id=workspace_id, headers=headers)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:32:04.3214795Z","session_start_time":null,"execution_start_time":"2025-04-03T10:32:04.322805Z","execution_finish_time":"2025-04-03T10:32:08.9640629Z","parent_msg_id":"66d1f530-e987-4a45-89e8-81ef2266e901"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The Extract-CSV already exists. No changes were made\nThe Refresh-Fabric already exists. No changes were made\nThe SharePoint-Shared-Functions already exists. No changes were made\nThe Ingest-SFTP already exists. No changes were made\nThe Extract-SP-Excel already exists. No changes were made\nThe Extract-Dictionary already exists. No changes were made\nThe Extract-XML already exists. No changes were made\nThe Demo - 10 Mins already exists. No changes were made\nThe Extract-CSV-Pandas already exists. No changes were made\nThe Extract-O365-API already exists. No changes were made\nThe Extract-Fabric-Logs already exists. No changes were made\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc8a1f0f-c7c9-4a10-96d9-cbe4ebf02181"},{"cell_type":"markdown","source":["### 5. Create Metadata and Data Warehouse SQL Objects"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb600388-b06e-4f66-bebf-e843454f14b6"},{"cell_type":"code","source":["META_DB_NAME = metadata_db_name\n","DW_NAME = warehouse_name\n","original_dir = os.getcwd()\n","engine_pool = {}\n","\n","class DBType(Enum):\n","    SQLDatabase = \"SQLDatabases\"\n","    Warehouse = \"Warehouses\"\n","\n","    def get_connection_string(self) -> str: \n","        display_name = DW_NAME if self == DBType.Warehouse else META_DB_NAME\n","        client = fabric.FabricRestClient()\n","        endpoint = f\"/v1/workspaces/{workspace_id}/{self.value}\"\n","        databases = client.get(endpoint).json()\n","\n","        selected_database = next((db for db in databases.get(\"value\", []) if db.get(\"displayName\") == display_name), None)\n","        if not selected_database:\n","            raise ValueError(f\"No {self.value} with displayName '{display_name}' found.\")\n","        \n","        server = selected_database['properties'].get('serverFqdn') or selected_database['properties'].get('connectionString')\n","        database = selected_database['properties'].get('databaseName', warehouse_name)\n","            \n","        return f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};database={database};LongAsMax=YES\"\n","\n","    def get_engine(self) -> Engine:\n","        if self not in engine_pool:\n","            token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n","            token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n","            engine_pool[self] = sqlalchemy.create_engine(\n","                \"mssql+pyodbc://\",\n","                creator=lambda: pyodbc.connect(self.get_connection_string(), attrs_before={1256: token_struct})\n","            )\n","        return engine_pool[self]\n","   \n","def read_table_order(file_path: str) -> list:\n","    if not os.path.exists(file_path):\n","        return []\n","    \n","    with open(file_path, \"r\") as f:\n","        return [line.strip() for line in f if line.strip()]\n","\n","def process_sql_script(db_type: DBType, script_path: str):\n","    if not os.path.exists(script_path):\n","        return\n","\n","    with open(script_path, \"r\") as file:\n","        script = file.read()\n","\n","    statements = script.split(\"\\nGO\\n\")\n","    \n","    object_name = script_path.split(\"/\")[-1].split(\".\")[0]\n","    db_name = script_path.split(\"/\")[0].replace(\".\", \" \")\n","    schema_name = script_path.split(\"/\")[1]\n","\n","    if \"Tables/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.tables WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"Views/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.views WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"StoredProcedures/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.procedures WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"Security/\" in script_path:  \n","        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{object_name}'\"\n","    else:\n","        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{schema_name}'\"\n","    \n","    with db_type.get_engine().connect() as conn:\n","        object_exists = conn.execute(text(check_query)).scalar() > 0\n","\n","        if not object_exists:\n","            try:\n","                for statement in filter(None, map(str.strip, statements)):\n","                    conn.execute(text(statement))\n","                conn.commit()\n","            except Exception as e:\n","                conn.rollback()\n","                print(f\"Failed to create {schema_name}.{object_name}: {e}\")\n","                raise\n","        else:\n","            print(f\"The object {schema_name}.{object_name} already exists in {db_name}. No changes made.\")\n","\n","def iterate_sql_objects(db_type: DBType, object_type: str, objects_created: set):\n","    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n","    visited_dirs = set()\n","    \n","    for root, _, files in os.walk(base_path):\n","        if root in visited_dirs:\n","            continue\n","        visited_dirs.add(root)\n","        \n","        for file in filter(lambda f: f.endswith(\".sql\"), files):\n","            script_path = os.path.join(root, file).lstrip(\"./\")\n","           \n","            if script_path in objects_created:\n","                continue\n","\n","            parts = script_path.split(os.sep)\n","            if object_type == \"Schema\":\n","                if any(folder in parts for folder in [\"Tables\", \"Views\", \"StoredProcedures\"]):\n","                    continue  \n","            else:\n","                if object_type not in parts:\n","                    continue  \n","\n","            process_sql_script(db_type, script_path)\n","            objects_created.add(script_path)\n","\n","def process_sql_objects(db_type: DBType, obj_type: str = None):\n","    table_orders_cache = {}\n","    has_changed_directory = False \n","    current_dir = os.path.abspath(os.getcwd())\n","\n","    table_order_files = {\n","        DBType.SQLDatabase: os.path.join(current_dir, \"Setup\", \"Files\", \"SQLDatabase\", \"MetaTableOrder.txt\"),\n","        DBType.Warehouse: os.path.join(current_dir, \"Setup\", \"Files\", \"Warehouse\", \"DWTableOrder.txt\")\n","    }\n","    \n","    for db_type_key, file_path in table_order_files.items():        \n","        if os.path.exists(file_path):\n","            table_orders_cache[db_type_key] = read_table_order(file_path)\n","\n","    table_order = table_orders_cache.get(db_type, [])\n","\n","    if not has_changed_directory:\n","        workspaces_dwa_path = os.path.join(current_dir, \"Workspaces\", \"DWA\")\n","        \n","        if os.path.exists(workspaces_dwa_path):\n","            os.chdir(workspaces_dwa_path)\n","            has_changed_directory = True \n","    \n","    objects_created = set()\n","    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n","    \n","    for table in table_order:\n","        schema = table.split(\".\", maxsplit=1)[0].strip(\"[]\")\n","        table_name = table.split(\".\", maxsplit=1)[1].strip(\"[]\")\n","        script_path = f\"{base_path}/{schema}/{obj_type}/{table_name}.sql\"\n","            \n","        if os.path.exists(script_path) and script_path not in objects_created:\n","            process_sql_script(db_type, script_path)\n","            objects_created.add(script_path)\n","    \n","    iterate_sql_objects(db_type, obj_type, objects_created)\n","    os.chdir(original_dir)\n","\n","#Create meta databse objects\n","process_sql_objects(DBType.SQLDatabase, \"Security\")\n","process_sql_objects(DBType.SQLDatabase, \"Tables\")\n","process_sql_objects(DBType.SQLDatabase, \"Views\")\n","process_sql_objects(DBType.SQLDatabase, \"StoredProcedures\") \n","\n","#Create warehouse objects\n","process_sql_objects(DBType.Warehouse, \"Schema\")\n","process_sql_objects(DBType.Warehouse, \"StoredProcedures\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:33:18.5204552Z","session_start_time":null,"execution_start_time":"2025-04-03T10:33:18.5219579Z","execution_finish_time":"2025-04-03T10:33:26.3073614Z","parent_msg_id":"e41842b1-eefb-49d8-9bfc-f779262f4dbc"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The object Security.audit already exists in Meta SQLDatabase. No changes made.\nThe object Security.config already exists in Meta SQLDatabase. No changes made.\nThe object Security.devops already exists in Meta SQLDatabase. No changes made.\nThe object dbo.dict_artefacts already exists in Meta SQLDatabase. No changes made.\nThe object config.Configurations already exists in Meta SQLDatabase. No changes made.\nThe object config.Datasets already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTables already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTableJoins already exists in Meta SQLDatabase. No changes made.\nThe object config.Templates already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroups already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupLinks already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupTables already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineGroups already exists in Meta SQLDatabase. No changes made.\nThe object config.Pipelines already exists in Meta SQLDatabase. No changes made.\nThe object config.DatasetLineage already exists in Meta SQLDatabase. No changes made.\nThe object audit.PipelineLog already exists in Meta SQLDatabase. No changes made.\nThe object audit.LoadLog already exists in Meta SQLDatabase. No changes made.\nThe object audit.ReleaseLog already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineMeta already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineMetaCache already exists in Meta SQLDatabase. No changes made.\n"]}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de3b1a99-895a-4e74-8a0a-32132f617a34"},{"cell_type":"markdown","source":["### 6. Upload Data Pipelines"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da333f14-9428-48c3-aead-65c5d0bf1117"},{"cell_type":"code","source":["# Stores new references for updating old data pipelines connections \n","lh_artifact_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", workspace_id) #Lakehouse ID\n","\n","meta_artifact_id = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", workspace_id) #SQL DB ID and Connection String\n","\n","sqldb_url = f'{BASE_URL}/workspaces/{workspace_id}/SQLDatabases/{meta_artifact_id}'\n","response = requests.get(sqldb_url, headers=headers)\n","\n","meta_endpoint = response.json().get(\"properties\", {}).get(\"serverFqdn\")\n","meta_databasename = response.json().get(\"properties\", {}).get(\"databaseName\")\n","\n","dw_artifact_id = fabric.resolve_item_id(warehouse_name, \"Warehouse\", workspace_id) #Warehouse ID and Connection String\n","\n","dw_url = f'{BASE_URL}/workspaces/{workspace_id}/warehouses/{dw_artifact_id}'\n","response = requests.get(dw_url, headers=headers)\n","\n","dw_endpoint = response.json().get(\"properties\", {}).get(\"connectionString\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:34:07.3743309Z","session_start_time":null,"execution_start_time":"2025-04-03T10:34:07.3755651Z","execution_finish_time":"2025-04-03T10:34:08.8757744Z","parent_msg_id":"ed2fdd3d-0df4-4491-8265-3e8c26df20d0"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 20, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eee6400d-7f6f-4b5f-a5a0-3b5433065a31"},{"cell_type":"code","source":["access_token = mssparkutils.credentials.getToken('pbi') #Rebuild access token \n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","class DataPipeline:\n","    pass\n","\n","class DataPipeline:\n","    name: str\n","    git_id: str\n","    real_id: str\n","    raw_definition: str\n","    definition: object\n","    pipeline_created: bool = False\n","\n","    def __init__(self, name, git_id, raw_definition, definition):\n","        self.name = name\n","        self.git_id = git_id\n","        self.definition = definition\n","        self.raw_definition = raw_definition\n","    def __hash__(self) -> int:\n","        return hash(self.git_id)\n","    def __eq__(self, other):\n","        return self.git_id == other.git_id\n","\n","    def __str__(self):\n","        return f\"DataPipeline: {self.name}\"\n","\n","    def __repr__(self):\n","        return f\"DataPipeline: {self.name}\"\n","\n","    def is_created(self) -> bool:\n","        return self in pipelines_created or self.pipeline_created\n","\n","    def create(self, pipelines):\n","        global pipelines_created\n","        if self.is_created():\n","            print(f\"{self.name} already created\")\n","            return\n","\n","        print(f\"Creating pipeline {self.name}\")\n","\n","        child_pipelines: list[DataPipeline] = self.get_child_pipelines(pipelines)\n","\n","        for child_pipeline in child_pipelines:\n","            if child_pipeline.is_created():\n","                continue\n","            child_pipeline.create(pipelines)\n","\n","        self.update_activity_references()\n","        self.upload_pipeline()\n","\n","        print(f\"{self.name} created and uploaded.\")\n","\n","    def get_child_pipelines(self, pipelines : list[DataPipeline]) -> list[DataPipeline]:\n","        self.child_pipelines: set[DataPipeline] = set()\n","\n","        def find_pipelines(activities : list [object]):\n","            for activity in activities:\n","                activity_type = activity.get(\"type\")\n","                type_properties = activity.get(\"typeProperties\", {})\n","\n","                if activity_type == \"ExecutePipeline\":\n","                    reference_name = type_properties.get(\"pipeline\", {}).get(\n","                        \"referenceName\"\n","                    )\n","                    matched_pipeline = next(\n","                        (x for x in pipelines if x.git_id == reference_name), None\n","                    )\n","                    if matched_pipeline:\n","                        self.child_pipelines.add(matched_pipeline)\n","\n","                elif activity_type == \"InvokePipeline\":\n","                    pipeline_id = type_properties.get(\"pipelineId\")\n","                    matched_pipeline = next(\n","                        (x for x in pipelines if x.git_id == pipeline_id), None\n","                    )\n","                    if matched_pipeline:\n","                        self.child_pipelines.add(matched_pipeline)\n","                    \n","                else:\n","                    for key in [\"activities\", \"ifTrueActivities\", \"ifFalseActivities\"]:\n","                        if key in type_properties:\n","                            find_pipelines(type_properties[key])\n","            return self.child_pipelines\n","\n","        return find_pipelines(\n","            self.definition.get(\"properties\", {}).get(\"activities\", [])\n","        )\n","\n","    def update_activity_references(self):\n","        global pipelines_created\n","\n","        def fetch_trident_notebooks():\n","            notebooks_url = f\"{BASE_URL}/workspaces/{workspace_id}/notebooks\"\n","            response = requests.get(notebooks_url, headers=headers)\n","            response.raise_for_status()\n","            return {notebook[\"displayName\"]: notebook[\"id\"] for notebook in response.json().get(\"value\", [])}\n","\n","        trident_notebooks = fetch_trident_notebooks()\n","\n","        new_definition = self.raw_definition\n","        for pipeline in pipelines_created: \n","            new_definition = new_definition.replace(pipeline.git_id, pipeline.real_id)\n","\n","        definition_json = json.loads(new_definition)\n","\n","        def replace_values(obj):\n","            if isinstance(obj, dict):\n","                for key, value in obj.items():\n","                    if key == \"type\" and isinstance(value, str):\n","                        if \"Lakehouse\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict) :\n","                                if \"artifactId\" in obj[\"typeProperties\"]  and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = lh_artifact_id  \n","\n","                        elif \"DataWarehouse\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n","                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = dw_artifact_id \n","                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n","                                obj[\"endpoint\"] = dw_endpoint \n","\n","                        elif \"FabricSqlDatabase\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n","                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = meta_artifact_id  \n","                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n","                                obj[\"endpoint\"] = meta_endpoint  \n","\n","                        elif \"TridentNotebook\" in value:\n","                            notebook_name = obj.get(\"name\")\n","                            if notebook_name in trident_notebooks:\n","                                obj[\"typeProperties\"][\"notebookId\"] = trident_notebooks[notebook_name]\n","\n","                    elif key == \"workspaceId\" and isinstance(value, str) and not value.startswith(\"@\"):\n","                        obj[key] = workspace_id  \n","\n","                    replace_values(value)\n","\n","            elif isinstance(obj, list):\n","                for item in obj:\n","                    replace_values(item)\n","\n","        replace_values(definition_json)\n","\n","        self.raw_definition = json.dumps(definition_json, indent=4)\n","\n","    def upload_pipeline(self):\n","        global pipelines_created\n","        pipelines_url = f\"{BASE_URL}/workspaces/{workspace_id}/dataPipelines\"\n","        pipeline_b64 = base64.b64encode(self.raw_definition.encode()).decode()\n","\n","        payload = {\n","            \"displayName\": self.name,\n","            \"definition\": {\n","                \"parts\": [\n","                    {\n","                        \"path\": \"pipeline.content.json\",\n","                        \"payload\": pipeline_b64,\n","                        \"payloadType\": \"InlineBase64\",\n","                    }\n","                ]\n","            },\n","        }\n","\n","        create_pipeline_request = requests.post(pipelines_url, headers=headers, data=json.dumps(payload))\n","        if not create_pipeline_request.ok:\n","            print(create_pipeline_request.json())\n","            raise RuntimeError(f\"Failed to create pipeline {self.name}. ({create_pipeline_request.text})\")\n","        \n","        self.real_id = create_pipeline_request.json().get(\"id\")\n","        self.pipeline_created = True\n","        pipelines_created.append(self)\n","    \n","pipelines: list[DataPipeline] = []\n","\n","for path, dirs, files in os.walk(\".\"):\n","    if not path.endswith(\".DataPipeline\"):\n","        continue\n","\n","    pipeline_name = path.strip(\".\").split(\".\")[0].replace(\"\\\\\", \"/\").split(\"/\")[-1]\n","    platform = json.load(open(os.path.join(path, \".platform\")))\n","    raw_definition = open(os.path.join(path, \"pipeline-content.json\")).read()\n","    definition = json.loads(raw_definition)\n","    pipeline_id = platform[\"config\"][\"logicalId\"]\n","\n","    data_pipeline = DataPipeline(pipeline_name, pipeline_id, raw_definition, definition)\n","    pipelines.append(data_pipeline)\n","\n","pipelines_url = f\"{BASE_URL}/workspaces/{workspace_id}/dataPipelines\"\n","response = requests.get(pipelines_url, headers=headers)\n","response.raise_for_status()\n","\n","pipelines_created: list[DataPipeline] = []\n","\n","pipelines_existing: list[(str, str)] = [\n","    (pipeline[\"displayName\"], pipeline[\"id\"])\n","    for pipeline in response.json().get(\"value\", [])\n","]\n","\n","for pipeline_name, pipeline_id in pipelines_existing:\n","    pipeline = next((x for x in pipelines if x.name == pipeline_name), None)\n","    if not pipeline:\n","        continue\n","\n","    pipeline.real_id = pipeline_id\n","    pipeline.pipeline_created = True\n","    pipelines_created.append(pipeline)\n","\n","for pipeline in pipelines:\n","    if pipeline in pipelines_created or pipeline.pipeline_created:\n","        continue\n","    pipeline.create(pipelines)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:34:10.8444281Z","session_start_time":null,"execution_start_time":"2025-04-03T10:34:10.846144Z","execution_finish_time":"2025-04-03T10:35:16.6966821Z","parent_msg_id":"aed11213-f0c9-4a10-adbf-42ff3a2c7888"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 21, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating pipeline TR_ConfigBackup\nCreating pipeline Backup_ConfigSchema\nCreating pipeline Environment-Variables\nEnvironment-Variables created and uploaded.\nBackup_ConfigSchema created and uploaded.\nTR_ConfigBackup created and uploaded.\nCreating pipeline Copy-Meta\nCopy-Meta created and uploaded.\nCreating pipeline Copy-SFTP-Prodata\nCopy-SFTP-Prodata created and uploaded.\nCreating pipeline TR_Ops\nTR_Ops created and uploaded.\nCreating pipeline Pipeline-Worker\nPipeline-Worker created and uploaded.\nCreating pipeline Pipeline-Controller\nPipeline-Controller created and uploaded.\nCreating pipeline Restore_ConfigSchema\nRestore_ConfigSchema created and uploaded.\nCreating pipeline TR_AdventureWorks\nTR_AdventureWorks created and uploaded.\n"]}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19d74a8c-843a-4ba4-bcc8-f55a28ebf92f"},{"cell_type":"markdown","source":["### 7. Create objects for Adventure Works when deploy_aw is set to True"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acc1cdcb-2625-449d-9a11-c903889ec341"},{"cell_type":"code","source":["if deploy_aw == False:\n","    mssparkutils.notebook.exit(1)\n","\n","class CustomTokenCredential:\n","    def get_token(self, *scopes, **kwargs):\n","        return AccessToken(notebookutils.credentials.getToken('storage'), expires_on=9999999999)\n","\n","credential = CustomTokenCredential()\n","service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n","fs = service_client.get_file_system_client(workspace_name)\n","\n","lh_paths = {\n","    \"tmp/landing/\": f\"{lakehouse_name}.Lakehouse/Files/landing\",\n","    \"tmp/Tables/\": f\"{lakehouse_name}.Lakehouse/Tables\"\n","}\n","\n","def unpack_files(Setup_dir, archives):\n","    clone_git_repo(repo_url, repo_dir)\n","    for archive, target in archives.items():\n","        shutil.unpack_archive(os.path.join(Setup_dir, archive), target, \"zip\")\n","\n","def upload_files(local_path, azure_path):\n","    for root, _, files in os.walk(local_path):\n","        for file in files:\n","            file_path_on_local = os.path.join(root, file)\n","            relative_path = os.path.relpath(root, local_path)\n","            file_path_on_azure = os.path.join(azure_path, relative_path, file).replace(\"\\\\\", \"/\")\n","            file_client = fs.get_file_client(file_path_on_azure)\n","            with open(file_path_on_local, \"rb\") as data:\n","                file_client.upload_data(data, overwrite=True)\n","\n","git_lh_directory = \"Setup/Files/Lakehouse\"\n","archives = {\n","    'AW_landing.zip': \"tmp/landing\",\n","    'AW_tables.zip': \"tmp/Tables\"\n","}\n","\n","unpack_files(git_lh_directory, archives)\n","for local, azure in lh_paths.items():\n","    upload_files(local, azure)\n","\n","#Deploy AW Warehouse Objects \n","process_sql_objects(DBType.Warehouse, \"Tables\")\n","process_sql_objects(DBType.Warehouse, \"Views\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"0b18a74d-7148-4609-86a1-cdda146cc5b1","normalized_state":"finished","queued_time":"2025-04-03T10:41:19.6963256Z","session_start_time":null,"execution_start_time":"2025-04-03T10:41:19.6975795Z","execution_finish_time":"2025-04-03T10:41:33.6492236Z","parent_msg_id":"34224ba6-6066-481d-a388-1a9cf1f6bdfd"},"text/plain":"StatementMeta(, 0b18a74d-7148-4609-86a1-cdda146cc5b1, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The object aw.DimCurrency already exists in DW Warehouse. No changes made.\nThe object aw.DimDepartmentGroup already exists in DW Warehouse. No changes made.\nThe object aw.DimScenario already exists in DW Warehouse. No changes made.\nThe object aw.DimDate already exists in DW Warehouse. No changes made.\nThe object aw.ReportAccountMap already exists in DW Warehouse. No changes made.\nThe object aw.FactFinance already exists in DW Warehouse. No changes made.\nThe object aw.DimAccount already exists in DW Warehouse. No changes made.\nThe object aw.DimOrganization already exists in DW Warehouse. No changes made.\n"]}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e775b-7c60-4d73-a9d7-d6bd77673d28"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"warehouse":{}}},"nbformat":4,"nbformat_minor":5}