{"cells":[{"cell_type":"markdown","source":["# **Data Warehouse Automation (DWA) Setup in Microsoft Fabric**\n","\n","- Modify the parameter values as needed.\n","- Execute each cell sequentially to ensure proper setup.\n","- Manually verify the created objects within the Fabric Workspace."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9a9979c7-719f-4bf4-9e63-f8a6aaf6ac67"},{"cell_type":"code","source":["lakehouse_name = \"LH\"            # Lakehouse name e.g. SupplychainBronze, FinanceLH\n","warehouse_name = \"DW\"            # Warehouse name e.g. OperationsGold, FinanceDW\n","metadata_db_name = \"Meta\"        # SQL Database name to store metadata information for the framework\n","lakehouse_schema_enabled = True  # If False then lakehouse and warehouse objects will need to be created manually\n","warehouse_case_sensitive = False # To make data warehouse case sensitive, default is False\n","deploy_aw = True                 # To deploy AdventureWorks files and objects set to True\n","branch= \"dev\"                    # main or dev for experimental release\n","\n","fabricsqldb_connection_name = \"Meta-DWA-Training\"               #Connection name for Fabric SQL Database shareable cloud\n","fabricdatapipelines_connection_name = \"Pipelines-DWA-Training\"  #Connection name for Fabric Data Pipelines shareable cloud\n","fabrricsql_connection_name =\"\"                                  #Connection name for SQL Server Pipelines (leave blank to skip)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"finished","queued_time":"2025-07-12T21:43:53.9334423Z","session_start_time":"2025-07-12T21:43:53.9345068Z","execution_start_time":"2025-07-12T21:44:03.6695417Z","execution_finish_time":"2025-07-12T21:44:04.0232794Z","parent_msg_id":"24135885-337b-4e9a-9e30-85454c305e4c"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"19abb929-7fea-4ae9-bb22-f9026054affe"},{"cell_type":"markdown","source":["#### Import of needed libraries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0e0e17fa-939d-433c-87be-af7c49a0b07c"},{"cell_type":"code","source":["import struct\n","import sqlalchemy\n","from sqlalchemy.sql import text\n","from notebookutils import mssparkutils\n","import sempy.fabric as fabric\n","import sempy.fabric as sf\n","import base64\n","from azure.core.credentials import AccessToken\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import DefaultAzureCredential\n","import os\n","import pyodbc\n","import shutil\n","from git import Repo\n","import requests\n","import json\n","import fnmatch\n","import time\n","from enum import Enum\n","from sqlalchemy.engine import Engine"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"finished","queued_time":"2025-07-12T21:43:54.8016421Z","session_start_time":null,"execution_start_time":"2025-07-12T21:44:04.0254208Z","execution_finish_time":"2025-07-12T21:44:10.1468001Z","parent_msg_id":"67bebf88-becf-4562-8482-1c3f73d1f10c"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee02c50f-f044-4ad4-81c3-d1afaa40738e"},{"cell_type":"markdown","source":["##### Setting up global parameters \n","Also Clone GIT repos"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"63fbf6f1-4603-4634-a9fc-222098ce0ca6"},{"cell_type":"code","source":["WORKSPACE_ID = fabric.get_workspace_id()\n","WORKSPACE_NAME = fabric.resolve_workspace_name()\n","repo_url = \"https://github.com/ProdataSQL/DWA\"\n","repo_dir = \"DWA_repo\"\n","\n","BASE_URL = \"https://api.fabric.microsoft.com/v1\"\n","access_token = notebookutils.credentials.getToken('pbi')\n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","\n","#Clone Repo into local storage\n","if os.path.exists(repo_dir):\n","    shutil.rmtree(repo_dir)\n","Repo.clone_from(url=repo_url, to_path=repo_dir,branch=branch, single_branch=True)\n","os.chdir(repo_dir)\n","\n","\n","            \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"finished","queued_time":"2025-07-12T21:43:56.9518188Z","session_start_time":null,"execution_start_time":"2025-07-12T21:44:10.1495264Z","execution_finish_time":"2025-07-12T21:44:17.8326305Z","parent_msg_id":"68679593-9827-48c8-b3ba-a8f37a38c33c"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1d0a5c9-1d97-4597-9dc1-c9aa6e7bd49a"},{"cell_type":"markdown","source":["### 1. Create Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecfe6963-fe87-4597-93cb-9bbd40254978"},{"cell_type":"code","source":["lakehouse_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/lakehouses\"\n","payload = {\n","    \"displayName\": f\"{lakehouse_name}\", \n","    \"description\": \"A schema-enabled lakehouse.\",\n","    \"creationPayload\": {\"enableSchemas\": f\"{lakehouse_schema_enabled}\"} \n","}\n","response = requests.post(lakehouse_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Lakehouse {lakehouse_name} already exists. No change was made.\")\n","elif response.status_code != 201:\n","    raise RuntimeError(f\"Failed to create Lakehouse {lakehouse_name} : {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"a09a661f-d704-4899-8869-46fbf1fb5cd3","normalized_state":"finished","queued_time":"2025-07-12T21:27:17.4409252Z","session_start_time":null,"execution_start_time":"2025-07-12T21:27:40.847418Z","execution_finish_time":"2025-07-12T21:27:41.6033448Z","parent_msg_id":"40248d38-7dd3-46f6-b004-d5c5c51ee59f"},"text/plain":"StatementMeta(, a09a661f-d704-4899-8869-46fbf1fb5cd3, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Lakehouse LH already exists. No change was made.\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa181b62-1c13-45fd-909d-c415eda39a1e"},{"cell_type":"markdown","source":["### 2. Create Metadata SQL Database\n","#### **Warning:** Verify the SQL DB creation before executing the next cell."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e66b8de5-1aee-4278-ad8d-f3baf2a2554b"},{"cell_type":"code","source":["payload = {\n","    \"displayName\": f\"{metadata_db_name}\",\n","    \"type\": \"SQLDatabase\",\n","    \"description\": \"SQL Database to store metadata for Framework\"\n","}\n","\n","sqldb_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/items'\n","response = requests.post(sqldb_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"SQL DB {metadata_db_name} already exists. No change was made.\")\n","else:\n","    start_time = time.time() \n","    max_wait_time = 600  \n","    check_interval = 60\n","    \n","    while time.time() - start_time < max_wait_time: #Sleep timer to wait for SQL DB creation\n","        time.sleep(check_interval)\n","        db_exists = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", WORKSPACE_ID)   \n","\n","        if db_exists:\n","            break\n","    else: \n","        raise RuntimeError(f\"Failed to create database {metadata_db_name} within the timeout period.\")\n","    \n","    if response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to create database {metadata_db_name}: {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"a09a661f-d704-4899-8869-46fbf1fb5cd3","normalized_state":"finished","queued_time":"2025-07-12T21:27:17.4424289Z","session_start_time":null,"execution_start_time":"2025-07-12T21:27:41.6055815Z","execution_finish_time":"2025-07-12T21:27:42.3949842Z","parent_msg_id":"623a8d60-a6af-4ef9-a1d1-b05ccbc02810"},"text/plain":"StatementMeta(, a09a661f-d704-4899-8869-46fbf1fb5cd3, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SQL DB Meta already exists. No change was made.\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"786c758b-d46c-462c-a5ff-3a9bf6edf8b0"},{"cell_type":"markdown","source":["### 3. Create Data Warehouse\n","#### **Warning:** Verify the Data Warehouse creation before executing the next cell."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"177d070a-a9c4-4beb-8eac-7f2a40374907"},{"cell_type":"code","source":["if warehouse_case_sensitive == True:\n","    warehouse_collation = \"Latin1_General_100_BIN2_UTF8\"\n","else:\n","    warehouse_collation = \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"\n","\n","payload = {\n","    \"displayName\": f\"{warehouse_name}\",\n","    \"type\": \"warehouse\",\n","    \"properties\": {\n","        \"collation\": f\"{warehouse_collation}\"  \n","    }\n","}\n","\n","dw_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/items'\n","response = requests.post(dw_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Warehouse {warehouse_name} already exists. No change was made.\")\n","else:\n","    start_time = time.time() \n","    max_wait_time = 600  \n","    check_interval = 60\n","    \n","    while time.time() - start_time < max_wait_time: #Sleep timer to wait for SQL DB creation\n","        time.sleep(check_interval)\n","        dw_exists = fabric.resolve_item_id(warehouse_name, \"Warehouse\", WORKSPACE_ID)   \n","\n","        if dw_exists:\n","            break\n","    else: \n","        raise RuntimeError(f\"Failed to create warehouse {warehouse_name} within the timeout period.\")\n","    \n","    if response.status_code not in [200, 201, 202]:\n","        raise RuntimeError(f\"Failed to create warehouse {warehouse_name}: {response.status_code}, {response.text}\")        "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"a09a661f-d704-4899-8869-46fbf1fb5cd3","normalized_state":"finished","queued_time":"2025-07-12T21:27:17.4440507Z","session_start_time":null,"execution_start_time":"2025-07-12T21:27:42.3972197Z","execution_finish_time":"2025-07-12T21:27:43.180863Z","parent_msg_id":"c71646f0-ee41-45a4-aef5-206b975b1348"},"text/plain":"StatementMeta(, a09a661f-d704-4899-8869-46fbf1fb5cd3, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warehouse DW already exists. No change was made.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcc75513-071f-4146-b283-9fddeaa22a6b"},{"cell_type":"markdown","source":["### 4. Upload Notebook Templates"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"954779cc-808c-4fb1-bdbd-94360f997005"},{"cell_type":"code","source":["import sempy.fabric as sf\n","lh_artifact_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", WORKSPACE_ID)\n","\n","def get_notebooks(directory, pattern):\n","    notebook_files = []\n","    for root, dirs, files in os.walk(directory):\n","        for file in fnmatch.filter(files, pattern):\n","            notebook_files.append(os.path.join(root, file))\n","    return notebook_files\n","\n","import json\n","\n","def parse_python_to_cells(py_content):\n","    cells = []\n","    current_cell_lines = []\n","    cell_type = \"code\"\n","    is_param_cell = False\n","    in_metadata_block = False\n","    skipped_first_comment = False\n","    metadata_lines = []\n","    is_default_lakehouse = False  \n","\n","    def add_cell():\n","        if not current_cell_lines:\n","            return\n","        metadata = {}\n","        if is_param_cell:\n","            metadata = {\n","                \"microsoft\": {\n","                    \"language\": \"python\",\n","                    \"language_group\": \"synapse_pyspark\"\n","                },\n","                \"tags\": [\"parameters\"],\n","            }\n","        cells.append({\n","            \"cell_type\": cell_type,\n","            \"metadata\": metadata,\n","            \"source\": current_cell_lines.copy(),\n","            \"execution_count\": None,\n","            \"outputs\": [] if cell_type == \"code\" else None\n","        })\n","        current_cell_lines.clear()\n","\n","    for line in py_content.splitlines(keepends=True):\n","        stripped = line.strip()\n","        if not skipped_first_comment and stripped.startswith(\"#\"):\n","            skipped_first_comment = True\n","            continue\n","        if stripped.startswith(\"# META\"):\n","            in_metadata_block = True\n","            metadata_lines.append(stripped[6:] if stripped.startswith(\"# META \") else \"\")\n","            continue\n","        elif in_metadata_block:\n","            if not stripped.startswith(\"#\") or stripped == \"\":\n","                in_metadata_block = False\n","                metadata_str = \"\\n\".join(metadata_lines)\n","                try:\n","                    meta_json = json.loads(metadata_str)\n","                    dependencies = meta_json.get(\"dependencies\", {})\n","                    lakehouse = dependencies.get(\"lakehouse\", {})\n","                    if lakehouse.get(\"default_lakehouse_name\"):\n","                        is_default_lakehouse = True\n","                except json.JSONDecodeError:\n","                    pass\n","                metadata_lines.clear()\n","            else:\n","                metadata_lines.append(stripped[6:] if stripped.startswith(\"# META \") else \"\")\n","            continue\n","        if \"# PARAMETERS CELL\" in stripped:\n","            add_cell()\n","            cell_type = \"code\"\n","            is_param_cell = True\n","            continue\n","        elif \"# CELL\" in stripped:\n","            add_cell()\n","            cell_type = \"code\"\n","            is_param_cell = False\n","            continue\n","        elif \"# MARKDOWN\" in stripped or stripped.startswith(\"# %% [markdown]\"):\n","            add_cell()\n","            cell_type = \"markdown\"\n","            is_param_cell = False\n","            continue\n","        if stripped:\n","            current_cell_lines.append(line)\n","    add_cell()\n","    return cells, is_default_lakehouse\n","\n","def py_to_notebook(py_content, notebook_name):\n","    notebook_cells, is_default_lakehouse = parse_python_to_cells(py_content)\n","    metadata = {\n","        \"language_info\": {\n","            \"name\": \"python\",\n","            \"language_group\": \"synapse_pyspark\"\n","        }\n","    }\n","    if is_default_lakehouse:\n","        metadata[\"dependencies\"] = {\n","            \"lakehouse\": {\n","                \"default_lakehouse\": lh_artifact_id,\n","                \"default_lakehouse_name\": lakehouse_name,\n","                \"default_lakehouse_workspace_id\": WORKSPACE_ID\n","            }\n","        }\n","    notebook_content = {\n","        \"nbformat\": 4,\n","        \"nbformat_minor\": 5,\n","        \"cells\": notebook_cells,\n","        \"metadata\": metadata\n","    }\n","    return notebook_content\n","\n","def create_folder(folder_name: str, parent_folder_id: str=None, workspace_id: str = None):\n","    if not workspace_id:\n","        workspace_id = sf.get_notebook_workspace_id()\n","    client = fabric.FabricRestClient()\n","    response = client.get( f\"v1/workspaces/{workspace_id}/folders\").json()\n","    folders = response.get(\"value\", [])\n","    folder_id=next((f[\"id\"] for f in folders if f[\"displayName\"].lower() ==  folder_name.lower()), None)\n","    if folder_id:\n","            return folder_id\n","    else:\n","        client = fabric.FabricRestClient()\n","        if parent_folder_id:\n","            json={   \n","                \"displayName\": folder_name,\n","                 \"parentFolderId\": parent_folder_id\n","                \n","            }\n","        else:\n","            json={   \n","                \"displayName\": folder_name,\n","            }       \n","        response = client.post(f\"v1/workspaces/{workspace_id}/folders\",json=json).json()\n","        return response['id']\n","\n","def upload_notebook(notebook_name, notebook_content, folder_id):\n","    notebook_json = json.dumps(notebook_content)\n","    notebook_base64 = base64.b64encode(notebook_json.encode('utf-8')).decode('utf-8')\n","    notebook_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/notebooks\"\n","    payload = {\n","        \"displayName\": notebook_name,\n","        \"type\":\"Notebook\",\n","        \"folderId\": folder_id,\n","        \"definition\": {\n","            \"format\": \"ipynb\",\n","            \"parts\": [\n","                {\n","                    \"path\": \"artifact.content.ipynb\",\n","                    \"payload\": notebook_base64,\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","    fabric_response = requests.post(\n","        notebook_url,\n","        headers=headers,\n","        data=json.dumps(payload)\n","    )\n","    if fabric_response.status_code == 400:\n","        print(f\"The {notebook_name} already exists. No changes were made\")\n","    elif fabric_response.status_code not in [200,201,202]:\n","        raise RuntimeError(f\"Failed to upload {notebook_name}: {fabric_response.status_code} - {fabric_response.text}\")\n","\n","directory = \"Workspaces/DWA/\"\n","pattern = \"*.py\"\n","notebook_files = get_notebooks(directory, pattern)\n","folder_cache = {}\n","i=0\n","\n","for notebook_file in notebook_files:\n","    i+=1\n","    parent_folder_id=None\n","    directory, file_name = os.path.split(notebook_file)\n","    path = \"/\".join( directory.split(\"/\")[2:-1])\n","    if path in folder_cache:\n","        parent_folder_id=folder_cache[path]\n","    if not parent_folder_id:\n","        for folder in path.split('/'):\n","            parent_folder_id=create_folder(folder, parent_folder_id)\n","        folder_cache[path] = parent_folder_id\n","    base_name = os.path.basename(directory)\n","    if base_name.endswith('.Notebook'):\n","        notebook_name = base_name.split('.')[0]\n","    with open(notebook_file, 'r') as file:\n","        py_content = file.read()\n","    notebook_content = py_to_notebook(py_content, notebook_name)\n","    upload_notebook(notebook_name, notebook_content, parent_folder_id)\n","   "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"a09a661f-d704-4899-8869-46fbf1fb5cd3","normalized_state":"finished","queued_time":"2025-07-12T21:27:17.4456268Z","session_start_time":null,"execution_start_time":"2025-07-12T21:27:43.1835519Z","execution_finish_time":"2025-07-12T21:28:49.1946656Z","parent_msg_id":"cbf17c19-fa79-412a-8545-d1f91a5dd2db"},"text/plain":"StatementMeta(, a09a661f-d704-4899-8869-46fbf1fb5cd3, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The Export-SQL already exists. No changes were made\nThe Export-CSV already exists. No changes were made\nThe Export-SP already exists. No changes were made\nThe Export-Parquet already exists. No changes were made\nThe Export-Excel already exists. No changes were made\nThe Refresh-Fabric already exists. No changes were made\nThe Ingest-SP already exists. No changes were made\nThe Ingest-SFTP already exists. No changes were made\nThe Copy-Blob already exists. No changes were made\nThe MD-Sync Python already exists. No changes were made\nThe SQL-Connection-Shared-Functions already exists. No changes were made\nThe SharePoint-Shared-Functions already exists. No changes were made\nThe MD-Sync already exists. No changes were made\nThe Extract-JSON already exists. No changes were made\nThe Extract-Excel already exists. No changes were made\nThe Extract-SP-Excel already exists. No changes were made\nThe Extract-O365-API already exists. No changes were made\nThe Extract-CSV already exists. No changes were made\nThe Extract-CSV-Pandas already exists. No changes were made\nThe Extract-XML already exists. No changes were made\nThe ArchiveFiles already exists. No changes were made\nThe Extract-Dictionary already exists. No changes were made\nThe Extract-Artefacts already exists. No changes were made\nThe Extract-Fabric-Logs already exists. No changes were made\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc8a1f0f-c7c9-4a10-96d9-cbe4ebf02181"},{"cell_type":"markdown","source":["### 5. Create Metadata and Data Warehouse SQL Objects"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb600388-b06e-4f66-bebf-e843454f14b6"},{"cell_type":"code","source":["META_DB_NAME = metadata_db_name\n","DW_NAME = warehouse_name\n","original_dir = os.getcwd()\n","engine_pool = {}\n","\n","class DBType(Enum):\n","    SQLDatabase = \"SQLDatabases\"\n","    Warehouse = \"Warehouses\"\n","\n","    def get_connection_string(self) -> str: \n","        display_name = DW_NAME if self == DBType.Warehouse else META_DB_NAME\n","        client = fabric.FabricRestClient()\n","        endpoint = f\"/v1/workspaces/{WORKSPACE_ID}/{self.value}\"\n","        databases = client.get(endpoint).json()\n","\n","        selected_database = next((db for db in databases.get(\"value\", []) if db.get(\"displayName\") == display_name), None)\n","        if not selected_database:\n","            raise ValueError(f\"No {self.value} with displayName '{display_name}' found.\")\n","        \n","        server = selected_database['properties'].get('serverFqdn') or selected_database['properties'].get('connectionString')\n","        database = selected_database['properties'].get('databaseName', warehouse_name)\n","            \n","        return f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};database={database};LongAsMax=YES\"\n","\n","    def get_engine(self) -> Engine:\n","        if self not in engine_pool:\n","            token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n","            token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n","            engine_pool[self] = sqlalchemy.create_engine(\n","                \"mssql+pyodbc://\",\n","                creator=lambda: pyodbc.connect(self.get_connection_string(), attrs_before={1256: token_struct})\n","            )\n","        return engine_pool[self]\n","   \n","def read_table_order(file_path: str) -> list:\n","    if not os.path.exists(file_path):\n","        return []\n","    \n","    with open(file_path, \"r\") as f:\n","        return [line.strip() for line in f if line.strip()]\n","\n","def process_sql_script(db_type: DBType, script_path: str):\n","    if not os.path.exists(script_path):\n","        return\n","\n","    with open(script_path, \"r\") as file:\n","        script = file.read()\n","\n","    statements = script.split(\"\\nGO\\n\")\n","    \n","    object_name = script_path.split(\"/\")[-1].split(\".\")[0]\n","    db_name = script_path.split(\"/\")[0].replace(\".\", \" \")\n","    schema_name = script_path.split(\"/\")[1]\n","\n","    if \"Tables/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.tables WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"Views/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.views WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"StoredProcedures/\" in script_path:\n","        check_query = f\"SELECT COUNT(*) FROM sys.procedures WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n","    elif \"Security/\" in script_path:  \n","        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{object_name}'\"\n","    else:\n","        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{schema_name}'\"\n","    \n","    with db_type.get_engine().connect() as conn:\n","        object_exists = conn.execute(text(check_query)).scalar() > 0\n","\n","        if not object_exists:\n","            try:\n","                for statement in filter(None, map(str.strip, statements)):\n","                    conn.execute(text(statement))\n","                conn.commit()\n","            except Exception as e:\n","                conn.rollback()\n","                print(f\"Failed to create {schema_name}.{object_name}: {e}\")\n","                raise\n","        else:\n","            print(f\"The object {schema_name}.{object_name} already exists in {db_name}. No changes made.\")\n","\n","def iterate_sql_objects(db_type: DBType, object_type: str, objects_created: set):\n","    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n","    visited_dirs = set()\n","    \n","    for root, _, files in os.walk(base_path):\n","        if root in visited_dirs:\n","            continue\n","        visited_dirs.add(root)\n","        \n","        for file in filter(lambda f: f.endswith(\".sql\"), files):\n","            script_path = os.path.join(root, file).lstrip(\"./\")\n","           \n","            if script_path in objects_created:\n","                continue\n","\n","            parts = script_path.split(os.sep)\n","            if object_type == \"Schema\":\n","                if any(folder in parts for folder in [\"Tables\", \"Views\", \"StoredProcedures\"]):\n","                    continue  \n","            else:\n","                if object_type not in parts:\n","                    continue  \n","\n","            process_sql_script(db_type, script_path)\n","            objects_created.add(script_path)\n","\n","def process_sql_objects(db_type: DBType, obj_type: str = None):\n","    table_orders_cache = {}\n","    has_changed_directory = False \n","    current_dir = os.path.abspath(os.getcwd())\n","\n","    table_order_files = {\n","        DBType.SQLDatabase: os.path.join(current_dir, \"Setup\", \"Files\", \"SQLDatabase\", \"MetaTableOrder.txt\"),\n","        DBType.Warehouse: os.path.join(current_dir, \"Setup\", \"Files\", \"Warehouse\", \"DWTableOrder.txt\")\n","    }\n","    \n","    for db_type_key, file_path in table_order_files.items():        \n","        if os.path.exists(file_path):\n","            table_orders_cache[db_type_key] = read_table_order(file_path)\n","\n","    table_order = table_orders_cache.get(db_type, [])\n","\n","    if not has_changed_directory:\n","        workspaces_dwa_path = os.path.join(current_dir, \"Workspaces\", \"DWA\")\n","        \n","        if os.path.exists(workspaces_dwa_path):\n","            os.chdir(workspaces_dwa_path)\n","            has_changed_directory = True \n","    \n","    objects_created = set()\n","    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n","    \n","    for table in table_order:\n","        schema = table.split(\".\", maxsplit=1)[0].strip(\"[]\")\n","        table_name = table.split(\".\", maxsplit=1)[1].strip(\"[]\")\n","        script_path = f\"{base_path}/{schema}/{obj_type}/{table_name}.sql\"\n","            \n","        if os.path.exists(script_path) and script_path not in objects_created:\n","            process_sql_script(db_type, script_path)\n","            objects_created.add(script_path)\n","    \n","    iterate_sql_objects(db_type, obj_type, objects_created)\n","    os.chdir(original_dir)\n","\n","#Create meta databse objects\n","process_sql_objects(DBType.SQLDatabase, \"Security\")\n","process_sql_objects(DBType.SQLDatabase, \"Tables\")\n","process_sql_objects(DBType.SQLDatabase, \"Views\")\n","process_sql_objects(DBType.SQLDatabase, \"StoredProcedures\") \n","\n","#Create warehouse objects\n","process_sql_objects(DBType.Warehouse, \"Schema\")\n","process_sql_objects(DBType.Warehouse, \"StoredProcedures\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"a09a661f-d704-4899-8869-46fbf1fb5cd3","normalized_state":"finished","queued_time":"2025-07-12T21:27:17.4471913Z","session_start_time":null,"execution_start_time":"2025-07-12T21:28:49.1971331Z","execution_finish_time":"2025-07-12T21:28:55.5535528Z","parent_msg_id":"39d9ab07-021a-4759-85d2-7cf810be73c3"},"text/plain":"StatementMeta(, a09a661f-d704-4899-8869-46fbf1fb5cd3, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The object Security.audit already exists in Meta SQLDatabase. No changes made.\nThe object Security.devops already exists in Meta SQLDatabase. No changes made.\nThe object Security.config already exists in Meta SQLDatabase. No changes made.\nThe object dbo.dict_artefacts already exists in Meta SQLDatabase. No changes made.\nThe object config.Configurations already exists in Meta SQLDatabase. No changes made.\nThe object config.IdentityMethods already exists in Meta SQLDatabase. No changes made.\nThe object config.Datasets already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroups already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTables already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTableJoins already exists in Meta SQLDatabase. No changes made.\nThe object config.edwTableLineage already exists in Meta SQLDatabase. No changes made.\nThe object config.Templates already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupLinks already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineGroups already exists in Meta SQLDatabase. No changes made.\nThe object config.Pipelines already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineTables already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupPipelines already exists in Meta SQLDatabase. No changes made.\nThe object config.PackageGroupTables already exists in Meta SQLDatabase. No changes made.\nThe object config.DatasetLineage already exists in Meta SQLDatabase. No changes made.\nThe object config.cdcSqlTables already exists in Meta SQLDatabase. No changes made.\nThe object audit.PipelineLog already exists in Meta SQLDatabase. No changes made.\nThe object audit.ReleaseLog already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineMeta already exists in Meta SQLDatabase. No changes made.\nThe object config.PipelineMetaCache already exists in Meta SQLDatabase. No changes made.\nThe object config.Artefacts already exists in Meta SQLDatabase. No changes made.\nThe object devops.usp_ConfigPackageTables already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_GetTables already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_PipelineBuildMetaData already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_PipelineQueue already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_cdcSqlTables already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_cdcSqlBegin already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_OpsDatasets already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_PipelineSequence already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_GetWorkspaces already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_cdcSqlCommit already exists in Meta SQLDatabase. No changes made.\nThe object config.usp_TruncateAll already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineLogCopy already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineStart already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineEndFailure already exists in Meta SQLDatabase. No changes made.\nThe object audit.usp_PipelineEndSuccess already exists in Meta SQLDatabase. No changes made.\nThe object reports.reports already exists in DW Warehouse. No changes made.\nThe object stg.stg already exists in DW Warehouse. No changes made.\nThe object dwa.dwa already exists in DW Warehouse. No changes made.\nThe object int.int already exists in DW Warehouse. No changes made.\nThe object aw.aw already exists in DW Warehouse. No changes made.\nThe object aw_int.aw_int already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_Insert already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_PrestageView already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_Delete already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_CTAS already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableDrop already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableCreate already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_DeploySchemaDrift already exists in DW Warehouse. No changes made.\nThe object dwa.usp_PipelinePostExecute already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_CheckSchemaDrift already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableCreateColumns already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableSwap already exists in DW Warehouse. No changes made.\nThe object dwa.usp_TableLoad_Update already exists in DW Warehouse. No changes made.\nThe object int.usp_LoadNumbers already exists in DW Warehouse. No changes made.\nThe object int.usp_SettingEndDate already exists in DW Warehouse. No changes made.\nThe object aw.usp_AccountRangeMapCreate already exists in DW Warehouse. No changes made.\nThe object dbo.sp_whoisactive already exists in DW Warehouse. No changes made.\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de3b1a99-895a-4e74-8a0a-32132f617a34"},{"cell_type":"markdown","source":["### 6. Upload Data Pipelines"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da333f14-9428-48c3-aead-65c5d0bf1117"},{"cell_type":"code","source":["# Stores new references for updating old data pipelines connections \n","lh_artifact_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", WORKSPACE_ID) #Lakehouse ID\n","meta_artifact_id = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", WORKSPACE_ID) #SQL DB ID and Connection String\n","sqldb_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/SQLDatabases/{meta_artifact_id}'\n","response = requests.get(sqldb_url, headers=headers)\n","\n","meta_endpoint = response.json().get(\"properties\", {}).get(\"serverFqdn\")\n","meta_databasename = response.json().get(\"properties\", {}).get(\"databaseName\")\n","\n","dw_artifact_id = fabric.resolve_item_id(warehouse_name, \"Warehouse\", WORKSPACE_ID) #Warehouse ID and Connection String\n","\n","dw_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/warehouses/{dw_artifact_id}'\n","response = requests.get(dw_url, headers=headers)\n","\n","dw_endpoint = response.json().get(\"properties\", {}).get(\"connectionString\")\n","\n","def create_folder(folder_name: str, parent_folder_id: str=None, workspace_id: str = None):\n","    if not workspace_id:\n","        workspace_id = sf.get_notebook_workspace_id()\n","    client = fabric.FabricRestClient()\n","    response = client.get( f\"v1/workspaces/{workspace_id}/folders\").json()\n","    folders = response.get(\"value\", [])\n","    folder_id=next((f[\"id\"] for f in folders if f[\"displayName\"].lower() ==  folder_name.lower()), None)\n","    if folder_id:\n","            return folder_id\n","    else:\n","        client = fabric.FabricRestClient()\n","        if parent_folder_id:\n","            json={   \n","                \"displayName\": folder_name,\n","                 \"parentFolderId\": parent_folder_id\n","                \n","            }\n","        else:\n","            json={   \n","                \"displayName\": folder_name,\n","            }       \n","        response = client.post(f\"v1/workspaces/{workspace_id}/folders\",json=json).json()\n","        return response['id']\n","\n","def get_connection_id_by_name(connection_name: str) -> str:\n","    url = f\"{BASE_URL}/connections\"\n","\n","    response = requests.get(url, headers=headers)\n","    response.raise_for_status()\n","\n","    connections = response.json().get(\"value\", [])\n","\n","    for conn in connections:\n","        if conn.get(\"displayName\") == connection_name:\n","            return conn.get(\"id\")\n","\n","    raise ValueError(f\"Connection with name '{connection_name}' not found in workspace {WORKSPACE_ID}.\")\n","\n","fabricsqldb_connection_id = get_connection_id_by_name(fabricsqldb_connection_name)\n","fabricdatapipelines_connection_id = get_connection_id_by_name(fabricdatapipelines_connection_name)\n","\n","\n","current_dir = os.path.abspath(os.getcwd())\n","file_path = os.path.join(current_dir, \"Setup\", \"Files\", \"Workspace\", \"AWDataPipelines.txt\")\n","if os.path.exists(file_path):\n","    with open(file_path, \"r\") as f:\n","        aw_pipelines = [line.strip() for line in f if line.strip()]\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"finished","queued_time":"2025-07-12T21:44:04.9321566Z","session_start_time":null,"execution_start_time":"2025-07-12T21:44:17.8348419Z","execution_finish_time":"2025-07-12T21:44:20.1196589Z","parent_msg_id":"be10e4b6-2704-49b2-b7da-fa0f6cfb1ebe"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eee6400d-7f6f-4b5f-a5a0-3b5433065a31"},{"cell_type":"code","source":["access_token = mssparkutils.credentials.getToken('pbi') #Rebuild access token \n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","env_variables_map = {\n","    \"5941a6c0-8c98-4d79-b065-a3789e9e0960\": WORKSPACE_ID,\n","    \"fkm4vwf6l6zebg4lqrhbtdcmsq-yctecwmyrr4u3mdfun4j5hqjma.database.fabric.microsoft.com\": meta_endpoint,\n","    \"Meta-fe70c606-af27-4f64-973a-2be877526212\": meta_databasename,\n","    \"fkm4vwf6l6zebg4lqrhbtdcmsq-yctecwmyrr4u3mdfun4j5hqjma.datawarehouse.fabric.microsoft.com\": dw_endpoint,\n","    \"DW\": warehouse_name,\n","    \"d58f4f2d-59d7-406d-ae4c-898354a6a75f\" : lh_artifact_id,\n","    \"fe70c606-af27-4f64-973a-2be877526212\" : meta_artifact_id\n","}\n","\n","class DataPipeline:\n","    pass\n","\n","class DataPipeline:\n","    name: str\n","    git_id: str\n","    real_id: str\n","    raw_definition: str\n","    definition: object\n","    pipeline_created: bool = False\n","    folder_id: str\n","\n","    def __init__(self, name, git_id, raw_definition, definition, folder_id):\n","        self.name = name\n","        self.git_id = git_id\n","        self.definition = definition\n","        self.raw_definition = raw_definition\n","        self.folder_id=folder_id\n","    def __hash__(self) -> int:\n","        return hash(self.git_id)\n","    def __eq__(self, other):\n","        return self.git_id == other.git_id\n","\n","    def __str__(self):\n","        return f\"DataPipeline: {self.name}\"\n","\n","    def __repr__(self):\n","        return f\"DataPipeline: {self.name}\"\n","\n","    def is_created(self) -> bool:\n","        return self in pipelines_created or self.pipeline_created\n","\n","    def create(self, pipelines):\n","        global pipelines_created\n","\n","        if self.name in aw_pipelines:\n","            print(f\"Skipping {self.name} as requires SFTP Connection. Install Manually if required.\")\n","            return\n","\n","        if self.is_created():\n","            print(f\"{self.name} already created\")\n","            return\n","\n","        print(f\"Creating pipeline {self.name}\")\n","\n","        child_pipelines: list[DataPipeline] = self.get_child_pipelines(pipelines)\n","\n","        for child_pipeline in child_pipelines:\n","            if child_pipeline.is_created():\n","                continue\n","            child_pipeline.create(pipelines)\n","\n","        self.update_activity_references()\n","        self.upload_pipeline()\n","\n","        print(f\"{self.name} created and uploaded.\")\n","\n","    def get_child_pipelines(self, pipelines : list[DataPipeline]) -> list[DataPipeline]:\n","        self.child_pipelines: set[DataPipeline] = set()\n","\n","        def find_pipelines(activities : list [object]):\n","            for activity in activities:\n","                activity_type = activity.get(\"type\")\n","                type_properties = activity.get(\"typeProperties\", {})\n","\n","                if activity_type == \"ExecutePipeline\":\n","                    reference_name = type_properties.get(\"pipeline\", {}).get(\n","                        \"referenceName\"\n","                    )\n","                    matched_pipeline = next(\n","                        (x for x in pipelines if x.git_id == reference_name), None\n","                    )\n","                    if matched_pipeline:\n","                        self.child_pipelines.add(matched_pipeline)\n","\n","                elif activity_type == \"InvokePipeline\":\n","                    pipeline_id = type_properties.get(\"pipelineId\")\n","                    matched_pipeline = next(\n","                        (x for x in pipelines if x.git_id == pipeline_id), None\n","                    )\n","                    if matched_pipeline:\n","                        self.child_pipelines.add(matched_pipeline)\n","                    \n","                else:\n","                    for key in [\"activities\", \"ifTrueActivities\", \"ifFalseActivities\"]:\n","                        if key in type_properties:\n","                            find_pipelines(type_properties[key])\n","            return self.child_pipelines\n","\n","        return find_pipelines(\n","            self.definition.get(\"properties\", {}).get(\"activities\", [])\n","        )\n","\n","    def update_activity_references(self):\n","        global pipelines_created\n","\n","        def fetch_trident_notebooks():\n","            notebooks_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/notebooks\"\n","            response = requests.get(notebooks_url, headers=headers)\n","            response.raise_for_status()\n","            return {notebook[\"displayName\"]: notebook[\"id\"] for notebook in response.json().get(\"value\", [])}\n","\n","        trident_notebooks = fetch_trident_notebooks()\n","\n","        new_definition = self.raw_definition\n","        for pipeline in pipelines_created: \n","            new_definition = new_definition.replace(pipeline.git_id, pipeline.real_id)\n","        for source_id, target_id in env_variables_map.items():  \n","            new_definition = new_definition.replace(source_id, target_id)\n","\n","        definition_json = json.loads(new_definition)\n","\n","        def replace_values(obj):\n","            if isinstance(obj, dict):\n","                for key, value in obj.items():\n","                    if key == \"type\" and isinstance(value, str):\n","                        if \"Lakehouse\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict) :\n","                                if \"artifactId\" in obj[\"typeProperties\"]  and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = lh_artifact_id  \n","                        \n","                        elif \"DataWarehouse\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n","                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = dw_artifact_id \n","                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n","                                obj[\"endpoint\"] = dw_endpoint \n","\n","                        elif \"FabricSqlDatabase\" in value:\n","                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n","                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n","                                    obj[\"typeProperties\"][\"artifactId\"] = meta_artifact_id  \n","                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n","                                obj[\"endpoint\"] = meta_endpoint  \n","                            if \"externalReferences\" in obj and isinstance(obj[\"externalReferences\"], dict):\n","                                obj[\"externalReferences\"][\"connection\"] = fabricsqldb_connection_id\n","\n","                        elif \"InvokePipeline\" in value:\n","                            if \"externalReferences\" in obj and isinstance(obj[\"externalReferences\"], dict):\n","                                obj[\"externalReferences\"][\"connection\"] = fabricdatapipelines_connection_id\n","\n","                        elif \"TridentNotebook\" in value:\n","                            notebook_name = obj.get(\"name\")\n","                            if notebook_name in trident_notebooks:\n","                                obj[\"typeProperties\"][\"notebookId\"] = trident_notebooks[notebook_name]\n","\n","                    elif key == \"workspaceId\" and isinstance(value, str) and not value.startswith(\"@\"):\n","                        obj[key] = WORKSPACE_ID  \n","\n","                    replace_values(value)\n","\n","            elif isinstance(obj, list):\n","                for item in obj:\n","                    replace_values(item)\n","\n","        replace_values(definition_json)\n","\n","        self.raw_definition = json.dumps(definition_json, indent=4)\n","\n","    def upload_pipeline(self):\n","        global pipelines_created\n","        pipelines_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/dataPipelines\"\n","        pipeline_b64 = base64.b64encode(self.raw_definition.encode()).decode()\n","\n","        payload = {\n","            \"displayName\": self.name,\n","            \"folderId\" : self.folder_id,\n","            \"definition\": {\n","                \"parts\": [\n","                    {\n","                        \"path\": \"pipeline.content.json\",\n","                        \"payload\": pipeline_b64,\n","                        \"payloadType\": \"InlineBase64\",\n","                    }\n","                ]\n","            },\n","        }\n","        create_pipeline_request = requests.post(pipelines_url, headers=headers, data=json.dumps(payload))\n","        if not create_pipeline_request.ok:\n","            print(create_pipeline_request.json())\n","            raise RuntimeError(f\"Failed to create pipeline {self.name}. ({create_pipeline_request.text})\")\n","        self.real_id = create_pipeline_request.json().get(\"id\")\n","        self.pipeline_created = True\n","        pipelines_created.append(self)\n","    \n","pipelines: list[DataPipeline] = []\n","folder_cache = {}\n","for path, dirs, files in os.walk(\".\"):\n","    if not path.endswith(\".DataPipeline\"):\n","        continue\n","    folder_id=None\n","    folder_path = \"/\".join( path.split(\"/\")[3:-1])\n","    if folder_path  in folder_cache:\n","        folder_id=folder_cache[folder_path]\n","    else:\n","        for folder in folder_path.split('/'):\n","            folder_id=create_folder(folder, folder_id)\n","        folder_cache[folder_path] = folder_id\n","    pipeline_name = path.strip(\".\").split(\".\")[0].replace(\"\\\\\", \"/\").split(\"/\")[-1]\n","    platform = json.load(open(os.path.join(path, \".platform\")))\n","    raw_definition = open(os.path.join(path, \"pipeline-content.json\")).read()\n","    definition = json.loads(raw_definition)\n","    pipeline_id = platform[\"config\"][\"logicalId\"]\n","\n","    data_pipeline = DataPipeline(pipeline_name, pipeline_id, raw_definition, definition, folder_id)\n","    pipelines.append(data_pipeline)\n","pipelines_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/dataPipelines\"\n","response = requests.get(pipelines_url, headers=headers)\n","response.raise_for_status()\n","\n","pipelines_created: list[DataPipeline] = []\n","\n","pipelines_existing: list[(str, str)] = [\n","    (pipeline[\"displayName\"], pipeline[\"id\"])\n","    for pipeline in response.json().get(\"value\", [])\n","]\n","\n","for pipeline_name, pipeline_id in pipelines_existing:\n","    pipeline = next((x for x in pipelines if x.name == pipeline_name), None)\n","    if not pipeline:\n","        continue\n","\n","    pipeline.real_id = pipeline_id\n","    pipeline.pipeline_created = True\n","    pipelines_created.append(pipeline)\n","i=0\n","\n","for pipeline in pipelines:\n","    i+=1\n","    if pipeline in pipelines_created or pipeline.pipeline_created:\n","        continue\n","    #if i>1 : break\n","    pipeline.create(pipelines)\n","\n","  "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"finished","queued_time":"2025-07-12T21:44:07.289178Z","session_start_time":null,"execution_start_time":"2025-07-12T21:44:20.1221182Z","execution_finish_time":"2025-07-12T21:44:22.4560734Z","parent_msg_id":"01933d10-0831-4aa3-81fa-3abf79d77015"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Skipping Copy-SFTP-Prodata as requires SFTP Connection. Install Manually if required.\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19d74a8c-843a-4ba4-bcc8-f55a28ebf92f"},{"cell_type":"markdown","source":["### 7. Create objects for Adventure Works when deploy_aw is set to True"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acc1cdcb-2625-449d-9a11-c903889ec341"},{"cell_type":"code","source":["if deploy_aw == False:\n","    mssparkutils.notebook.exit(1)\n","\n","class CustomTokenCredential:\n","    def get_token(self, *scopes, **kwargs):\n","        return AccessToken(notebookutils.credentials.getToken('storage'), expires_on=9999999999)\n","\n","credential = CustomTokenCredential()\n","service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n","fs = service_client.get_file_system_client(WORKSPACE_NAME)\n","\n","lh_paths = {\n","    \"tmp/landing/\": f\"{lakehouse_name}.Lakehouse/Files/landing\",\n","    \"tmp/backup/\": f\"{lakehouse_name}.Lakehouse/Files/backup\",\n","    \"tmp/Tables/\": f\"{lakehouse_name}.Lakehouse/Tables\"\n","}\n","\n","def unpack_files(Setup_dir, archives):\n","    for archive, target in archives.items():\n","        shutil.unpack_archive(os.path.join(Setup_dir, archive), target, \"zip\")\n","\n","def upload_files(local_path, azure_path):\n","    for root, _, files in os.walk(local_path):\n","        if root.endswith(\"/_metadata\"): continue\n","        for file in files:\n","            file_path_on_local = os.path.join(root, file)\n","            relative_path = os.path.relpath(root, local_path)\n","            file_path_on_azure = os.path.join(azure_path, relative_path, file).replace(\"\\\\\", \"/\")\n","            file_client = fs.get_file_client(file_path_on_azure)\n","            with open(file_path_on_local, \"rb\") as data:\n","                file_client.upload_data(data, overwrite=True)\n","\n","git_lh_directory = \"Setup/Files/Lakehouse\"\n","archives = {\n","    'AW_landing.zip': \"tmp/landing\",\n","    'AW_Backup.zip': \"tmp/backup\",\n","    'AW_tables.zip': \"tmp/Tables\"\n","}\n","\n","unpack_files(git_lh_directory, archives)\n","for local, azure in lh_paths.items():\n","    upload_files(local, azure)\n","\n","#Deploy AW Warehouse Objects \n","time.sleep(120)  # 2mins wait for MD Sync of the Lakehouse objects\n","process_sql_objects(DBType.Warehouse, \"Tables\")\n","process_sql_objects(DBType.Warehouse, \"Views\")\n","print ('AdventureWorks Samples Deployed.')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"finished","queued_time":"2025-07-12T21:44:12.8307491Z","session_start_time":null,"execution_start_time":"2025-07-12T21:44:22.4581934Z","execution_finish_time":"2025-07-12T21:46:31.5720533Z","parent_msg_id":"514e108d-d88c-4042-a1b6-a55aa9920e6c"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"NameError","evalue":"name 'process_sql_objects' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#Deploy AW Warehouse Objects \u001b[39;00m\n\u001b[1;32m     45\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m120\u001b[39m)  \u001b[38;5;66;03m# 2mins wait for MD Sync of the Lakehouse objects\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m process_sql_objects(DBType\u001b[38;5;241m.\u001b[39mWarehouse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m process_sql_objects(DBType\u001b[38;5;241m.\u001b[39mWarehouse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdventureWorks Samples Deployed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'process_sql_objects' is not defined"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e775b-7c60-4d73-a9d7-d6bd77673d28"},{"cell_type":"code","source":["#Restore Latest Configuration\n","import pandas as pd\n","\n","if deploy_aw == False:\n","    mssparkutils.notebook.exit(1)\n","\n","WORKSPACE_ID = fabric.get_workspace_id()\n","WORKSPACE_NAME = fabric.resolve_workspace_name()\n","lakehouse_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", WORKSPACE_ID)\n","client = fabric.FabricRestClient()\n","items = fabric.FabricRestClient().get(f\"/v1/workspaces/{WORKSPACE_ID}/SQLDatabases\").json()[\"value\"]\n","sql_database=next((endpoint for endpoint in items if endpoint[\"displayName\"] == metadata_db_name ))\n","sql_end_point = sql_database[\"properties\"][\"serverFqdn\"]\n","sql_database_name = sql_database[\"properties\"][\"databaseName\"]\n","connection_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={sql_end_point};database={sql_database_name}\"\n","token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n","token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n","engine=sqlalchemy.create_engine(\"mssql+pyodbc://\", creator=lambda: pyodbc.connect(connection_string, attrs_before={1256: token_struct}))\n","token = notebookutils.credentials.getToken(\"https://storage.azure.com/\")\n","headers = {\"Authorization\": f\"Bearer {token}\"}\n","\n","class CustomTokenCredential:\n","    def get_token(self, *scopes, **kwargs):\n","        return AccessToken(notebookutils.credentials.getToken('storage'), expires_on=9999999999)\n","\n","credential = CustomTokenCredential()\n","service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n","fs = service_client.get_file_system_client(WORKSPACE_NAME)\n","\n","path =  f\"{lakehouse_name}.Lakehouse/Files/backup/config\"\n","paths = fs.get_paths(path=path)\n","folders = [p.name[len(path.rstrip('/') + '/'):] for p in paths if p.is_directory and '/' not in p.name[len(path.rstrip('/') + '/'):]]\n","max_folder = max(folders) if folders else None # Return Last Backup of Config Data\n","\n","path =  f\"{lakehouse_name}.Lakehouse/Files/backup/config/{max_folder}\"\n","paths = fs.get_paths(path=path)\n","files = [p.name[len(path.rstrip('/') + '/'):] for p in paths if not p.is_directory and '/' not in p.name[len(path.rstrip('/') + '/'):]]\n","\n","with engine.connect() as alchemy_connection:\n","    cursor = engine.raw_connection().cursor()\n","    cursor.execute (\"exec config.usp_TruncateAll\")\n","    cursor.commit()\n","    rows = pd.read_sql_query(\"select * from config.Configurations\", alchemy_connection)\n","    if not rows.empty:\n","        print ('Sample Metadata already deployed. Skipping.')\n","        notebookutils.notebook.exit(1)\n","    tables = pd.read_sql_query (\"exec [config].[usp_GetTables]\", alchemy_connection)\n","    for _, row in tables.iterrows():\n","        table = row['name']  # adjust this column name to match your output\n","        file_url = f\"https://onelake.dfs.fabric.microsoft.com/{WORKSPACE_ID}/{lakehouse_id}/Files/backup/config/{max_folder}/{table}.parquet\"\n","        response = requests.head(file_url, headers=headers)\n","\n","        if response.status_code != 200:\n","            continue\n","        file_path = f\"abfss://{WORKSPACE_ID}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files/backup/config/{max_folder}/{table}.parquet\"\n","        df = spark.read.parquet(file_path).toPandas()\n","        print (f\"restoring {table}...\")\n","        df.to_sql(table, schema='config', con=engine, if_exists='append', index=False)\n","    \n","    print ('Updating config.Configurations')\n","    settings = {\n","        \"Lakehouse\": lakehouse_name ,\n","        \"LakehouseID\": lakehouse_id,\n","        \"WorkspaceID\": WORKSPACE_ID\n","    }   \n","    settings_json = json.dumps(settings) \n","    sql = f\"UPDATE config.Configurations SET ConnectionSettings ='{settings_json}' WHERE ConfigurationID=1\"\n","    cursor.execute (sql)\n","    cursor.commit()\n","\n","    settings = {\n","        \"Workspace\": WORKSPACE_NAME,\n","        \"WorkspaceID\": WORKSPACE_ID\n","    }   \n","    settings_json = json.dumps(settings) \n","    sql = f\"UPDATE config.Configurations SET ConnectionSettings ='{settings_json}' WHERE ConfigurationID=2\"\n","    cursor.execute (sql)\n","    cursor.commit()\n","\n","print ('')\n","print ('Sample Metadata restored. Now run TR_Ops Pipeline to complete')\n","\n","\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":"3076ef69-2597-4adb-9efb-4ed1151db149","normalized_state":"cancelled","queued_time":"2025-07-12T21:44:15.9434471Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-07-12T21:46:31.5741776Z","parent_msg_id":"745ad780-6292-4e53-a5f9-3a825dd5c945"},"text/plain":"StatementMeta(, 3076ef69-2597-4adb-9efb-4ed1151db149, -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b9b06dba-27d5-41e5-a8c6-3aea4099d953"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"warehouse":{}}},"nbformat":4,"nbformat_minor":5}