{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9979c7-719f-4bf4-9e63-f8a6aaf6ac67",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Data Warehouse Automation (DWA) Setup in Microsoft Fabric**\n",
    "- Modify the parameter values as needed.\n",
    "- Create your connections on your workspace before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abb929-7fea-4ae9-bb22-f9026054affe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-03T16:24:42.1399604Z",
       "execution_start_time": "2025-08-03T16:24:41.7518405Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5b143256-486a-4134-bb89-e8fd2ab1c05b",
       "queued_time": "2025-08-03T16:24:32.5130133Z",
       "session_id": "4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb",
       "session_start_time": "2025-08-03T16:24:32.5140606Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lakehouse_name = \"LH\"            # Lakehouse name e.g. FinanceLH, FinanceBronze, \n",
    "warehouse_name = \"DW\"            # Warehouse name e.g. FinanceDW, FinanceGold\n",
    "metadata_db_name = \"Meta\"        # SQL Database name to store metadata information for the framework\n",
    "lakehouse_schema_enabled = True  # If False then lakehouse and warehouse objects will need to be created manually\n",
    "warehouse_case_sensitive = False # To make data warehouse case sensitive, default is False\n",
    "deploy_aw = True                 # To deploy AdventureWorks files and objects set to True\n",
    "branch= \"main\"                    # main or dev for experimental release\n",
    "\n",
    "# Create these connections on your workspace before running. https://github.com/ProdataSQL/DWA/wiki/Create-Cloud-Connection\n",
    "fabricsqldb_connection_name = \"Meta-DWA-Training\"               #Connection name for Fabric SQL Database shareable cloud\n",
    "fabricdatapipelines_connection_name = \"Pipelines-DWA-Training\"  #Connection name for Fabric Data Pipelines shareable cloud\n",
    "pbi_connection_name =\"PBI-DWA-Training\"                         #Connection name for Power BI Semantic Models (used for refresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e17fa-939d-433c-87be-af7c49a0b07c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Import of needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee02c50f-f044-4ad4-81c3-d1afaa40738e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-03T16:25:17.4084423Z",
       "execution_start_time": "2025-08-03T16:25:09.7066262Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "853b0e4a-bb86-4bb8-89c3-cf8623f282b7",
       "queued_time": "2025-08-03T16:25:09.7054579Z",
       "session_id": "4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import struct\n",
    "import sqlalchemy\n",
    "from sqlalchemy.sql import text\n",
    "from notebookutils import mssparkutils\n",
    "import sempy.fabric as fabric\n",
    "import sempy.fabric as sf\n",
    "import base64\n",
    "from azure.core.credentials import AccessToken\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import pyodbc\n",
    "import shutil\n",
    "from git import Repo\n",
    "import requests\n",
    "import json\n",
    "import fnmatch\n",
    "import time\n",
    "from enum import Enum\n",
    "from sqlalchemy.engine import Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fbf6f1-4603-4634-a9fc-222098ce0ca6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Setting up global parameters \n",
    "Also Clone GIT repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d0a5c9-1d97-4597-9dc1-c9aa6e7bd49a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-03T16:25:26.9434809Z",
       "execution_start_time": "2025-08-03T16:25:17.4105868Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1b871056-2e40-4561-95e9-634789e25a06",
       "queued_time": "2025-08-03T16:25:12.6467262Z",
       "session_id": "4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WORKSPACE_ID = fabric.get_workspace_id()\n",
    "WORKSPACE_NAME = fabric.resolve_workspace_name()\n",
    "repo_url = \"https://github.com/ProdataSQL/DWA\"\n",
    "repo_dir = \"DWA_repo\"\n",
    "\n",
    "BASE_URL = \"https://api.fabric.microsoft.com/v1\"\n",
    "access_token = notebookutils.credentials.getToken('pbi')\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "#Clone Repo into local storage\n",
    "if os.path.exists(repo_dir):\n",
    "    shutil.rmtree(repo_dir)\n",
    "Repo.clone_from(url=repo_url, to_path=repo_dir,branch=branch, single_branch=True)\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe6963-fe87-4597-93cb-9bbd40254978",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Create Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa181b62-1c13-45fd-909d-c415eda39a1e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-03T16:25:27.6921027Z",
       "execution_start_time": "2025-08-03T16:25:26.9456577Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "79556a9c-b19d-4c47-9d39-2edad3511941",
       "queued_time": "2025-08-03T16:25:16.7959238Z",
       "session_id": "4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lakehouse LH already exists. No change was made.\n"
     ]
    }
   ],
   "source": [
    "lakehouse_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/lakehouses\"\n",
    "payload = {\n",
    "    \"displayName\": f\"{lakehouse_name}\", \n",
    "    \"description\": \"A schema-enabled lakehouse.\",\n",
    "    \"creationPayload\": {\"enableSchemas\": f\"{lakehouse_schema_enabled}\"} \n",
    "}\n",
    "response = requests.post(lakehouse_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 400:\n",
    "    print(f\"Lakehouse {lakehouse_name} already exists. No change was made.\")\n",
    "elif response.status_code != 201:\n",
    "    raise RuntimeError(f\"Failed to create Lakehouse {lakehouse_name} : {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b8de5-1aee-4278-ad8d-f3baf2a2554b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 2. Create Metadata SQL Database\n",
    "#### **Warning:** Verify the SQL DB creation before executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786c758b-d46c-462c-a5ff-3a9bf6edf8b0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-03T16:25:28.4825305Z",
       "execution_start_time": "2025-08-03T16:25:27.6944232Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "802148b1-c435-464d-8473-1bf508be0a30",
       "queued_time": "2025-08-03T16:25:20.2367187Z",
       "session_id": "4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL DB Meta already exists. No change was made.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"displayName\": f\"{metadata_db_name}\",\n",
    "    \"type\": \"SQLDatabase\",\n",
    "    \"description\": \"SQL Database to store metadata for Framework\"\n",
    "}\n",
    "\n",
    "sqldb_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/items'\n",
    "response = requests.post(sqldb_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 400:\n",
    "    print(f\"SQL DB {metadata_db_name} already exists. No change was made.\")\n",
    "else:\n",
    "    start_time = time.time() \n",
    "    max_wait_time = 600  \n",
    "    check_interval = 60\n",
    "    \n",
    "    while time.time() - start_time < max_wait_time: #Sleep timer to wait for SQL DB creation\n",
    "        time.sleep(check_interval)\n",
    "        db_exists = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", WORKSPACE_ID)   \n",
    "\n",
    "        if db_exists:\n",
    "            break\n",
    "    else: \n",
    "        raise RuntimeError(f\"Failed to create database {metadata_db_name} within the timeout period.\")\n",
    "    \n",
    "    if response.status_code not in [200, 201, 202]:\n",
    "        raise RuntimeError(f\"Failed to create database {metadata_db_name}: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d070a-a9c4-4beb-8eac-7f2a40374907",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Create Data Warehouse\n",
    "#### **Warning:** Verify the Data Warehouse creation before executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc75513-071f-4146-b283-9fddeaa22a6b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-03T16:25:29.2198234Z",
       "execution_start_time": "2025-08-03T16:25:28.4846689Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1b7e4328-aac7-434c-a30f-83548d5a2cb2",
       "queued_time": "2025-08-03T16:25:25.1158545Z",
       "session_id": "4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 4ede4f56-0b0c-4b8c-b639-cdc6d836dfeb, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse DW already exists. No change was made.\n"
     ]
    }
   ],
   "source": [
    "if warehouse_case_sensitive == True:\n",
    "    warehouse_collation = \"Latin1_General_100_BIN2_UTF8\"\n",
    "else:\n",
    "    warehouse_collation = \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"\n",
    "\n",
    "payload = {\n",
    "    \"displayName\": f\"{warehouse_name}\",\n",
    "    \"type\": \"warehouse\",\n",
    "    \"properties\": {\n",
    "        \"collation\": f\"{warehouse_collation}\"  \n",
    "    }\n",
    "}\n",
    "\n",
    "dw_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/items'\n",
    "response = requests.post(dw_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 400:\n",
    "    print(f\"Warehouse {warehouse_name} already exists. No change was made.\")\n",
    "else:\n",
    "    start_time = time.time() \n",
    "    max_wait_time = 600  \n",
    "    check_interval = 60\n",
    "    \n",
    "    while time.time() - start_time < max_wait_time: #Sleep timer to wait for SQL DB creation\n",
    "        time.sleep(check_interval)\n",
    "        dw_exists = fabric.resolve_item_id(warehouse_name, \"Warehouse\", WORKSPACE_ID)   \n",
    "\n",
    "        if dw_exists:\n",
    "            break\n",
    "    else: \n",
    "        raise RuntimeError(f\"Failed to create warehouse {warehouse_name} within the timeout period.\")\n",
    "    \n",
    "    if response.status_code not in [200, 201, 202]:\n",
    "        raise RuntimeError(f\"Failed to create warehouse {warehouse_name}: {response.status_code}, {response.text}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954779cc-808c-4fb1-bdbd-94360f997005",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 4. Upload Notebook Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8a1f0f-c7c9-4a10-96d9-cbe4ebf02181",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-14T19:07:55.427008Z",
       "execution_start_time": "2025-07-14T19:05:48.7227784Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4243a159-74de-4e51-989c-84543b0809cb",
       "queued_time": "2025-07-14T19:03:17.9649949Z",
       "session_id": "929963e1-609d-4bbe-8728-c790daed3f81",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 929963e1-609d-4bbe-8728-c790daed3f81, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract-Excel created.\n",
      "Extract-SP-Excel created.\n",
      "Extract-Parquet-CDC created.\n",
      "Extract-O365-API created.\n",
      "Extract-CSV-Pandas created.\n",
      "Extract-CSV created.\n",
      "Extract-XML created.\n",
      "Extract-JSON created.\n",
      "Export-SQL created.\n",
      "Export-CSV created.\n",
      "Export-SP created.\n",
      "Export-Excel created.\n",
      "Export-Parquet created.\n",
      "Refresh-Fabric created.\n",
      "Copy-Blob created.\n",
      "Ingest-SFTP created.\n",
      "Ingest-SP created.\n",
      "MD-Sync Python created.\n",
      "MD-Sync created.\n",
      "SQL-Connection-Shared-Functions created.\n",
      "SharePoint-Shared-Functions created.\n",
      "Transform-LH-DuckDB created.\n",
      "ArchiveFiles created.\n",
      "Extract-Dictionary created.\n",
      "Extract-Fabric-Logs created.\n",
      "Extract-Artefacts created.\n"
     ]
    }
   ],
   "source": [
    "import sempy.fabric as sf\n",
    "import re\n",
    "lh_artifact_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", WORKSPACE_ID)\n",
    "\n",
    "def get_notebooks(directory, pattern):\n",
    "    notebook_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in fnmatch.filter(files, pattern):\n",
    "            notebook_files.append(os.path.join(root, file))\n",
    "    return notebook_files\n",
    "\n",
    "import json\n",
    "\n",
    "def parse_python_to_cells(py_content):\n",
    "    cells = []\n",
    "    current_cell_lines = []\n",
    "    cell_type = \"code\"\n",
    "    is_param_cell = False\n",
    "    in_metadata_block = False\n",
    "    skipped_first_comment = False\n",
    "    metadata_lines = []\n",
    "    is_default_lakehouse = False  \n",
    "\n",
    "    def add_cell():\n",
    "        if not current_cell_lines:\n",
    "            return\n",
    "        metadata = {}\n",
    "        if is_param_cell:\n",
    "            metadata = {\n",
    "                \"microsoft\": {\n",
    "                    \"language\": \"python\",\n",
    "                    \"language_group\": \"synapse_pyspark\"\n",
    "                },\n",
    "                \"tags\": [\"parameters\"],\n",
    "            }\n",
    "        cells.append({\n",
    "            \"cell_type\": cell_type,\n",
    "            \"metadata\": metadata,\n",
    "            \"source\": current_cell_lines.copy(),\n",
    "            \"execution_count\": None,\n",
    "            \"outputs\": [] if cell_type == \"code\" else None\n",
    "        })\n",
    "        current_cell_lines.clear()\n",
    "\n",
    "    for line in py_content.splitlines(keepends=True):\n",
    "        stripped = line.strip()\n",
    "        if not skipped_first_comment and stripped.startswith(\"#\"):\n",
    "            skipped_first_comment = True\n",
    "            continue\n",
    "        if stripped.startswith(\"# META\"):\n",
    "            in_metadata_block = True\n",
    "            metadata_lines.append(stripped[6:] if stripped.startswith(\"# META \") else \"\")\n",
    "            continue\n",
    "        elif in_metadata_block:\n",
    "            if not stripped.startswith(\"#\") or stripped == \"\":\n",
    "                in_metadata_block = False\n",
    "                metadata_str = \"\\n\".join(metadata_lines)\n",
    "                try:\n",
    "                    meta_json = json.loads(metadata_str)\n",
    "                    dependencies = meta_json.get(\"dependencies\", {})\n",
    "                    lakehouse = dependencies.get(\"lakehouse\", {})\n",
    "                    if lakehouse.get(\"default_lakehouse_name\"):\n",
    "                        is_default_lakehouse = True\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "                metadata_lines.clear()\n",
    "            else:\n",
    "                metadata_lines.append(stripped[6:] if stripped.startswith(\"# META \") else \"\")\n",
    "            continue\n",
    "        if \"# PARAMETERS CELL\" in stripped:\n",
    "            add_cell()\n",
    "            cell_type = \"code\"\n",
    "            is_param_cell = True\n",
    "            continue\n",
    "        elif \"# CELL\" in stripped:\n",
    "            add_cell()\n",
    "            cell_type = \"code\"\n",
    "            is_param_cell = False\n",
    "            continue\n",
    "        elif \"# MARKDOWN\" in stripped or stripped.startswith(\"# %% [markdown]\"):\n",
    "            add_cell()\n",
    "            cell_type = \"markdown\"\n",
    "            is_param_cell = False\n",
    "            continue\n",
    "        if stripped:\n",
    "            if cell_type == \"markdown\" and stripped.startswith(\"#\"):\n",
    "                cleaned_line = re.sub(r'^#\\s?(.*)', r'\\1', stripped) + \"\\n\"\n",
    "                current_cell_lines.append(cleaned_line)\n",
    "            else:\n",
    "                current_cell_lines.append(line)\n",
    "    add_cell()\n",
    "    return cells, is_default_lakehouse\n",
    "\n",
    "def py_to_notebook(py_content, notebook_name):\n",
    "    notebook_cells, is_default_lakehouse = parse_python_to_cells(py_content)\n",
    "    metadata = {\n",
    "        \"language_info\": {\n",
    "            \"name\": \"python\",\n",
    "            \"language_group\": \"synapse_pyspark\"\n",
    "        }\n",
    "    }\n",
    "    if is_default_lakehouse:\n",
    "        metadata[\"dependencies\"] = {\n",
    "            \"lakehouse\": {\n",
    "                \"default_lakehouse\": lh_artifact_id,\n",
    "                \"default_lakehouse_name\": lakehouse_name,\n",
    "                \"default_lakehouse_workspace_id\": WORKSPACE_ID\n",
    "            }\n",
    "        }\n",
    "    notebook_content = {\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 5,\n",
    "        \"cells\": notebook_cells,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    return notebook_content\n",
    "\n",
    "def create_folder(folder_name: str, parent_folder_id: str=None, workspace_id: str = None):\n",
    "    if not workspace_id:\n",
    "        workspace_id = sf.get_notebook_workspace_id()\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get( f\"v1/workspaces/{workspace_id}/folders\").json()\n",
    "    folders = response.get(\"value\", [])\n",
    "    folder_id=next((f[\"id\"] for f in folders if f[\"displayName\"].lower() ==  folder_name.lower()), None)\n",
    "    if folder_id:\n",
    "            return folder_id\n",
    "    else:\n",
    "        client = fabric.FabricRestClient()\n",
    "        if parent_folder_id:\n",
    "            json={   \n",
    "                \"displayName\": folder_name,\n",
    "                 \"parentFolderId\": parent_folder_id\n",
    "                \n",
    "            }\n",
    "        else:\n",
    "            json={   \n",
    "                \"displayName\": folder_name,\n",
    "            }       \n",
    "        response = client.post(f\"v1/workspaces/{workspace_id}/folders\",json=json).json()\n",
    "        return response['id']\n",
    "\n",
    "def upload_notebook(notebook_name, notebook_content, folder_id):\n",
    "    notebook_json = json.dumps(notebook_content)\n",
    "    notebook_base64 = base64.b64encode(notebook_json.encode('utf-8')).decode('utf-8')\n",
    "    notebook_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/notebooks\"\n",
    "    payload = {\n",
    "        \"displayName\": notebook_name,\n",
    "        \"type\":\"Notebook\",\n",
    "        \"folderId\": folder_id,\n",
    "        \"definition\": {\n",
    "            \"format\": \"ipynb\",\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"path\": \"artifact.content.ipynb\",\n",
    "                    \"payload\": notebook_base64,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    fabric_response = requests.post(\n",
    "        notebook_url,\n",
    "        headers=headers,\n",
    "        data=json.dumps(payload)\n",
    "    )\n",
    "    if fabric_response.status_code == 400:\n",
    "        print(f\"The {notebook_name} already exists. No changes were made\")\n",
    "    elif fabric_response.status_code not in [200,201,202]:\n",
    "        raise RuntimeError(f\"Failed to upload {notebook_name}: {fabric_response.status_code} - {fabric_response.text}\")\n",
    "    else:\n",
    "         print(f\"{notebook_name} created.\")\n",
    "\n",
    "directory = \"Workspaces/DWA/\"\n",
    "pattern = \"*.py\"\n",
    "notebook_files = get_notebooks(directory, pattern)\n",
    "folder_cache = {}\n",
    "i=0\n",
    "\n",
    "for notebook_file in notebook_files:\n",
    "    i+=1\n",
    "    parent_folder_id=None\n",
    "    directory, file_name = os.path.split(notebook_file)\n",
    "    path = \"/\".join( directory.split(\"/\")[2:-1])\n",
    "    if path in folder_cache:\n",
    "        parent_folder_id=folder_cache[path]\n",
    "    if not parent_folder_id:\n",
    "        for folder in path.split('/'):\n",
    "            parent_folder_id=create_folder(folder, parent_folder_id)\n",
    "        folder_cache[path] = parent_folder_id\n",
    "    base_name = os.path.basename(directory)\n",
    "    if base_name.endswith('.Notebook'):\n",
    "        notebook_name = base_name.split('.')[0]\n",
    "    with open(notebook_file, 'r') as file:\n",
    "        py_content = file.read()\n",
    "    notebook_content = py_to_notebook(py_content, notebook_name)\n",
    "    upload_notebook(notebook_name, notebook_content, parent_folder_id)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb600388-b06e-4f66-bebf-e843454f14b6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 5. Create Metadata and Data Warehouse SQL Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3b1a99-895a-4e74-8a0a-32132f617a34",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-14T19:08:01.4628139Z",
       "execution_start_time": "2025-07-14T19:07:55.4294553Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f68dafca-6afb-4543-9fbd-6046fee70178",
       "queued_time": "2025-07-14T19:03:18.0395327Z",
       "session_id": "929963e1-609d-4bbe-8728-c790daed3f81",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 929963e1-609d-4bbe-8728-c790daed3f81, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "META_DB_NAME = metadata_db_name\n",
    "DW_NAME = warehouse_name\n",
    "original_dir = os.getcwd()\n",
    "engine_pool = {}\n",
    "\n",
    "class DBType(Enum):\n",
    "    SQLDatabase = \"SQLDatabases\"\n",
    "    Warehouse = \"Warehouses\"\n",
    "\n",
    "    def get_connection_string(self) -> str: \n",
    "        display_name = DW_NAME if self == DBType.Warehouse else META_DB_NAME\n",
    "        client = fabric.FabricRestClient()\n",
    "        endpoint = f\"/v1/workspaces/{WORKSPACE_ID}/{self.value}\"\n",
    "        databases = client.get(endpoint).json()\n",
    "\n",
    "        selected_database = next((db for db in databases.get(\"value\", []) if db.get(\"displayName\") == display_name), None)\n",
    "        if not selected_database:\n",
    "            raise ValueError(f\"No {self.value} with displayName '{display_name}' found.\")\n",
    "        \n",
    "        server = selected_database['properties'].get('serverFqdn') or selected_database['properties'].get('connectionString')\n",
    "        database = selected_database['properties'].get('databaseName', warehouse_name)\n",
    "            \n",
    "        return f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};database={database};LongAsMax=YES\"\n",
    "\n",
    "    def get_engine(self) -> Engine:\n",
    "        if self not in engine_pool:\n",
    "            token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n",
    "            token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "            engine_pool[self] = sqlalchemy.create_engine(\n",
    "                \"mssql+pyodbc://\",\n",
    "                creator=lambda: pyodbc.connect(self.get_connection_string(), attrs_before={1256: token_struct})\n",
    "            )\n",
    "        return engine_pool[self]\n",
    "   \n",
    "def read_table_order(file_path: str) -> list:\n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def process_sql_script(db_type: DBType, script_path: str):\n",
    "    if not os.path.exists(script_path):\n",
    "        return\n",
    "    \n",
    "    filename = os.path.basename(script_path)\n",
    "    if \"prodata.ie\" in filename:\n",
    "        print(f\"Skipping {filename} because it ends with 'prodata.ie'\")\n",
    "        return\n",
    "\n",
    "    with open(script_path, \"r\") as file:\n",
    "        script = file.read()\n",
    "\n",
    "    statements = script.split(\"\\nGO\\n\")\n",
    "    \n",
    "    object_name = script_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    db_name = script_path.split(\"/\")[0].replace(\".\", \" \")\n",
    "    schema_name = script_path.split(\"/\")[1]\n",
    "\n",
    "    if \"Tables/\" in script_path:\n",
    "        check_query = f\"SELECT COUNT(*) FROM sys.tables WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n",
    "    elif \"Views/\" in script_path:\n",
    "        check_query = f\"SELECT COUNT(*) FROM sys.views WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n",
    "    elif \"StoredProcedures/\" in script_path:\n",
    "        check_query = f\"SELECT COUNT(*) FROM sys.procedures WHERE name = '{object_name}' AND schema_id = SCHEMA_ID('{schema_name}')\"\n",
    "    elif \"Security/\" in script_path:  \n",
    "        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{object_name}'\"\n",
    "    else:\n",
    "        check_query = f\"SELECT COUNT(*) FROM sys.schemas WHERE name = '{schema_name}'\"\n",
    "    \n",
    "    with db_type.get_engine().connect() as conn:\n",
    "        object_exists = conn.execute(text(check_query)).scalar() > 0\n",
    "\n",
    "        if not object_exists:\n",
    "            try:\n",
    "                for statement in filter(None, map(str.strip, statements)):\n",
    "                    conn.execute(text(statement))\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                print(f\"Failed to create {schema_name}.{object_name}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"The object {schema_name}.{object_name} already exists in {db_name}. No changes made.\")\n",
    "\n",
    "def iterate_sql_objects(db_type: DBType, object_type: str, objects_created: set):\n",
    "    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n",
    "    visited_dirs = set()\n",
    "    \n",
    "    for root, _, files in os.walk(base_path):\n",
    "        if root in visited_dirs:\n",
    "            continue\n",
    "        visited_dirs.add(root)\n",
    "        \n",
    "        for file in filter(lambda f: f.endswith(\".sql\"), files):\n",
    "            script_path = os.path.join(root, file).lstrip(\"./\")\n",
    "           \n",
    "            if script_path in objects_created:\n",
    "                continue\n",
    "\n",
    "            parts = script_path.split(os.sep)\n",
    "            if object_type == \"Schema\":\n",
    "                if any(folder in parts for folder in [\"Tables\", \"Views\", \"StoredProcedures\"]):\n",
    "                    continue  \n",
    "            else:\n",
    "                if object_type not in parts:\n",
    "                    continue  \n",
    "\n",
    "            process_sql_script(db_type, script_path)\n",
    "            objects_created.add(script_path)\n",
    "\n",
    "def process_sql_objects(db_type: DBType, obj_type: str = None):\n",
    "    table_orders_cache = {}\n",
    "    has_changed_directory = False \n",
    "    current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "    table_order_files = {\n",
    "        DBType.SQLDatabase: os.path.join(current_dir, \"Setup\", \"Files\", \"SQLDatabase\", \"MetaTableOrder.txt\"),\n",
    "        DBType.Warehouse: os.path.join(current_dir, \"Setup\", \"Files\", \"Warehouse\", \"DWTableOrder.txt\")\n",
    "    }\n",
    "    \n",
    "    for db_type_key, file_path in table_order_files.items():        \n",
    "        if os.path.exists(file_path):\n",
    "            table_orders_cache[db_type_key] = read_table_order(file_path)\n",
    "\n",
    "    table_order = table_orders_cache.get(db_type, [])\n",
    "\n",
    "    if not has_changed_directory:\n",
    "        workspaces_dwa_path = os.path.join(current_dir, \"Workspaces\", \"DWA\")\n",
    "        \n",
    "        if os.path.exists(workspaces_dwa_path):\n",
    "            os.chdir(workspaces_dwa_path)\n",
    "            has_changed_directory = True \n",
    "    \n",
    "    objects_created = set()\n",
    "    base_path = \"Meta.SQLDatabase\" if db_type == DBType.SQLDatabase else \"DW.Warehouse\"\n",
    "    \n",
    "    for table in table_order:\n",
    "        schema = table.split(\".\", maxsplit=1)[0].strip(\"[]\")\n",
    "        table_name = table.split(\".\", maxsplit=1)[1].strip(\"[]\")\n",
    "        script_path = f\"{base_path}/{schema}/{obj_type}/{table_name}.sql\"\n",
    "            \n",
    "        if os.path.exists(script_path) and script_path not in objects_created:\n",
    "            process_sql_script(db_type, script_path)\n",
    "            objects_created.add(script_path)\n",
    "    \n",
    "    iterate_sql_objects(db_type, obj_type, objects_created)\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "#Create meta databse objects\n",
    "process_sql_objects(DBType.SQLDatabase, \"Security\")\n",
    "process_sql_objects(DBType.SQLDatabase, \"Tables\")\n",
    "process_sql_objects(DBType.SQLDatabase, \"Views\")\n",
    "process_sql_objects(DBType.SQLDatabase, \"StoredProcedures\") \n",
    "\n",
    "#Create warehouse objects\n",
    "process_sql_objects(DBType.Warehouse, \"Schema\")\n",
    "process_sql_objects(DBType.Warehouse, \"StoredProcedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da333f14-9428-48c3-aead-65c5d0bf1117",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 6. Upload Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee6400d-7f6f-4b5f-a5a0-3b5433065a31",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-14T19:08:03.7752646Z",
       "execution_start_time": "2025-07-14T19:08:01.4651434Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9eeee6f1-19df-4a76-85e6-7e54b1f76d2b",
       "queued_time": "2025-07-14T19:03:18.141491Z",
       "session_id": "929963e1-609d-4bbe-8728-c790daed3f81",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 929963e1-609d-4bbe-8728-c790daed3f81, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stores new references for updating old data pipelines connections \n",
    "lh_artifact_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", WORKSPACE_ID) #Lakehouse ID\n",
    "meta_artifact_id = fabric.resolve_item_id(metadata_db_name, \"SqlDatabase\", WORKSPACE_ID) #SQL DB ID and Connection String\n",
    "sqldb_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/SQLDatabases/{meta_artifact_id}'\n",
    "response = requests.get(sqldb_url, headers=headers)\n",
    "\n",
    "meta_endpoint = response.json().get(\"properties\", {}).get(\"serverFqdn\")\n",
    "meta_databasename = response.json().get(\"properties\", {}).get(\"databaseName\")\n",
    "\n",
    "dw_artifact_id = fabric.resolve_item_id(warehouse_name, \"Warehouse\", WORKSPACE_ID) #Warehouse ID and Connection String\n",
    "\n",
    "dw_url = f'{BASE_URL}/workspaces/{WORKSPACE_ID}/warehouses/{dw_artifact_id}'\n",
    "response = requests.get(dw_url, headers=headers)\n",
    "\n",
    "dw_endpoint = response.json().get(\"properties\", {}).get(\"connectionString\")\n",
    "\n",
    "def create_folder(folder_name: str, parent_folder_id: str=None, workspace_id: str = None):\n",
    "    if not workspace_id:\n",
    "        workspace_id = sf.get_notebook_workspace_id()\n",
    "    client = fabric.FabricRestClient()\n",
    "    response = client.get( f\"v1/workspaces/{workspace_id}/folders\").json()\n",
    "    folders = response.get(\"value\", [])\n",
    "    folder_id=next((f[\"id\"] for f in folders if f[\"displayName\"].lower() ==  folder_name.lower()), None)\n",
    "    if folder_id:\n",
    "            return folder_id\n",
    "    else:\n",
    "        client = fabric.FabricRestClient()\n",
    "        if parent_folder_id:\n",
    "            json={   \n",
    "                \"displayName\": folder_name,\n",
    "                 \"parentFolderId\": parent_folder_id\n",
    "                \n",
    "            }\n",
    "        else:\n",
    "            json={   \n",
    "                \"displayName\": folder_name,\n",
    "            }       \n",
    "        response = client.post(f\"v1/workspaces/{workspace_id}/folders\",json=json).json()\n",
    "        return response['id']\n",
    "\n",
    "def get_connection_id_by_name(connection_name: str) -> str:\n",
    "    url = f\"{BASE_URL}/connections\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    connections = response.json().get(\"value\", [])\n",
    "\n",
    "    for conn in connections:\n",
    "        if conn.get(\"displayName\") == connection_name:\n",
    "            return conn.get(\"id\")\n",
    "\n",
    "    raise ValueError(f\"Connection with name '{connection_name}' not found in workspace {WORKSPACE_ID}.\")\n",
    "\n",
    "fabricsqldb_connection_id = get_connection_id_by_name(fabricsqldb_connection_name)\n",
    "fabricdatapipelines_connection_id = get_connection_id_by_name(fabricdatapipelines_connection_name)\n",
    "pbi_connection_id = get_connection_id_by_name(pbi_connection_name)\n",
    "\n",
    "current_dir = os.path.abspath(os.getcwd())\n",
    "file_path = os.path.join(current_dir, \"Setup\", \"Files\", \"Workspace\", \"AWDataPipelines.txt\")\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        aw_pipelines = [line.strip() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d74a8c-843a-4ba4-bcc8-f55a28ebf92f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-14T19:09:25.5796781Z",
       "execution_start_time": "2025-07-14T19:08:03.7779434Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8b4ac3f0-b004-4a30-abf9-29c3d2a5fdbf",
       "queued_time": "2025-07-14T19:03:18.2318772Z",
       "session_id": "929963e1-609d-4bbe-8728-c790daed3f81",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 929963e1-609d-4bbe-8728-c790daed3f81, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipeline Pipeline-Worker\n",
      "Creating pipeline Environment-Variables\n",
      "Environment-Variables created and uploaded.\n",
      "Pipeline-Worker created and uploaded.\n",
      "Creating pipeline Extract-SQL\n",
      "Extract-SQL created and uploaded.\n",
      "Creating pipeline Refresh-PBI\n",
      "Refresh-PBI created and uploaded.\n",
      "Skipping Copy-SFTP-Prodata as requires SFTP Connection. Install Manually if required.\n",
      "Creating pipeline Ingest-SQL-CDC\n",
      "Ingest-SQL-CDC created and uploaded.\n",
      "Creating pipeline Pipeline-Controller\n",
      "Pipeline-Controller created and uploaded.\n",
      "Creating pipeline Backup_ConfigSchema\n",
      "Backup_ConfigSchema created and uploaded.\n",
      "Creating pipeline Restore_ConfigSchema\n",
      "Restore_ConfigSchema created and uploaded.\n",
      "Creating pipeline TR_ConfigBackup\n",
      "TR_ConfigBackup created and uploaded.\n",
      "Creating pipeline TR_Ops\n",
      "TR_Ops created and uploaded.\n",
      "Creating pipeline TR_AdventureWorks\n",
      "TR_AdventureWorks created and uploaded.\n"
     ]
    }
   ],
   "source": [
    "access_token = mssparkutils.credentials.getToken('pbi') #Rebuild access token \n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "env_variables_map = {\n",
    "    \"5941a6c0-8c98-4d79-b065-a3789e9e0960\": WORKSPACE_ID,\n",
    "    \"fkm4vwf6l6zebg4lqrhbtdcmsq-yctecwmyrr4u3mdfun4j5hqjma.database.fabric.microsoft.com\": meta_endpoint,\n",
    "    \"Meta-fe70c606-af27-4f64-973a-2be877526212\": meta_databasename,\n",
    "    \"fkm4vwf6l6zebg4lqrhbtdcmsq-yctecwmyrr4u3mdfun4j5hqjma.datawarehouse.fabric.microsoft.com\": dw_endpoint,\n",
    "    \"DW\": warehouse_name,\n",
    "    \"d58f4f2d-59d7-406d-ae4c-898354a6a75f\" : lh_artifact_id,\n",
    "    \"fe70c606-af27-4f64-973a-2be877526212\" : meta_artifact_id\n",
    "}\n",
    "\n",
    "class DataPipeline:\n",
    "    pass\n",
    "\n",
    "class DataPipeline:\n",
    "    name: str\n",
    "    git_id: str\n",
    "    real_id: str\n",
    "    raw_definition: str\n",
    "    definition: object\n",
    "    pipeline_created: bool = False\n",
    "    folder_id: str\n",
    "\n",
    "    def __init__(self, name, git_id, raw_definition, definition, folder_id):\n",
    "        self.name = name\n",
    "        self.git_id = git_id\n",
    "        self.definition = definition\n",
    "        self.raw_definition = raw_definition\n",
    "        self.folder_id=folder_id\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(self.git_id)\n",
    "    def __eq__(self, other):\n",
    "        return self.git_id == other.git_id\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"DataPipeline: {self.name}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DataPipeline: {self.name}\"\n",
    "\n",
    "    def is_created(self) -> bool:\n",
    "        return self in pipelines_created or self.pipeline_created\n",
    "\n",
    "    def create(self, pipelines):\n",
    "        global pipelines_created\n",
    "\n",
    "        if self.name in aw_pipelines:\n",
    "            print(f\"Skipping {self.name} as requires SFTP Connection. Install Manually if required.\")\n",
    "            return\n",
    "\n",
    "        if self.is_created():\n",
    "            print(f\"{self.name} already created\")\n",
    "            return\n",
    "\n",
    "        print(f\"Creating pipeline {self.name}\")\n",
    "\n",
    "        child_pipelines: list[DataPipeline] = self.get_child_pipelines(pipelines)\n",
    "\n",
    "        for child_pipeline in child_pipelines:\n",
    "            if child_pipeline.is_created():\n",
    "                continue\n",
    "            child_pipeline.create(pipelines)\n",
    "\n",
    "        self.update_activity_references()\n",
    "        self.upload_pipeline()\n",
    "\n",
    "        print(f\"{self.name} created and uploaded.\")\n",
    "\n",
    "    def get_child_pipelines(self, pipelines : list[DataPipeline]) -> list[DataPipeline]:\n",
    "        self.child_pipelines: set[DataPipeline] = set()\n",
    "\n",
    "        def find_pipelines(activities : list [object]):\n",
    "            for activity in activities:\n",
    "                activity_type = activity.get(\"type\")\n",
    "                type_properties = activity.get(\"typeProperties\", {})\n",
    "\n",
    "                if activity_type == \"ExecutePipeline\":\n",
    "                    reference_name = type_properties.get(\"pipeline\", {}).get(\n",
    "                        \"referenceName\"\n",
    "                    )\n",
    "                    matched_pipeline = next(\n",
    "                        (x for x in pipelines if x.git_id == reference_name), None\n",
    "                    )\n",
    "                    if matched_pipeline:\n",
    "                        self.child_pipelines.add(matched_pipeline)\n",
    "\n",
    "                elif activity_type == \"InvokePipeline\":\n",
    "                    pipeline_id = type_properties.get(\"pipelineId\")\n",
    "                    matched_pipeline = next(\n",
    "                        (x for x in pipelines if x.git_id == pipeline_id), None\n",
    "                    )\n",
    "                    if matched_pipeline:\n",
    "                        self.child_pipelines.add(matched_pipeline)\n",
    "                    \n",
    "                else:\n",
    "                    for key in [\"activities\", \"ifTrueActivities\", \"ifFalseActivities\"]:\n",
    "                        if key in type_properties:\n",
    "                            find_pipelines(type_properties[key])\n",
    "            return self.child_pipelines\n",
    "\n",
    "        return find_pipelines(\n",
    "            self.definition.get(\"properties\", {}).get(\"activities\", [])\n",
    "        )\n",
    "\n",
    "    def update_activity_references(self):\n",
    "        global pipelines_created\n",
    "\n",
    "        def fetch_trident_notebooks():\n",
    "            notebooks_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/notebooks\"\n",
    "            response = requests.get(notebooks_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return {notebook[\"displayName\"]: notebook[\"id\"] for notebook in response.json().get(\"value\", [])}\n",
    "\n",
    "        trident_notebooks = fetch_trident_notebooks()\n",
    "\n",
    "        new_definition = self.raw_definition\n",
    "        for pipeline in pipelines_created: \n",
    "            new_definition = new_definition.replace(pipeline.git_id, pipeline.real_id)\n",
    "        for source_id, target_id in env_variables_map.items():  \n",
    "            new_definition = new_definition.replace(source_id, target_id)\n",
    "\n",
    "        definition_json = json.loads(new_definition)\n",
    "\n",
    "        def replace_values(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                for key, value in obj.items():\n",
    "                    if key == \"type\" and isinstance(value, str):\n",
    "                        if \"Lakehouse\" in value:\n",
    "                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict) :\n",
    "                                if \"artifactId\" in obj[\"typeProperties\"]  and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n",
    "                                    obj[\"typeProperties\"][\"artifactId\"] = lh_artifact_id  \n",
    "                        \n",
    "                        elif \"DataWarehouse\" in value:\n",
    "                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n",
    "                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n",
    "                                    obj[\"typeProperties\"][\"artifactId\"] = dw_artifact_id \n",
    "                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n",
    "                                obj[\"endpoint\"] = dw_endpoint \n",
    "\n",
    "                        elif \"FabricSqlDatabase\" in value:\n",
    "                            if \"typeProperties\" in obj and isinstance(obj[\"typeProperties\"], dict):\n",
    "                                if \"artifactId\" in obj[\"typeProperties\"] and not obj[\"typeProperties\"][\"artifactId\"].startswith(\"@\"):\n",
    "                                    obj[\"typeProperties\"][\"artifactId\"] = meta_artifact_id  \n",
    "                            if \"endpoint\" in obj and not obj[\"endpoint\"].startswith(\"@\"):\n",
    "                                obj[\"endpoint\"] = meta_endpoint  \n",
    "                            if \"externalReferences\" in obj and isinstance(obj[\"externalReferences\"], dict):\n",
    "                                obj[\"externalReferences\"][\"connection\"] = fabricsqldb_connection_id\n",
    "\n",
    "                        elif \"InvokePipeline\" in value:\n",
    "                            if \"externalReferences\" in obj and isinstance(obj[\"externalReferences\"], dict):\n",
    "                                obj[\"externalReferences\"][\"connection\"] = fabricdatapipelines_connection_id\n",
    "\n",
    "                        elif \"PBISemanticModelRefresh\" in value:\n",
    "                            if \"externalReferences\" in obj and isinstance(obj[\"externalReferences\"], dict):\n",
    "                                obj[\"externalReferences\"][\"connection\"] = pbi_connection_id\n",
    "                        \n",
    "                        elif \"TridentNotebook\" in value:\n",
    "                            notebook_name = obj.get(\"name\")\n",
    "                            if notebook_name in trident_notebooks:\n",
    "                                obj[\"typeProperties\"][\"notebookId\"] = trident_notebooks[notebook_name]\n",
    "\n",
    "                    elif key == \"workspaceId\" and isinstance(value, str) and not value.startswith(\"@\"):\n",
    "                        obj[key] = WORKSPACE_ID  \n",
    "\n",
    "                    replace_values(value)\n",
    "\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    replace_values(item)\n",
    "\n",
    "        replace_values(definition_json)\n",
    "\n",
    "        self.raw_definition = json.dumps(definition_json, indent=4)\n",
    "\n",
    "    def upload_pipeline(self):\n",
    "        global pipelines_created\n",
    "        pipelines_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/dataPipelines\"\n",
    "        pipeline_b64 = base64.b64encode(self.raw_definition.encode()).decode()\n",
    "\n",
    "        payload = {\n",
    "            \"displayName\": self.name,\n",
    "            \"folderId\" : self.folder_id,\n",
    "            \"definition\": {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"path\": \"pipeline.content.json\",\n",
    "                        \"payload\": pipeline_b64,\n",
    "                        \"payloadType\": \"InlineBase64\",\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "        create_pipeline_request = requests.post(pipelines_url, headers=headers, data=json.dumps(payload))\n",
    "        if not create_pipeline_request.ok:\n",
    "            print(create_pipeline_request.json())\n",
    "            raise RuntimeError(f\"Failed to create pipeline {self.name}. ({create_pipeline_request.text})\")\n",
    "        self.real_id = create_pipeline_request.json().get(\"id\")\n",
    "        self.pipeline_created = True\n",
    "        pipelines_created.append(self)\n",
    "    \n",
    "pipelines: list[DataPipeline] = []\n",
    "folder_cache = {}\n",
    "for path, dirs, files in os.walk(\".\"):\n",
    "    if not path.endswith(\".DataPipeline\"):\n",
    "        continue\n",
    "    folder_id=None\n",
    "    folder_path = \"/\".join( path.split(\"/\")[3:-1])\n",
    "    if folder_path  in folder_cache:\n",
    "        folder_id=folder_cache[folder_path]\n",
    "    else:\n",
    "        for folder in folder_path.split('/'):\n",
    "            folder_id=create_folder(folder, folder_id)\n",
    "        folder_cache[folder_path] = folder_id\n",
    "    pipeline_name = path.strip(\".\").split(\".\")[0].replace(\"\\\\\", \"/\").split(\"/\")[-1]\n",
    "    platform = json.load(open(os.path.join(path, \".platform\")))\n",
    "    raw_definition = open(os.path.join(path, \"pipeline-content.json\")).read()\n",
    "    definition = json.loads(raw_definition)\n",
    "    pipeline_id = platform[\"config\"][\"logicalId\"]\n",
    "\n",
    "    data_pipeline = DataPipeline(pipeline_name, pipeline_id, raw_definition, definition, folder_id)\n",
    "    pipelines.append(data_pipeline)\n",
    "pipelines_url = f\"{BASE_URL}/workspaces/{WORKSPACE_ID}/dataPipelines\"\n",
    "response = requests.get(pipelines_url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "pipelines_created: list[DataPipeline] = []\n",
    "\n",
    "pipelines_existing: list[(str, str)] = [\n",
    "    (pipeline[\"displayName\"], pipeline[\"id\"])\n",
    "    for pipeline in response.json().get(\"value\", [])\n",
    "]\n",
    "\n",
    "for pipeline_name, pipeline_id in pipelines_existing:\n",
    "    pipeline = next((x for x in pipelines if x.name == pipeline_name), None)\n",
    "    if not pipeline:\n",
    "        continue\n",
    "\n",
    "    pipeline.real_id = pipeline_id\n",
    "    pipeline.pipeline_created = True\n",
    "    pipelines_created.append(pipeline)\n",
    "i=0\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    i+=1\n",
    "    if pipeline in pipelines_created or pipeline.pipeline_created:\n",
    "        continue\n",
    "    #if i>1 : break\n",
    "    pipeline.create(pipelines)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1cdcb-2625-449d-9a11-c903889ec341",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 7. Create objects for Adventure Works when deploy_aw is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350e775b-7c60-4d73-a9d7-d6bd77673d28",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-14T19:11:34.6981076Z",
       "execution_start_time": "2025-07-14T19:09:25.581636Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c75098f4-7b9a-489f-a2f3-b648f08947ca",
       "queued_time": "2025-07-14T19:03:18.3015063Z",
       "session_id": "929963e1-609d-4bbe-8728-c790daed3f81",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 929963e1-609d-4bbe-8728-c790daed3f81, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdventureWorks Samples Deployed.\n"
     ]
    }
   ],
   "source": [
    "if deploy_aw == False:\n",
    "    mssparkutils.notebook.exit(1)\n",
    "\n",
    "class CustomTokenCredential:\n",
    "    def get_token(self, *scopes, **kwargs):\n",
    "        return AccessToken(notebookutils.credentials.getToken('storage'), expires_on=9999999999)\n",
    "\n",
    "credential = CustomTokenCredential()\n",
    "service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n",
    "fs = service_client.get_file_system_client(WORKSPACE_NAME)\n",
    "\n",
    "lh_paths = {\n",
    "    \"tmp/landing/\": f\"{lakehouse_name}.Lakehouse/Files/landing\",\n",
    "    \"tmp/backup/\": f\"{lakehouse_name}.Lakehouse/Files/backup\",\n",
    "    \"tmp/Tables/\": f\"{lakehouse_name}.Lakehouse/Tables\"\n",
    "}\n",
    "\n",
    "def unpack_files(Setup_dir, archives):\n",
    "    for archive, target in archives.items():\n",
    "        shutil.unpack_archive(os.path.join(Setup_dir, archive), target, \"zip\")\n",
    "\n",
    "def upload_files(local_path, azure_path):\n",
    "    for root, _, files in os.walk(local_path):\n",
    "        if root.endswith(\"/_metadata\"): continue\n",
    "        for file in files:\n",
    "            file_path_on_local = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(root, local_path)\n",
    "            file_path_on_azure = os.path.join(azure_path, relative_path, file).replace(\"\\\\\", \"/\")\n",
    "            file_client = fs.get_file_client(file_path_on_azure)\n",
    "            with open(file_path_on_local, \"rb\") as data:\n",
    "                file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "git_lh_directory = \"Setup/Files/Lakehouse\"\n",
    "archives = {\n",
    "    'AW_landing.zip': \"tmp/landing\",\n",
    "    'AW_Backup.zip': \"tmp/backup\",\n",
    "    'AW_tables.zip': \"tmp/Tables\"\n",
    "}\n",
    "\n",
    "unpack_files(git_lh_directory, archives)\n",
    "for local, azure in lh_paths.items():\n",
    "    upload_files(local, azure)\n",
    "\n",
    "#Deploy AW Warehouse Objects \n",
    "time.sleep(120)  # 2mins wait for MD Sync of the Lakehouse objects\n",
    "process_sql_objects(DBType.Warehouse, \"Tables\")\n",
    "process_sql_objects(DBType.Warehouse, \"Views\")\n",
    "print ('AdventureWorks Samples Deployed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b06dba-27d5-41e5-a8c6-3aea4099d953",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-14T19:12:01.1615146Z",
       "execution_start_time": "2025-07-14T19:11:34.7003662Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3b82af23-9fca-4ba6-a02a-d9262e156b01",
       "queued_time": "2025-07-14T19:03:18.3907112Z",
       "session_id": "929963e1-609d-4bbe-8728-c790daed3f81",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 929963e1-609d-4bbe-8728-c790daed3f81, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring cdcSqlTables...\n",
      "restoring Configurations...\n",
      "restoring edwTableLineage...\n",
      "restoring IdentityMethods...\n",
      "restoring PackageGroups...\n",
      "restoring Templates...\n",
      "restoring Datasets...\n",
      "restoring edwTables...\n",
      "restoring PackageGroupLinks...\n",
      "restoring PipelineGroups...\n",
      "restoring edwTableJoins...\n",
      "restoring PackageGroupTables...\n",
      "restoring Pipelines...\n",
      "restoring DatasetLineage...\n",
      "restoring PackageGroupPipelines...\n",
      "restoring PipelineTables...\n",
      "Updating config.Configurations\n",
      "\n",
      "Sample Metadata restored. Now run TR_Ops Pipeline to complete\n"
     ]
    }
   ],
   "source": [
    "#Restore Latest Configuration\n",
    "import pandas as pd\n",
    "\n",
    "if deploy_aw == False:\n",
    "    mssparkutils.notebook.exit(1)\n",
    "\n",
    "WORKSPACE_ID = fabric.get_workspace_id()\n",
    "WORKSPACE_NAME = fabric.resolve_workspace_name()\n",
    "lakehouse_id = fabric.resolve_item_id(lakehouse_name, \"Lakehouse\", WORKSPACE_ID)\n",
    "client = fabric.FabricRestClient()\n",
    "items = fabric.FabricRestClient().get(f\"/v1/workspaces/{WORKSPACE_ID}/SQLDatabases\").json()[\"value\"]\n",
    "sql_database=next((endpoint for endpoint in items if endpoint[\"displayName\"] == metadata_db_name ))\n",
    "sql_end_point = sql_database[\"properties\"][\"serverFqdn\"]\n",
    "sql_database_name = sql_database[\"properties\"][\"databaseName\"]\n",
    "connection_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={sql_end_point};database={sql_database_name}\"\n",
    "token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n",
    "token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "engine=sqlalchemy.create_engine(\"mssql+pyodbc://\", creator=lambda: pyodbc.connect(connection_string, attrs_before={1256: token_struct}))\n",
    "token = notebookutils.credentials.getToken(\"https://storage.azure.com/\")\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "class CustomTokenCredential:\n",
    "    def get_token(self, *scopes, **kwargs):\n",
    "        return AccessToken(notebookutils.credentials.getToken('storage'), expires_on=9999999999)\n",
    "\n",
    "credential = CustomTokenCredential()\n",
    "service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n",
    "fs = service_client.get_file_system_client(WORKSPACE_NAME)\n",
    "\n",
    "path =  f\"{lakehouse_name}.Lakehouse/Files/backup/config\"\n",
    "paths = fs.get_paths(path=path)\n",
    "folders = [p.name[len(path.rstrip('/') + '/'):] for p in paths if p.is_directory and '/' not in p.name[len(path.rstrip('/') + '/'):]]\n",
    "max_folder = max(folders) if folders else None # Return Last Backup of Config Data\n",
    "\n",
    "path =  f\"{lakehouse_name}.Lakehouse/Files/backup/config/{max_folder}\"\n",
    "paths = fs.get_paths(path=path)\n",
    "files = [p.name[len(path.rstrip('/') + '/'):] for p in paths if not p.is_directory and '/' not in p.name[len(path.rstrip('/') + '/'):]]\n",
    "\n",
    "with engine.connect() as alchemy_connection:\n",
    "    cursor = engine.raw_connection().cursor()\n",
    "    cursor.execute (\"exec config.usp_TruncateAll\")\n",
    "    cursor.commit()\n",
    "    rows = pd.read_sql_query(\"select * from config.Configurations\", alchemy_connection)\n",
    "    if not rows.empty:\n",
    "        print ('Sample Metadata already deployed. Skipping.')\n",
    "        notebookutils.notebook.exit(1)\n",
    "    tables = pd.read_sql_query (\"exec [config].[usp_GetTables]\", alchemy_connection)\n",
    "    for _, row in tables.iterrows():\n",
    "        table = row['name']  # adjust this column name to match your output\n",
    "        file_url = f\"https://onelake.dfs.fabric.microsoft.com/{WORKSPACE_ID}/{lakehouse_id}/Files/backup/config/{max_folder}/{table}.parquet\"\n",
    "        response = requests.head(file_url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "        file_path = f\"abfss://{WORKSPACE_ID}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files/backup/config/{max_folder}/{table}.parquet\"\n",
    "        df = spark.read.parquet(file_path).toPandas()\n",
    "        print (f\"restoring {table}...\")\n",
    "        df.to_sql(table, schema='config', con=engine, if_exists='append', index=False)\n",
    "    \n",
    "    print ('Updating config.Configurations')\n",
    "    settings = {\n",
    "        \"Lakehouse\": lakehouse_name ,\n",
    "        \"LakehouseID\": lakehouse_id,\n",
    "        \"WorkspaceID\": WORKSPACE_ID\n",
    "    }   \n",
    "    settings_json = json.dumps(settings) \n",
    "    sql = f\"UPDATE config.Configurations SET ConnectionSettings ='{settings_json}' WHERE ConfigurationID=1\"\n",
    "    cursor.execute (sql)\n",
    "    cursor.commit()\n",
    "\n",
    "    settings = {\n",
    "        \"Workspace\": WORKSPACE_NAME,\n",
    "        \"WorkspaceID\": WORKSPACE_ID\n",
    "    }   \n",
    "    settings_json = json.dumps(settings) \n",
    "    sql = f\"UPDATE config.Configurations SET ConnectionSettings ='{settings_json}' WHERE ConfigurationID=2\"\n",
    "    cursor.execute (sql)\n",
    "    cursor.commit()\n",
    "\n",
    "print ('')\n",
    "print ('Sample Metadata restored. Now run TR_Ops Pipeline to complete')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "warehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
