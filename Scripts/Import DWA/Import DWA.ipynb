{"cells":[{"cell_type":"markdown","source":["# Script to import Data Warehouse objects in the Fabric Workspace"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9a9979c7-719f-4bf4-9e63-f8a6aaf6ac67"},{"cell_type":"markdown","source":["**Parameter Block**\n","\n","Update the parameters before executing the notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5122a7e-7fa5-4735-bbb6-283b7b78ebbd"},{"cell_type":"code","source":["#Parameters\n","workspace_name = 'Deployment_DWA_Test' #Add your workspace name to import DWA\n","deploy_aw = True #Set this flag to True if you want to deploy AW \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:07.8760215Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:07.8775731Z","execution_finish_time":"2025-03-11T09:03:08.2666743Z","parent_msg_id":"5d871c16-9c79-4057-809b-2e8571b48bd1"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"19abb929-7fea-4ae9-bb22-f9026054affe"},{"cell_type":"code","source":["import struct\n","import sqlalchemy\n","from sqlalchemy.sql import text\n","from notebookutils import mssparkutils\n","import sempy.fabric as fabric\n","import base64\n","from azure.core.credentials import AccessToken\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import DefaultAzureCredential\n","import os\n","import pyodbc\n","import shutil\n","from git import Repo\n","import requests\n","import json"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:07.9537779Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:08.2691917Z","execution_finish_time":"2025-03-11T09:03:08.5768852Z","parent_msg_id":"898e3334-2f5e-486d-8ba3-e1afff69cf47"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee02c50f-f044-4ad4-81c3-d1afaa40738e"},{"cell_type":"markdown","source":["**Code block to create a Lakehouse**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb059d74-91d2-4033-afe8-4b1eb645d12b"},{"cell_type":"code","source":["workspace_id = spark.conf.get(\"trident.workspace.id\")\n","base_url = \"https://api.fabric.microsoft.com/v1\"\n","access_token = mssparkutils.credentials.getToken('pbi')\n","\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","#Create Lakehouse\n","lakehouse_url = f\"{base_url}/workspaces/{workspace_id}/lakehouses\"\n","payload = {\n","    \"displayName\": \"LH\",\n","    \"description\": \"A schema-enabled lakehouse.\",\n","    \"creationPayload\": {\"enableSchemas\": True}\n","}\n","response = requests.post(lakehouse_url, headers=headers, json=payload)\n","\n","if response.status_code == 201:\n","    print(f\"Lakehouse {payload['displayName']} created successfully!\")\n","    print(response.json())\n","elif response.status_code == 400:\n","    print(f\"Lakehouse {payload['displayName']} is already in use\")\n","else:\n","    print(\"Failed to create Lakehouse: \")\n","    raise RuntimeError(response.status_code, response.text)\n","\n","lakehouse_name = payload[\"displayName\"]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:08.1103135Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:08.5796871Z","execution_finish_time":"2025-03-11T09:03:09.3507376Z","parent_msg_id":"227e5b26-d854-4c81-a3ce-852eb436bf19"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Lakehouse LH is already in use\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa181b62-1c13-45fd-909d-c415eda39a1e"},{"cell_type":"markdown","source":[" **Code block to create metadata based SQL DB**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a342f7b3-a3f4-49d7-9b7d-bf09c495fe48"},{"cell_type":"code","source":["payload = {\n","    \"displayName\": \"Meta\",\n","    \"type\": \"SQLDatabase\",\n","    \"description\": \"Created using Python in Microsoft Fabric\"\n","}\n","\n","sqldb_url = f'{base_url}/workspaces/{workspace_id}/items'\n","response = requests.post(sqldb_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"SQL DB {payload['displayName']} is already in use\")\n","else:\n","    time.sleep(300)                    # Wait for database creation\n","    if response.status_code == 201:\n","        print(f\"Database {body['displayName']} created successfully:\")\n","        print(response.json())\n","    else:\n","        print(f\"Failed to create database: \", response.status_code, response.text)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:08.202552Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:09.353035Z","execution_finish_time":"2025-03-11T09:03:10.1327264Z","parent_msg_id":"923acee8-c7db-4d7b-b8e3-96b1f5077393"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SQL DB Meta is already in use\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"786c758b-d46c-462c-a5ff-3a9bf6edf8b0"},{"cell_type":"markdown","source":["**Code block to create Data Warehouse**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76027a2b-59de-4c6c-b527-41634724ced6"},{"cell_type":"code","source":["payload = {\n","    \"displayName\": \"DW\",\n","    \"type\": \"warehouse\",\n","    \"properties\": {\n","        \"collation\": \"Latin1_General_100_CI_AS_KS_WS_SC_UTF8\"\n","    },\n","    \"description\": \"Created using Python in Microsoft Fabric\"\n","}\n","\n","sqldb_url = f'{base_url}/workspaces/{workspace_id}/items'\n","response = requests.post(sqldb_url, headers=headers, json=payload)\n","\n","if response.status_code == 400:\n","    print(f\"Warehouse {payload['displayName']} is already in use\")\n","else:\n","    time.sleep(300)                    # Wait for database creation\n","    if response.status_code == 201:\n","        print(f\"Warehouse {body['displayName']} created successfully:\")\n","        print(response.json())\n","    else:\n","        print(f\"Failed to create warehouse: \", response.status_code, response.text)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:08.312039Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:10.1350713Z","execution_finish_time":"2025-03-11T09:03:10.9536985Z","parent_msg_id":"208def47-b809-415d-925d-22b4cb695319"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warehouse DW is already in use\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcc75513-071f-4146-b283-9fddeaa22a6b"},{"cell_type":"markdown","source":["**Code Block to copy Adventure Works Files into Lakehouse Files/Landing/aw when deploy_aw is set to TRUE**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71d54c7b-f316-4190-ad91-28666bd6bd60"},{"cell_type":"code","source":["if deploy_aw == False:\n","    mssparkutils.notebook.exit(1)\n","else:\n","    class CustomTokenCredential:\n","        def __init__(self, token):\n","            self.token = token\n","\n","        def get_token(self, *scopes, **kwargs):\n","            return AccessToken(self.token, expires_on=9999999999)  # Set a far future expiration\n","\n","    credential = CustomTokenCredential(notebookutils.credentials.getToken('storage'))\n","    service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential=credential)\n","    fs = service_client.get_file_system_client(workspace_name)\n","\n","    lh_paths = {\n","        \"tmp/landing/\": f\"{lakehouse_name}.Lakehouse/Files/landing\",\n","        \"tmp/Tables/\": f\"{lakehouse_name}.Lakehouse/Tables\"\n","    }\n","\n","    def clone_and_unpack(repo_url, repo_dir, backup_dir, archives):\n","        if os.path.exists(repo_dir):\n","            print(\"Repo already cloned.\")\n","        Repo.clone_from(repo_url, repo_dir)\n","        os.chdir(repo_dir)\n","        for archive, target in archives.items():\n","            shutil.unpack_archive(os.path.join(backup_dir, archive), target, \"zip\")\n","\n","    def upload_files(local_path, azure_path):\n","        for root, _, files in os.walk(local_path):\n","            for file in files:\n","                file_path_on_local = os.path.join(root, file)\n","                relative_path = os.path.relpath(root, local_path)\n","                file_path_on_azure = os.path.join(azure_path, relative_path, file).replace(\"\\\\\", \"/\")\n","                file_client = fs.get_file_client(file_path_on_azure)\n","                with open(file_path_on_local, \"rb\") as data:\n","                    file_client.upload_data(data, overwrite=True)\n","\n","    repo_url = \"https://github.com/ProdataSQL/DWA\"\n","    repo_dir = \"DWA_repo\"\n","    git_lh_directory = \"Backup/Files/Lakehouse\"\n","    archives = {\n","        'AW_landing.zip': \"tmp/landing\",\n","        'AW_tables.zip': \"tmp/Tables\"\n","    }\n","\n","    clone_and_unpack(repo_url, repo_dir, git_lh_directory, archives)\n","\n","    for local, azure in lh_paths.items():\n","        upload_files(local, azure)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:08.430339Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:10.9562754Z","execution_finish_time":"2025-03-11T09:03:26.7756644Z","parent_msg_id":"35d97fcb-9efc-4c1f-baf4-17a08bd67f3e"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e775b-7c60-4d73-a9d7-d6bd77673d28"},{"cell_type":"markdown","source":["**Code to Import Notebooks**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"826cde8d-8742-4f41-8860-3f2958d5800f"},{"cell_type":"code","source":["github_api_url = \"https://api.github.com/repos/ProdataSQL/DWA/contents/Workspaces/DWA\"\n","github_raw_base = \"https://raw.githubusercontent.com/ProdataSQL/DWA/main\"\n","notebook_url = f\"{base_url}/workspaces/{workspace_id}/notebooks\"\n","\n","# Fetch directory contents from GitHub\n","response = requests.get(github_api_url)\n","if response.status_code == 200:\n","    items = response.json()\n","    for item in items:\n","        # Identify directories ending with .Notebook\n","        if item['type'] == 'dir' and item['name'].endswith('.Notebook'):\n","            notebook_name = item['name']\n","            contents_url = f\"{github_raw_base}/Workspaces/DWA/{notebook_name}/notebook-content.py\"\n","            contents_response = requests.get(contents_url)\n","            if contents_response.status_code == 200:\n","                # Convert contents.py to Jupyter Notebook format\n","                py_content = contents_response.text\n","                notebook_content = {\n","                    \"nbformat\": 4,\n","                    \"nbformat_minor\": 5,\n","                    \"cells\": [\n","                        {\n","                            \"cell_type\": \"code\",\n","                            \"source\": [py_content],\n","                            \"execution_count\": None,\n","                            \"outputs\": []\n","                        }\n","                    ],\n","                    \"metadata\": {\n","                        \"language_info\": {\n","                            \"name\": \"python\"\n","                        }\n","                    }\n","                }\n","                # Encode notebook content in Base64\n","                notebook_json = json.dumps(notebook_content)\n","                notebook_base64 = base64.b64encode(notebook_json.encode('utf-8')).decode('utf-8')\n","                \n","                # Prepare payload for Fabric API\n","                payload = {\n","                    \"displayName\": notebook_name,\n","                    \"description\": f\"Imported notebook {notebook_name}\",\n","                    \"definition\": {\n","                        \"format\": \"ipynb\",\n","                        \"parts\": [\n","                            {\n","                                \"path\": \"artifact.content.ipynb\",\n","                                \"payload\": notebook_base64,\n","                                \"payloadType\": \"InlineBase64\"\n","                            }\n","                        ]\n","                    }\n","                }\n","                \n","                # Upload notebook to Microsoft Fabric\n","                fabric_response = requests.post(\n","                    notebook_url.format(workspace_id=workspace_id),\n","                    headers=headers,\n","                    data=json.dumps(payload)\n","                )\n","                \n","                if fabric_response.status_code in [200, 201, 202]:\n","                    print(f\"Successfully uploaded: {notebook_name}\")\n","                else:\n","                    print(f\"Failed to upload {notebook_name}: {fabric_response.status_code} - {fabric_response.text}\")\n","            else:\n","                raise RuntimeError(f\"Failed to download contents.py for {notebook_name}: {contents_response.status_code}\")\n","else:\n","    raise RuntimeError(f\"Failed to fetch GitHub directory contents: {response.status_code}, {response.text}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:03:08.6801428Z","session_start_time":null,"execution_start_time":"2025-03-11T09:03:26.7786763Z","execution_finish_time":"2025-03-11T09:03:34.5231462Z","parent_msg_id":"0d47cb7c-c7ee-4386-b7ee-860073410ba4"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Failed to upload Demo - 10 Mins.Notebook: 400 - {\"requestId\":\"a859b8ae-1aa5-47cb-88d8-195b067469b3\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Demo - 10 Mins.Notebook' is already in use\"}\nFailed to upload Extract-CSV-Pandas.Notebook: 400 - {\"requestId\":\"5ce30ee9-66b2-4a0a-bbc0-ee4d9f956dfe\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-CSV-Pandas.Notebook' is already in use\"}\nFailed to upload Extract-CSV.Notebook: 400 - {\"requestId\":\"24dd5354-4e3c-4a6b-b95b-161122a395b4\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-CSV.Notebook' is already in use\"}\nFailed to upload Extract-Dictionary.Notebook: 400 - {\"requestId\":\"a9a667d5-0a15-464b-907f-5e7980b03b8a\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-Dictionary.Notebook' is already in use\"}\nFailed to upload Extract-Fabric-Logs.Notebook: 400 - {\"requestId\":\"8c900dff-1f01-462c-ac2c-792251a31e45\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-Fabric-Logs.Notebook' is already in use\"}\nFailed to upload Extract-O365-API.Notebook: 400 - {\"requestId\":\"5dda9545-df80-4224-9fee-fe5f80d32da2\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-O365-API.Notebook' is already in use\"}\nFailed to upload Extract-SP-Excel.Notebook: 400 - {\"requestId\":\"908c50c3-5ef6-4477-9985-e007c1e6a55a\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-SP-Excel.Notebook' is already in use\"}\nFailed to upload Extract-XML.Notebook: 400 - {\"requestId\":\"d3aa8119-0240-4c6c-8bf8-d6ca94286870\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Extract-XML.Notebook' is already in use\"}\nFailed to upload Ingest-SFTP.Notebook: 400 - {\"requestId\":\"aa72ddb2-b05d-49c1-98fa-5c492ff5e270\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Ingest-SFTP.Notebook' is already in use\"}\nFailed to upload Refresh-Fabric.Notebook: 400 - {\"requestId\":\"bc1c4eb9-c936-498c-b2c2-7b98d83915d7\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'Refresh-Fabric.Notebook' is already in use\"}\nFailed to upload SharePoint-Shared-Functions.Notebook: 400 - {\"requestId\":\"58520b6f-b48a-4e2b-bde0-528e2019a815\",\"errorCode\":\"ItemDisplayNameAlreadyInUse\",\"message\":\"Requested 'SharePoint-Shared-Functions.Notebook' is already in use\"}\n"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0401c12c-4d62-4d74-b64c-a4a2f11db058"},{"cell_type":"markdown","source":["**Deploy Meta DB and Warehouse SQL Objects**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c9e46bad-f4eb-452f-9e8d-1380cc6ae57b"},{"cell_type":"code","source":["def get_connection_string(workspace_id: str, db_type: str, display_name: str) -> str:\n","    client = fabric.FabricRestClient()\n","    endpoint = f\"/v1/workspaces/{workspace_id}/{db_type}\"\n","    databases = client.get(endpoint).json()\n","    database = next((db for db in databases.get(\"value\", []) if db.get(\"displayName\") == display_name), None)\n","    \n","    if not database:\n","        raise ValueError(f\"No {db_type} with displayName '{display_name}' found.\")\n","    \n","    server = database['properties'].get('serverFqdn') or database['properties'].get('connectionString')\n","    if db_type == \"Warehouses\":\n","        database = display_name\n","    else: \n","        database = database['properties']['databaseName']\n","    return f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};database={database};LongAsMax=YES\"\n","\n","def create_engine(connection_string: str):\n","    token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n","    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n","    return sqlalchemy.create_engine(\"mssql+pyodbc://\", creator=lambda: pyodbc.connect(connection_string, attrs_before={1256: token_struct}))\n","\n","def execute_sql_script(engine, script):\n","    statements = script.split(\"\\nGO\\n\")\n","    check_objects_query = \"SELECT COUNT(*) AS TableCount FROM sys.tables; \"\n","    \n","    with engine.connect() as conn:\n","        result = conn.execute(text(check_objects_query))\n","        rows = [dict(row) for row in result.mappings()]\n","\n","        if rows[0]['TableCount'] == 0:\n","            try:\n","                for statement in statements:\n","                    statement = statement.strip()\n","                    if statement:\n","                        conn.execute(text(statement))\n","                        conn.commit()\n","            except:\n","                conn.rollback()\n","                raise\n","        else:\n","            print(f\"Objects already exist\")\n","\n","def process_sql_script(workspace_id: str, db_type: str, display_name: str, script_path: str):\n","    connection_string = get_connection_string(workspace_id, db_type, display_name)\n","    engine = create_engine(connection_string)\n","    \n","    with open(script_path, \"r\", encoding=\"utf-16\") as file:\n","        script = file.read()\n","    \n","    execute_sql_script(engine, script)\n","\n","# Process Meta SQL Database\n","process_sql_script(workspace_id, \"SQLDatabases\", \"Meta\", \"Backup/Files/SQLDB/SQLDB_Script.sql\")\n","\n","# Process Data Warehouse\n","process_sql_script(workspace_id, \"Warehouses\", \"DW\", \"Backup/Files/DataWarehouse/DW_script.sql\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"finished","queued_time":"2025-03-11T09:15:31.822073Z","session_start_time":null,"execution_start_time":"2025-03-11T09:15:31.8238209Z","execution_finish_time":"2025-03-11T09:15:35.2333256Z","parent_msg_id":"561b0529-0044-4369-a3ec-f1e2848cf317"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, 27, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Engine(mssql+pyodbc://)\nObjects already exist\nEngine(mssql+pyodbc://)\nObjects already exist\n"]}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38dabb2f-5a1b-4975-a11e-4e2c2f29e6d2"},{"cell_type":"markdown","source":["**Code to import Data Pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"83eaf211-ce90-45c5-b1ec-a3defa45de40"},{"cell_type":"code","source":["#pipelines_url = f\"{base_url}/workspaces/{workspace_id}/dataPipelines\"\n","#\n","## Fetch GitHub directory contents\n","#response = requests.get(github_api_url)\n","#\n","#if response.status_code == 200:\n","#    for item in response.json():\n","#        # Identify directories ending with .DataPipeline\n","#        if item.get('type') == 'dir' and item.get('name', '').endswith('.DataPipeline'):\n","#            pipeline_name = item['name']                                  \n","#            file_url = f\"{github_raw_base}{pipeline_name}/pipeline-content.json\"\n","#            r = requests.get(file_url)\n","#            if r.status_code != 200:\n","#                print(f\"Failed to download {pipeline_name}: {r.status_code}\")\n","#                continue\n","#            try:\n","#                pipeline_def = r.json()  # The JSON definition of your data pipeline\n","#            except Exception as e:\n","#                print(f\"Error parsing JSON for {pipeline_name}: {e}\")\n","#                continue\n","#\n","#            # Encode pipeline content in Base64\n","#            pipeline_b64 = base64.b64encode(json.dumps(pipeline_def).encode()).decode()\n","#            \n","#            # Prepare payload for Fabric DataPipeline API\n","#            payload = {\n","#                \"displayName\": pipeline_name,\n","#                \"description\": f\"Imported data pipeline {pipeline_name}\",\n","#                \"definition\": {\n","#                    \"format\": \"json\",\n","#                    \"parts\": [{\n","#                        \"path\": \"artifact.content.json\",\n","#                        \"payload\": pipeline_b64,\n","#                        \"payloadType\": \"InlineBase64\"\n","#                    }]\n","#                }\n","#            }\n","#\n","#            # Check if the pipeline already exists\n","#            list_response = requests.get(pipelines_url, headers=headers)\n","#            existing = None\n","#            if list_response.status_code == 200:\n","#                pipelines = list_response.json()\n","#                if isinstance(pipelines, dict) and \"value\" in pipelines:\n","#                    pipelines = pipelines[\"value\"]\n","#                for pl in pipelines:\n","#                    if isinstance(pl, dict) and pl.get('displayName') == pipeline_name:\n","#                        existing = pl\n","#                        break\n","#\n","#            if existing:\n","#                update_url = f\"{base_url}/workspaces/{workspace_id}/dataPipelines/{existing['id']}\"\n","#                upd_response = requests.put(update_url, headers=headers, data=json.dumps(payload))\n","#                if upd_response.status_code in [200, 201, 202]:\n","#                    print(f\"Updated: {pipeline_name}\")\n","#                else:\n","#                    print(f\"Failed to update {pipeline_name}: {upd_response.status_code} - {upd_response.text}\")\n","#            else:\n","#                post_response = requests.post(pipelines_url, headers=headers, data=json.dumps(payload))\n","#                if post_response.status_code in [200, 201, 202]:\n","#                    print(f\"Uploaded: {pipeline_name}\")\n","#                else:\n","#                    print(f\"Failed to upload {pipeline_name}: {post_response.status_code} - {post_response.text}\")\n","#else:\n","#    raise RuntimeError(f\"Failed to fetch GitHub directory contents: {response.status_code}, {response.text}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":"8c28822d-cef4-44ad-8d1e-adb8ba4fe942","normalized_state":"cancelled","queued_time":"2025-03-11T09:03:09.192472Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-03-11T09:03:36.8382263Z","parent_msg_id":"1e236767-3311-40a3-8cdc-3fcc6e03ab1d"},"text/plain":"StatementMeta(, 8c28822d-cef4-44ad-8d1e-adb8ba4fe942, -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7d012a86-c550-4558-a960-333b7ed0e837"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"warehouse":{}}},"nbformat":4,"nbformat_minor":5}